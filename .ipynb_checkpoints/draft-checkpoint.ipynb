{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Library and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "AUDIO_FILES_PATH = \"auditary_emotion_recognition/data_interview/Audio/Audio\"\n",
    "AUDIO_LABEL_PATH = \"auditary_emotion_recognition/data_interview/Labels/turker_scores_full_interview.csv\"\n",
    "import pickle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from acousticFeatures import getAllFeatures\n",
    "import parselmouth \n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_acoustic_features = pickle.load(open('processed-data/max_acoustic_features.obj', 'rb'))\n",
    "min_acoustic_features = pickle.load(open('processed-data/min_acoustic_features.obj', 'rb'))\n",
    "mean_acoustic_features = pickle.load(open('processed-data/mean_acoustic_features.obj', 'rb'))\n",
    "std_acoustic_features = pickle.load(open('processed-data/std_acoustic_features.obj', 'rb'))\n",
    "\n",
    "#Load model\n",
    "clf = MLPClassifier(hidden_layer_sizes= (100,), learning_rate_init= 0.0001)\n",
    "clf = pickle.load(open(\"model/model_classification.sav\", 'rb'))\n",
    "model_regression_arouse = pickle.load(open(\"model/model_regression_arouse.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the acoustic feature values in Interview data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut a segment of audio for analyzing\n",
    "sound1 = AudioSegment.from_file(AUDIO_FILES_PATH + \"/\" + \"P11.wav\", frame_rate= 44100)\n",
    "sound2 = sound1[6000:8000]\n",
    "left, right = sound2.split_to_mono()\n",
    "sound = parselmouth.Sound(left.get_array_of_samples())\n",
    "\n",
    "acoustic_features_interview = np.array(getAllFeatures(sound))\n",
    "for e in zip(min_acoustic_features, max_acoustic_features, acoustic_features_interview):\n",
    "    belong = False\n",
    "    if e[0] <= e[2] and e[2] <= e[1]:\n",
    "        belong = True\n",
    "    #print (e,\"          .Belong:\", belong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the acoustic feature values in EMO (france) data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output: 535, 535\n",
      "Size after filtering input: 133, output: 133\n",
      "(133, 88)\n"
     ]
    }
   ],
   "source": [
    "file_handler_input = open('processed-data/input-EMO.obj', 'rb')\n",
    "file_handler_output = open('processed-data/output-EMO.obj', 'rb')\n",
    "input_EMO = np.array(pickle.load(file_handler_input))\n",
    "output_EMO = np.array(pickle.load(file_handler_output))\n",
    "print(\"Size input, output: {}, {}\".format(len(input_EMO), len(input_EMO)))\n",
    "LABELS = ['F', 'T']\n",
    "EMOTION = {'L': 'bordom', 'F': 'happiness', 'T': 'sadness', 'N': 'neutral'}\n",
    "in_out_EMO  = filter(lambda x: x[1] in LABELS , zip(input_EMO, output_EMO))\n",
    "input_EMO_filtered, output_EMO_filtered = zip(*in_out_EMO)\n",
    "print(\"Size after filtering input: {}, output: {}\".format(len(input_EMO_filtered), len(output_EMO_filtered)))\n",
    "\n",
    "input_EMO_filtered = np.array(input_EMO_filtered)\n",
    "print(input_EMO_filtered.shape)\n",
    "for e in zip(min_acoustic_features, max_acoustic_features, input_EMO_filtered[3]):\n",
    "    belong = False\n",
    "    if e[0] <= e[2] and e[2] <= e[1]:\n",
    "        belong = True\n",
    "    #print (e,\"          .Belong:\", belong)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test acoustic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 51.952\n",
      "Energy: 19246096.017249998\n",
      "Power: 370459.19343336154\n",
      "neam f1: 651.7347966638118\n",
      "neam f2: 1834.245626660736\n",
      "neam f3: 2761.7925726076214\n",
      "jitter: 0.025611750905398262\n",
      "shimmer: 0.09486117702767627\n"
     ]
    }
   ],
   "source": [
    "import pydub \n",
    "import numpy as np\n",
    "import sys\n",
    "import parselmouth \n",
    "\n",
    "def getStatistic(numpy_arr):\n",
    "    numpy_arr = np.array(numpy_arr)\n",
    "    numpy_arr = numpy_arr[numpy_arr != 0]\n",
    "    max_v = np.max(numpy_arr)\n",
    "    min_v = np.min(numpy_arr)\n",
    "    range_v = np.max(numpy_arr) - np.min(numpy_arr)\n",
    "    mean_v = np.mean(numpy_arr)\n",
    "    median_v = np.median(numpy_arr)\n",
    "    per25_v = np.percentile(numpy_arr, 25)\n",
    "    per75_v= np.percentile(numpy_arr, 75)\n",
    "    std_v = np.std(numpy_arr)\n",
    "    return np.array([max_v, min_v, range_v, mean_v, median_v, per25_v, per75_v, std_v])\n",
    "\n",
    "def calculateJitter(data):\n",
    "    \"\"\"Data is list of time of peaks\"\"\"\n",
    "    data = np.array(data)\n",
    "    data = data[data != 0]\n",
    "    n = len(data)\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    for i in range(n):\n",
    "        if i > 0:\n",
    "            sum1 += abs(data[i-1] - data[i])\n",
    "        sum2 += data[i]\n",
    "    sum1 /= float(n - 1)\n",
    "    sum2 /= float(n)\n",
    "    return (sum1 / sum2)\n",
    "\n",
    "\n",
    "#Get Shimmer\n",
    "def calculateShimmer(data):\n",
    "    data = np.array(data)\n",
    "    data = data[data != 0]\n",
    "    n = len(data)\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    for i in range(n):\n",
    "        if i > 0:\n",
    "            sum1 += abs(data[i-1] - data[i])\n",
    "        sum2 += data[i]\n",
    "    sum1 /= float(n - 1)\n",
    "    sum2 /= float(n)\n",
    "    return   (sum1 / sum2)\n",
    "\n",
    "audio = AudioSegment.from_file(AUDIO_FILES_PATH + \"/\" + \"P1.wav\")\n",
    "audio_q1 = audio[0:51952]\n",
    "left, right = audio_q1.split_to_mono()\n",
    "sound = parselmouth.Sound(left.get_array_of_samples(), audio_q1.frame_rate)\n",
    "\n",
    "#Duration\n",
    "duration = sound.duration\n",
    "print(\"Duration:\", duration)\n",
    "\n",
    "#Energy\n",
    "energy = sound.get_energy()\n",
    "print(\"Energy:\", energy)\n",
    "\n",
    "#Power\n",
    "power = sound.get_power()\n",
    "print(\"Power:\", power)\n",
    "\n",
    "#Pitch\n",
    "pitch = sound.to_pitch(time_step = 0.01)\n",
    "num_frames = pitch.get_number_of_frames()\n",
    "frames = [pitch.get_frame(i) for i in range(1, num_frames+1)]\n",
    "times = [pitch.get_time_from_frame_number(i) for i in range(1, num_frames+1)]\n",
    "formants = sound.to_formant_burg()\n",
    "\n",
    "#Get intensity statistic\n",
    "intensity_arr = [frame.intensity for frame in frames]\n",
    "intensity_stat = getStatistic(intensity_arr)\n",
    "#print(intensity_stat)\n",
    "#print(pitch)\n",
    "\n",
    "#Frequency\n",
    "f1_arr = [formants.get_value_at_time(1, time) for time in times]\n",
    "print(\"neam f1:\", np.mean(f1_arr))\n",
    "\n",
    "f2_arr = [formants.get_value_at_time(2, time) for time in times]\n",
    "print(\"neam f2:\", np.mean(f2_arr))\n",
    "\n",
    "f3_arr = [formants.get_value_at_time(3, time) for time in times]\n",
    "print(\"neam f3:\", np.mean(f3_arr))\n",
    "\n",
    "\n",
    "#Jitter\n",
    "#Jitter\n",
    "f0_arr = pitch.selected_array['frequency']\n",
    "f0_arr = np.array(f0_arr)\n",
    "f0_arr = f0_arr[f0_arr !=  0]\n",
    "jitter = calculateJitter(1000/f0_arr)\n",
    "print(\"jitter:\", jitter)\n",
    "\n",
    "#Shimmer\n",
    "amplitude_arr = pitch.selected_array['strength']\n",
    "shimmer = calculateShimmer(amplitude_arr)\n",
    "print(\"shimmer:\", shimmer)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36424412 0.49       0.13844262]\n"
     ]
    }
   ],
   "source": [
    "from acousticFeatures import getAllFeatures\n",
    "\n",
    "f = getAllFeatures(sound)\n",
    "#print(f)\n",
    "print(f[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
