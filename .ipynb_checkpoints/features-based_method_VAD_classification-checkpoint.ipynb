{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries and Define constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#### Training based on features of audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots_adjust\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output: 10018 ,  10018\n"
     ]
    }
   ],
   "source": [
    "##Loading  data from files\n",
    "filehandlerInput = open('processed-data/input_VAD.obj', 'rb')\n",
    "filehandlerOutput = open('processed-data/output_VAD.obj', 'rb')\n",
    "input = pickle.load(filehandlerInput)\n",
    "output = pickle.load(filehandlerOutput)\n",
    "print(\"Size input, output:\", len(input),\", \", len(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data and Analyze data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct output:  {0.5, 1.5, 2.5, 3.5, 3.0, 4.0, 2.0, 4.5, 5.0, 1.0, 2.3333, 2.6667, 4.3333, 2.25, 2.75, 3.25, 3.75, 4.75, 4.25, 5.5, 1.3333, 3.3333, 4.6667, 1.6667, 3.6667}\n",
      "Average labels: 2.7782824016769814\n",
      "Std labels: 0.8970523348000802\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x576 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3,3, figsize=(5,8))\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "\n",
    "## Draw distribution of raw output\n",
    "flattened_out = output.flatten()\n",
    "print(\"Distinct output: \", set(flattened_out));\n",
    "print(\"Average labels:\", np.average(output[:,0]))\n",
    "print(\"Std labels:\", np.std(output[:,0]))\n",
    "axs[0][0].hist(output[:,0])\n",
    "axs[0][0].set_title(\"Distribution Valance\")\n",
    "axs[0][1].hist(output[:,1])\n",
    "axs[0][1].set_title(\"Distribution Arouse\")\n",
    "axs[0][2].hist(output[:,2])\n",
    "axs[0][2].set_title(\"Distribution Dominance\")\n",
    "\n",
    "\n",
    "## Around output into integer.\n",
    "# for i in range(0, output.size):\n",
    "#     output.flat[i] = int(np.round(output.flat[i]))\n",
    "# output = output.astype(int)\n",
    "\n",
    "# flattened_out = output.flatten()\n",
    "# print(\"\\nDistinct output: \", set(flattened_out));\n",
    "# print(\"Average labels:\", np.average(output[:,0]))\n",
    "# print(\"Std labels:\", np.std(output[:,0]))\n",
    "# axs[1][0].hist(output[:,0])\n",
    "# axs[1][0].set_title(\"Distribution Valance\")\n",
    "# axs[1][1].hist(output[:,1])\n",
    "# axs[1][1].set_title(\"Distribution Arouse\")\n",
    "# axs[1][2].hist(output[:,2])\n",
    "# axs[1][2].set_title(\"Distribution Dominance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output after remove Nan values  10009 ,  10009\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from pprint import pprint\n",
    "input_filtered = input[~np.any(np.isnan(input), axis=1)]\n",
    "output_filtered = output[~np.any(np.isnan(input), axis=1)]\n",
    "print(\"Size input, output after remove Nan values \", len(input_filtered), \", \", len(output_filtered))\n",
    "\n",
    "#Normalize input\n",
    "input_filtered = (input_filtered - input_filtered.min(axis=0)) / (input_filtered.max(axis=0) - input_filtered.min(axis=0))\n",
    "\n",
    "#Normalize output\n",
    "#output_filtered = [to_categorical(out, 6) for out in output_filtered]\n",
    "def categorize_output(output):\n",
    "    if (output <= 2.5):\n",
    "        return 0\n",
    "    elif  output >= 4:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# def categorize_output(output):\n",
    "#     return np.around(output)\n",
    "   \n",
    "\n",
    "output_filtered = np.array([list(map(lambda x: to_categorical(categorize_output(x), 3), col)) for col in output_filtered])\n",
    "#output_filtered = np.array([list(map(lambda x: categorize_output(x), col)) for col in output_filtered])\n",
    "#output_filtered = (output_filtered - output_filtered.min(axis=0)) / (output_filtered.max(axis=0) - output_filtered.min(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffer data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "#Shuffer\n",
    "c = list(zip(input_filtered, output_filtered))\n",
    "random.shuffle(c)\n",
    "input_filtered, output_filtered = zip( * c)\n",
    "input_filtered = np.array(input_filtered)\n",
    "output_filtered = np.array(output_filtered)\n",
    "print(output_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seletion features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "num_features = 10\n",
    "run = False\n",
    "\n",
    "\n",
    "if run:\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFE(estimator, 5, step=1)\n",
    "    output_int = np.argmax(output_filtered[:,1], axis = 1)\n",
    "    selector = selector.fit(input_filtered, output_int)\n",
    "    print(\"selection support: \", selector.support_) \n",
    "    print(\"selection ranking: \", selector.ranking_)\n",
    "\n",
    "\n",
    "selection_ranking = np.array([1, 66, 11, 73, 4, 1, 7, 47, 8, 24, 1, 23, 27, 28, 40, 41, \n",
    " 6, 78, 30, 77, 16, 84, 10, 67, 3, 54, 76, 68, 56, 65, 31, 9, 55, 81, 21, 61, 38, \n",
    " 14, 17, 39, 51, 70, 62, 79, 43, 44, 57, 22, 69, 53, 34, 83, 35, 15, 80, 42, 18, 72,\n",
    " 12, 60, 25, 52, 1, 58, 59, 64, 75, 74, 6, 33, 20, 45, 5, 29, 49, 48, 13, 32, 82, 19, 63, 36, \n",
    " 1, 2, 46, 37, 50, 71])\n",
    "\n",
    "## Get high-rank features\n",
    "input_filtered = np.array(list(map(lambda x: x[selection_ranking <= num_features], input_filtered)))\n",
    "\n",
    "print(len(input_filtered[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "a>2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training, testing set:  7006 ,  3003\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_filtered, output_filtered, test_size=0.3, random_state=300)\n",
    "print(\"Size training, testing set: \", len(X_train), \", \", len(X_test))\n",
    "\n",
    "print(output_filtered[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Training on keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7006 samples, validate on 3003 samples\n",
      "Epoch 1/20\n",
      "7006/7006 [==============================] - ETA: 2:44 - loss: 1.0984 - acc: 0.593 - ETA: 6s - loss: 1.0763 - acc: 0.5312  - ETA: 2s - loss: 1.0403 - acc: 0.535 - ETA: 1s - loss: 1.0342 - acc: 0.527 - ETA: 0s - loss: 1.0284 - acc: 0.526 - ETA: 0s - loss: 1.0162 - acc: 0.535 - ETA: 0s - loss: 1.0117 - acc: 0.537 - 1s 170us/step - loss: 1.0132 - acc: 0.5345 - val_loss: 1.0050 - val_acc: 0.5305\n",
      "Epoch 2/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 1.0194 - acc: 0.468 - ETA: 0s - loss: 1.0141 - acc: 0.523 - ETA: 0s - loss: 1.0176 - acc: 0.517 - ETA: 0s - loss: 1.0072 - acc: 0.524 - ETA: 0s - loss: 1.0007 - acc: 0.532 - ETA: 0s - loss: 0.9997 - acc: 0.531 - ETA: 0s - loss: 0.9989 - acc: 0.532 - 0s 54us/step - loss: 0.9964 - acc: 0.5345 - val_loss: 0.9930 - val_acc: 0.5305\n",
      "Epoch 3/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 1.0163 - acc: 0.500 - ETA: 0s - loss: 1.0000 - acc: 0.524 - ETA: 0s - loss: 1.0013 - acc: 0.521 - ETA: 0s - loss: 0.9934 - acc: 0.528 - ETA: 0s - loss: 0.9888 - acc: 0.529 - ETA: 0s - loss: 0.9854 - acc: 0.531 - ETA: 0s - loss: 0.9820 - acc: 0.533 - 0s 52us/step - loss: 0.9814 - acc: 0.5345 - val_loss: 0.9723 - val_acc: 0.5305\n",
      "Epoch 4/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9560 - acc: 0.593 - ETA: 0s - loss: 0.9818 - acc: 0.540 - ETA: 0s - loss: 0.9786 - acc: 0.537 - ETA: 0s - loss: 0.9750 - acc: 0.539 - ETA: 0s - loss: 0.9753 - acc: 0.536 - ETA: 0s - loss: 0.9783 - acc: 0.534 - ETA: 0s - loss: 0.9764 - acc: 0.534 - 0s 51us/step - loss: 0.9763 - acc: 0.5345 - val_loss: 0.9667 - val_acc: 0.5305\n",
      "Epoch 5/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9833 - acc: 0.562 - ETA: 0s - loss: 0.9697 - acc: 0.543 - ETA: 0s - loss: 0.9712 - acc: 0.535 - ETA: 0s - loss: 0.9650 - acc: 0.548 - ETA: 0s - loss: 0.9771 - acc: 0.539 - ETA: 0s - loss: 0.9764 - acc: 0.540 - ETA: 0s - loss: 0.9753 - acc: 0.541 - ETA: 0s - loss: 0.9716 - acc: 0.545 - 0s 59us/step - loss: 0.9707 - acc: 0.5462 - val_loss: 0.9657 - val_acc: 0.5478\n",
      "Epoch 6/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9073 - acc: 0.593 - ETA: 0s - loss: 0.9572 - acc: 0.555 - ETA: 0s - loss: 0.9644 - acc: 0.550 - ETA: 0s - loss: 0.9598 - acc: 0.556 - ETA: 0s - loss: 0.9661 - acc: 0.553 - ETA: 0s - loss: 0.9684 - acc: 0.554 - ETA: 0s - loss: 0.9682 - acc: 0.552 - 0s 51us/step - loss: 0.9686 - acc: 0.5518 - val_loss: 0.9723 - val_acc: 0.5538\n",
      "Epoch 7/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 1.0416 - acc: 0.375 - ETA: 0s - loss: 0.9464 - acc: 0.569 - ETA: 0s - loss: 0.9524 - acc: 0.562 - ETA: 0s - loss: 0.9647 - acc: 0.547 - ETA: 0s - loss: 0.9634 - acc: 0.551 - ETA: 0s - loss: 0.9631 - acc: 0.553 - ETA: 0s - loss: 0.9665 - acc: 0.550 - 0s 54us/step - loss: 0.9673 - acc: 0.5498 - val_loss: 0.9594 - val_acc: 0.5534\n",
      "Epoch 8/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 1.0247 - acc: 0.562 - ETA: 0s - loss: 0.9767 - acc: 0.545 - ETA: 0s - loss: 0.9653 - acc: 0.552 - ETA: 0s - loss: 0.9648 - acc: 0.555 - ETA: 0s - loss: 0.9628 - acc: 0.556 - ETA: 0s - loss: 0.9667 - acc: 0.551 - ETA: 0s - loss: 0.9643 - acc: 0.552 - 0s 56us/step - loss: 0.9633 - acc: 0.5528 - val_loss: 0.9580 - val_acc: 0.5548\n",
      "Epoch 9/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9119 - acc: 0.500 - ETA: 0s - loss: 0.9436 - acc: 0.572 - ETA: 0s - loss: 0.9549 - acc: 0.564 - ETA: 0s - loss: 0.9594 - acc: 0.555 - ETA: 0s - loss: 0.9602 - acc: 0.553 - ETA: 0s - loss: 0.9603 - acc: 0.552 - ETA: 0s - loss: 0.9644 - acc: 0.549 - 0s 54us/step - loss: 0.9634 - acc: 0.5515 - val_loss: 0.9553 - val_acc: 0.5594\n",
      "Epoch 10/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 1.0658 - acc: 0.500 - ETA: 0s - loss: 0.9431 - acc: 0.579 - ETA: 0s - loss: 0.9577 - acc: 0.558 - ETA: 0s - loss: 0.9528 - acc: 0.562 - ETA: 0s - loss: 0.9568 - acc: 0.558 - ETA: 0s - loss: 0.9620 - acc: 0.551 - ETA: 0s - loss: 0.9576 - acc: 0.554 - 0s 53us/step - loss: 0.9563 - acc: 0.5552 - val_loss: 0.9472 - val_acc: 0.5558\n",
      "Epoch 11/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9483 - acc: 0.593 - ETA: 0s - loss: 0.9656 - acc: 0.543 - ETA: 0s - loss: 0.9540 - acc: 0.555 - ETA: 0s - loss: 0.9496 - acc: 0.558 - ETA: 0s - loss: 0.9530 - acc: 0.555 - ETA: 0s - loss: 0.9537 - acc: 0.552 - ETA: 0s - loss: 0.9532 - acc: 0.551 - ETA: 0s - loss: 0.9514 - acc: 0.552 - 0s 62us/step - loss: 0.9515 - acc: 0.5520 - val_loss: 0.9434 - val_acc: 0.5594\n",
      "Epoch 12/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.8407 - acc: 0.656 - ETA: 0s - loss: 0.9610 - acc: 0.542 - ETA: 0s - loss: 0.9530 - acc: 0.547 - ETA: 0s - loss: 0.9536 - acc: 0.544 - ETA: 0s - loss: 0.9434 - acc: 0.554 - ETA: 0s - loss: 0.9473 - acc: 0.549 - ETA: 0s - loss: 0.9444 - acc: 0.550 - 0s 53us/step - loss: 0.9429 - acc: 0.5521 - val_loss: 0.9321 - val_acc: 0.5538\n",
      "Epoch 13/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 1.0507 - acc: 0.593 - ETA: 0s - loss: 0.9325 - acc: 0.552 - ETA: 0s - loss: 0.9383 - acc: 0.554 - ETA: 0s - loss: 0.9355 - acc: 0.557 - ETA: 0s - loss: 0.9324 - acc: 0.562 - ETA: 0s - loss: 0.9352 - acc: 0.559 - ETA: 0s - loss: 0.9369 - acc: 0.556 - 0s 56us/step - loss: 0.9363 - acc: 0.5565 - val_loss: 0.9273 - val_acc: 0.5568\n",
      "Epoch 14/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.8846 - acc: 0.593 - ETA: 0s - loss: 0.9388 - acc: 0.542 - ETA: 0s - loss: 0.9258 - acc: 0.560 - ETA: 0s - loss: 0.9299 - acc: 0.559 - ETA: 0s - loss: 0.9402 - acc: 0.554 - ETA: 0s - loss: 0.9370 - acc: 0.557 - 0s 49us/step - loss: 0.9343 - acc: 0.5605 - val_loss: 0.9257 - val_acc: 0.5541\n",
      "Epoch 15/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9870 - acc: 0.468 - ETA: 0s - loss: 0.9151 - acc: 0.573 - ETA: 0s - loss: 0.9196 - acc: 0.563 - ETA: 0s - loss: 0.9282 - acc: 0.560 - ETA: 0s - loss: 0.9282 - acc: 0.563 - ETA: 0s - loss: 0.9291 - acc: 0.560 - ETA: 0s - loss: 0.9279 - acc: 0.560 - 0s 52us/step - loss: 0.9289 - acc: 0.5595 - val_loss: 0.9230 - val_acc: 0.5571\n",
      "Epoch 16/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.8058 - acc: 0.687 - ETA: 0s - loss: 0.9005 - acc: 0.577 - ETA: 0s - loss: 0.9038 - acc: 0.578 - ETA: 0s - loss: 0.9103 - acc: 0.571 - ETA: 0s - loss: 0.9161 - acc: 0.570 - ETA: 0s - loss: 0.9217 - acc: 0.564 - ETA: 0s - loss: 0.9251 - acc: 0.561 - 0s 51us/step - loss: 0.9253 - acc: 0.5604 - val_loss: 0.9278 - val_acc: 0.5661\n",
      "Epoch 17/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7922 - acc: 0.687 - ETA: 0s - loss: 0.9232 - acc: 0.576 - ETA: 0s - loss: 0.9277 - acc: 0.567 - ETA: 0s - loss: 0.9228 - acc: 0.571 - ETA: 0s - loss: 0.9214 - acc: 0.568 - ETA: 0s - loss: 0.9184 - acc: 0.568 - ETA: 0s - loss: 0.9221 - acc: 0.566 - 0s 53us/step - loss: 0.9220 - acc: 0.5647 - val_loss: 0.9254 - val_acc: 0.5611\n",
      "Epoch 18/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7626 - acc: 0.718 - ETA: 0s - loss: 0.9156 - acc: 0.587 - ETA: 0s - loss: 0.9173 - acc: 0.581 - ETA: 0s - loss: 0.9216 - acc: 0.571 - ETA: 0s - loss: 0.9209 - acc: 0.573 - ETA: 0s - loss: 0.9221 - acc: 0.569 - ETA: 0s - loss: 0.9215 - acc: 0.568 - 0s 56us/step - loss: 0.9235 - acc: 0.5664 - val_loss: 0.9324 - val_acc: 0.5608\n",
      "Epoch 19/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.8537 - acc: 0.562 - ETA: 0s - loss: 0.9285 - acc: 0.551 - ETA: 0s - loss: 0.9129 - acc: 0.563 - ETA: 0s - loss: 0.9197 - acc: 0.564 - ETA: 0s - loss: 0.9199 - acc: 0.567 - ETA: 0s - loss: 0.9203 - acc: 0.567 - ETA: 0s - loss: 0.9195 - acc: 0.569 - 0s 53us/step - loss: 0.9188 - acc: 0.5701 - val_loss: 0.9161 - val_acc: 0.5641\n",
      "Epoch 20/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 1.0162 - acc: 0.437 - ETA: 0s - loss: 0.8984 - acc: 0.586 - ETA: 0s - loss: 0.9031 - acc: 0.582 - ETA: 0s - loss: 0.9030 - acc: 0.584 - ETA: 0s - loss: 0.9087 - acc: 0.581 - ETA: 0s - loss: 0.9115 - acc: 0.579 - ETA: 0s - loss: 0.9158 - acc: 0.573 - 0s 53us/step - loss: 0.9160 - acc: 0.5734 - val_loss: 0.9200 - val_acc: 0.5641\n",
      "Train on 7006 samples, validate on 3003 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7006/7006 [==============================] - ETA: 2:03 - loss: 1.0987 - acc: 0.312 - ETA: 3s - loss: 1.0663 - acc: 0.5091  - ETA: 1s - loss: 1.0353 - acc: 0.512 - ETA: 0s - loss: 1.0227 - acc: 0.513 - ETA: 0s - loss: 1.0159 - acc: 0.518 - ETA: 0s - loss: 1.0118 - acc: 0.520 - ETA: 0s - loss: 1.0038 - acc: 0.520 - 1s 145us/step - loss: 0.9929 - acc: 0.5224 - val_loss: 0.8642 - val_acc: 0.5688\n",
      "Epoch 2/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.8882 - acc: 0.562 - ETA: 0s - loss: 0.8280 - acc: 0.574 - ETA: 0s - loss: 0.8281 - acc: 0.583 - ETA: 0s - loss: 0.8258 - acc: 0.596 - ETA: 0s - loss: 0.8208 - acc: 0.603 - ETA: 0s - loss: 0.8115 - acc: 0.607 - ETA: 0s - loss: 0.8100 - acc: 0.609 - 0s 57us/step - loss: 0.8051 - acc: 0.6125 - val_loss: 0.7765 - val_acc: 0.6354\n",
      "Epoch 3/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7865 - acc: 0.656 - ETA: 0s - loss: 0.7877 - acc: 0.636 - ETA: 0s - loss: 0.7849 - acc: 0.629 - ETA: 0s - loss: 0.7705 - acc: 0.635 - ETA: 0s - loss: 0.7724 - acc: 0.631 - ETA: 0s - loss: 0.7760 - acc: 0.635 - ETA: 0s - loss: 0.7769 - acc: 0.635 - ETA: 0s - loss: 0.7806 - acc: 0.633 - 0s 62us/step - loss: 0.7811 - acc: 0.6317 - val_loss: 0.8011 - val_acc: 0.6190\n",
      "Epoch 4/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7598 - acc: 0.593 - ETA: 0s - loss: 0.7684 - acc: 0.634 - ETA: 0s - loss: 0.7695 - acc: 0.635 - ETA: 0s - loss: 0.7689 - acc: 0.636 - ETA: 0s - loss: 0.7741 - acc: 0.635 - ETA: 0s - loss: 0.7736 - acc: 0.636 - ETA: 0s - loss: 0.7742 - acc: 0.634 - ETA: 0s - loss: 0.7738 - acc: 0.634 - 0s 63us/step - loss: 0.7758 - acc: 0.6333 - val_loss: 0.7723 - val_acc: 0.6490\n",
      "Epoch 5/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7582 - acc: 0.593 - ETA: 0s - loss: 0.8085 - acc: 0.595 - ETA: 0s - loss: 0.7824 - acc: 0.619 - ETA: 0s - loss: 0.7815 - acc: 0.625 - ETA: 0s - loss: 0.7805 - acc: 0.631 - ETA: 0s - loss: 0.7712 - acc: 0.635 - ETA: 0s - loss: 0.7762 - acc: 0.633 - 0s 57us/step - loss: 0.7779 - acc: 0.6342 - val_loss: 0.7630 - val_acc: 0.6434\n",
      "Epoch 6/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.6510 - acc: 0.781 - ETA: 0s - loss: 0.7694 - acc: 0.647 - ETA: 0s - loss: 0.7533 - acc: 0.649 - ETA: 0s - loss: 0.7633 - acc: 0.642 - ETA: 0s - loss: 0.7698 - acc: 0.642 - ETA: 0s - loss: 0.7688 - acc: 0.644 - ETA: 0s - loss: 0.7679 - acc: 0.642 - 0s 57us/step - loss: 0.7675 - acc: 0.6397 - val_loss: 0.7962 - val_acc: 0.6187\n",
      "Epoch 7/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.593 - ETA: 0s - loss: 0.7723 - acc: 0.615 - ETA: 0s - loss: 0.7755 - acc: 0.610 - ETA: 0s - loss: 0.7664 - acc: 0.623 - ETA: 0s - loss: 0.7684 - acc: 0.625 - ETA: 0s - loss: 0.7625 - acc: 0.628 - ETA: 0s - loss: 0.7659 - acc: 0.628 - ETA: 0s - loss: 0.7652 - acc: 0.632 - 0s 64us/step - loss: 0.7660 - acc: 0.6356 - val_loss: 0.7643 - val_acc: 0.6447\n",
      "Epoch 8/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9098 - acc: 0.531 - ETA: 0s - loss: 0.7434 - acc: 0.666 - ETA: 0s - loss: 0.7388 - acc: 0.669 - ETA: 0s - loss: 0.7590 - acc: 0.657 - ETA: 0s - loss: 0.7580 - acc: 0.649 - ETA: 0s - loss: 0.7580 - acc: 0.643 - ETA: 0s - loss: 0.7603 - acc: 0.637 - ETA: 0s - loss: 0.7643 - acc: 0.636 - 0s 61us/step - loss: 0.7626 - acc: 0.6377 - val_loss: 0.7668 - val_acc: 0.6390\n",
      "Epoch 9/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.9964 - acc: 0.468 - ETA: 0s - loss: 0.7559 - acc: 0.636 - ETA: 0s - loss: 0.7559 - acc: 0.645 - ETA: 0s - loss: 0.7538 - acc: 0.649 - ETA: 0s - loss: 0.7544 - acc: 0.645 - ETA: 0s - loss: 0.7590 - acc: 0.643 - ETA: 0s - loss: 0.7603 - acc: 0.643 - ETA: 0s - loss: 0.7593 - acc: 0.644 - 0s 63us/step - loss: 0.7598 - acc: 0.6447 - val_loss: 0.7842 - val_acc: 0.6367\n",
      "Epoch 10/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7213 - acc: 0.718 - ETA: 0s - loss: 0.7641 - acc: 0.645 - ETA: 0s - loss: 0.7595 - acc: 0.647 - ETA: 0s - loss: 0.7717 - acc: 0.639 - ETA: 0s - loss: 0.7690 - acc: 0.640 - ETA: 0s - loss: 0.7631 - acc: 0.644 - ETA: 0s - loss: 0.7637 - acc: 0.646 - 0s 57us/step - loss: 0.7646 - acc: 0.6447 - val_loss: 0.7586 - val_acc: 0.6510\n",
      "Epoch 11/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.6117 - acc: 0.750 - ETA: 0s - loss: 0.7744 - acc: 0.656 - ETA: 0s - loss: 0.7652 - acc: 0.645 - ETA: 0s - loss: 0.7654 - acc: 0.643 - ETA: 0s - loss: 0.7712 - acc: 0.633 - ETA: 0s - loss: 0.7663 - acc: 0.637 - ETA: 0s - loss: 0.7658 - acc: 0.634 - ETA: 0s - loss: 0.7615 - acc: 0.638 - 0s 64us/step - loss: 0.7589 - acc: 0.6406 - val_loss: 0.7501 - val_acc: 0.6494\n",
      "Epoch 12/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.6691 - acc: 0.687 - ETA: 0s - loss: 0.7389 - acc: 0.663 - ETA: 0s - loss: 0.7458 - acc: 0.651 - ETA: 0s - loss: 0.7494 - acc: 0.645 - ETA: 0s - loss: 0.7536 - acc: 0.643 - ETA: 0s - loss: 0.7477 - acc: 0.651 - ETA: 0s - loss: 0.7521 - acc: 0.646 - ETA: 0s - loss: 0.7529 - acc: 0.647 - 0s 63us/step - loss: 0.7523 - acc: 0.6463 - val_loss: 0.7498 - val_acc: 0.6517\n",
      "Epoch 13/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7627 - acc: 0.625 - ETA: 0s - loss: 0.7544 - acc: 0.628 - ETA: 0s - loss: 0.7663 - acc: 0.626 - ETA: 0s - loss: 0.7679 - acc: 0.632 - ETA: 0s - loss: 0.7543 - acc: 0.635 - ETA: 0s - loss: 0.7522 - acc: 0.637 - ETA: 0s - loss: 0.7478 - acc: 0.639 - ETA: 0s - loss: 0.7493 - acc: 0.639 - ETA: 0s - loss: 0.7519 - acc: 0.639 - 0s 69us/step - loss: 0.7531 - acc: 0.6393 - val_loss: 0.8068 - val_acc: 0.6317\n",
      "Epoch 14/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.6772 - acc: 0.718 - ETA: 0s - loss: 0.7479 - acc: 0.654 - ETA: 0s - loss: 0.7503 - acc: 0.644 - ETA: 0s - loss: 0.7517 - acc: 0.638 - ETA: 0s - loss: 0.7499 - acc: 0.642 - ETA: 0s - loss: 0.7544 - acc: 0.640 - ETA: 0s - loss: 0.7511 - acc: 0.644 - ETA: 0s - loss: 0.7512 - acc: 0.645 - 0s 64us/step - loss: 0.7534 - acc: 0.6420 - val_loss: 0.7588 - val_acc: 0.6474\n",
      "Epoch 15/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7422 - acc: 0.687 - ETA: 0s - loss: 0.7264 - acc: 0.641 - ETA: 0s - loss: 0.7464 - acc: 0.643 - ETA: 0s - loss: 0.7514 - acc: 0.639 - ETA: 0s - loss: 0.7541 - acc: 0.639 - ETA: 0s - loss: 0.7563 - acc: 0.640 - ETA: 0s - loss: 0.7540 - acc: 0.642 - ETA: 0s - loss: 0.7518 - acc: 0.641 - 0s 65us/step - loss: 0.7529 - acc: 0.6432 - val_loss: 0.7577 - val_acc: 0.6487\n",
      "Epoch 16/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.8791 - acc: 0.562 - ETA: 0s - loss: 0.7058 - acc: 0.672 - ETA: 0s - loss: 0.7425 - acc: 0.653 - ETA: 0s - loss: 0.7450 - acc: 0.652 - ETA: 0s - loss: 0.7380 - acc: 0.657 - ETA: 0s - loss: 0.7429 - acc: 0.652 - ETA: 0s - loss: 0.7499 - acc: 0.647 - ETA: 0s - loss: 0.7524 - acc: 0.643 - 0s 63us/step - loss: 0.7535 - acc: 0.6434 - val_loss: 0.7523 - val_acc: 0.6543\n",
      "Epoch 17/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7422 - acc: 0.593 - ETA: 0s - loss: 0.7385 - acc: 0.623 - ETA: 0s - loss: 0.7354 - acc: 0.648 - ETA: 0s - loss: 0.7505 - acc: 0.644 - ETA: 0s - loss: 0.7575 - acc: 0.639 - ETA: 0s - loss: 0.7523 - acc: 0.641 - ETA: 0s - loss: 0.7500 - acc: 0.645 - ETA: 0s - loss: 0.7527 - acc: 0.643 - ETA: 0s - loss: 0.7513 - acc: 0.644 - 1s 73us/step - loss: 0.7498 - acc: 0.6467 - val_loss: 0.7612 - val_acc: 0.6470\n",
      "Epoch 18/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.6813 - acc: 0.593 - ETA: 0s - loss: 0.7739 - acc: 0.635 - ETA: 0s - loss: 0.7758 - acc: 0.635 - ETA: 0s - loss: 0.7684 - acc: 0.635 - ETA: 0s - loss: 0.7561 - acc: 0.641 - ETA: 0s - loss: 0.7542 - acc: 0.637 - ETA: 0s - loss: 0.7518 - acc: 0.637 - ETA: 0s - loss: 0.7570 - acc: 0.636 - ETA: 0s - loss: 0.7553 - acc: 0.640 - 0s 69us/step - loss: 0.7533 - acc: 0.6422 - val_loss: 0.7546 - val_acc: 0.6487\n",
      "Epoch 19/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.7887 - acc: 0.468 - ETA: 0s - loss: 0.7417 - acc: 0.659 - ETA: 0s - loss: 0.7541 - acc: 0.649 - ETA: 0s - loss: 0.7429 - acc: 0.650 - ETA: 0s - loss: 0.7432 - acc: 0.654 - ETA: 0s - loss: 0.7541 - acc: 0.643 - ETA: 0s - loss: 0.7559 - acc: 0.642 - ETA: 0s - loss: 0.7526 - acc: 0.644 - 0s 65us/step - loss: 0.7499 - acc: 0.6434 - val_loss: 0.7938 - val_acc: 0.6294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "7006/7006 [==============================] - ETA: 0s - loss: 0.8641 - acc: 0.500 - ETA: 0s - loss: 0.7767 - acc: 0.634 - ETA: 0s - loss: 0.7497 - acc: 0.644 - ETA: 0s - loss: 0.7503 - acc: 0.645 - ETA: 0s - loss: 0.7481 - acc: 0.649 - ETA: 0s - loss: 0.7475 - acc: 0.649 - ETA: 0s - loss: 0.7493 - acc: 0.646 - 0s 57us/step - loss: 0.7495 - acc: 0.6460 - val_loss: 0.7543 - val_acc: 0.6450\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(15,), kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(32, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(16,kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(8, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(3),\n",
    "        Activation('softmax'),\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    model.fit(X_train, y_train[:,0], validation_data=(X_test, y_test[:,0]), epochs = 20)\n",
    "    #model.fit(X_train[0:2], y_train[0:2,0], validation_data=(X_test[0:2], y_test[0:2,0]), epochs = 1000)\n",
    "\n",
    "    model1 = Sequential([\n",
    "        Dense(64, input_shape=(15,), kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(64, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(64, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(64, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(64,kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(3),\n",
    "        Activation('softmax'),\n",
    "    ])\n",
    "   \n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    model1.fit(X_train, y_train[:,1], validation_data=(X_test, y_test[:,1]), epochs = 20)\n",
    "    \n",
    "    \n",
    "#     y1 = y_train[:,1]\n",
    "#     y1_int = np.argmax(y1, axis=1)\n",
    "    \n",
    "    \n",
    "#     from sklearn.linear_model import LogisticRegression\n",
    "#     from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "#     from imblearn.combine import SMOTEENN\n",
    "#     from imblearn.over_sampling import SMOTE\n",
    "#     from collections import Counter\n",
    "\n",
    "\n",
    "#     sm = SMOTE(random_state=0)\n",
    "#     x, y = sm.fit_sample(X_train, y1_int)\n",
    "#     print(\"label sizes before sampling: \", sorted(Counter(y1_int).items()))\n",
    "#     print(\"label sizes after sampling: \", sorted(Counter(y).items()))\n",
    "#     y = to_categorical(y, num_classes=3)\n",
    "#     print(\"len after down sampling: \", len(x))\n",
    "    \n",
    "    \n",
    "\n",
    "    #model1 = RandomForestClassifier(n_estimators = 300, max_features = 88, max_depth = 5)\n",
    "     \n",
    "#     model1 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(300,), random_state=30)\n",
    "#     sm = SMOTE(random_state=42)\n",
    "#     print(len(y_train[:,1]), len(X_train))\n",
    "#     x, y = sm.fit_sample(X_train, y_train[:,1])\n",
    "#     model1.fit(x, y)\n",
    "    \n",
    "    \n",
    "#     model2 = Sequential([\n",
    "#         Dense(64, input_shape=(88,), kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(32, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(16,kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(8, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(3),\n",
    "#         Activation('softmax'),\n",
    "#     ])\n",
    "#     model2.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "#     model2.fit(X_train, y_train[:,2], validation_data=(X_test, y_test[:,2]), epochs = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1436   73   84]\n",
      " [ 657   95   80]\n",
      " [ 375   40  163]] \n",
      "\n",
      "[[ 527  423    6]\n",
      " [ 224 1153  183]\n",
      " [   7  223  257]]\n",
      "[0.43983945 0.5091007  0.05105985] ,  1\n",
      "[0.7997398  0.19663186 0.00362838] ,  0\n",
      "[0.10352632 0.5638604  0.33261326] ,  1\n",
      "[0.46450007 0.49370843 0.04179144] ,  0\n",
      "[0.1736988  0.71604    0.11026114] ,  2\n",
      "[0.8196814  0.17729211 0.00302644] ,  0\n",
      "[0.1916399  0.6959412  0.11241885] ,  0\n",
      "[0.40433472 0.52628624 0.06937908] ,  1\n",
      "[0.24666813 0.6572551  0.09607679] ,  1\n",
      "[0.6426845  0.34341547 0.01390003] ,  0\n",
      "[0.00471046 0.2328426  0.762447  ] ,  2\n",
      "[0.27716175 0.6565145  0.06632364] ,  1\n",
      "[6.0217655e-05 4.0666975e-02 9.5927286e-01] ,  1\n",
      "[0.21773218 0.6257098  0.156558  ] ,  2\n",
      "[0.36837947 0.54631364 0.08530692] ,  1\n",
      "[0.01820715 0.36905456 0.6127383 ] ,  1\n",
      "[0.2399099  0.63259476 0.12749523] ,  0\n",
      "[0.868661   0.13045557 0.00088342] ,  1\n",
      "[0.01663009 0.3685368  0.6148332 ] ,  2\n",
      "[0.68370116 0.3010903  0.01520845] ,  0\n",
      "Log_loss: 0.7543448960358986\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-13b33f5cfc0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Log_loss:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0my_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "pre = model.predict_classes(X_test)\n",
    "y_int = np.argmax(y_test[:,0], axis=1)\n",
    "print(confusion_matrix(y_int, pre), \"\\n\")\n",
    "\n",
    "\n",
    "pre = model1.predict_classes(X_test)\n",
    "#pre = model1.predict(X_test)\n",
    "#pre = np.argmax(pre, axis =1)\n",
    "y_int = np.argmax(y_test[:,1], axis=1)\n",
    "print(confusion_matrix(y_int, pre))\n",
    "prob =  model1.predict(X_test)\n",
    "for i in range(0,20):\n",
    "    print(prob[i],\", \" ,y_int[i])\n",
    "print(\"Log_loss:\",log_loss(y_int, prob))\n",
    "\n",
    "pre = model2.predict_classes(X_test)\n",
    "y_int = np.argmax(y_test[:,2], axis=1)\n",
    "print(\"\\n\",confusion_matrix(y_int, pre))\n",
    "\n",
    "\n",
    "# [[1547    0   52]\n",
    "#  [ 769    0   41]\n",
    "#  [ 494    1   99]] \n",
    "\n",
    "# [[ 592  306    4]\n",
    "#  [ 376 1081  116]\n",
    "#  [  17  272  239]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard residual plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing around with prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict = model1.predict_classes(X_test)\n",
    "\n",
    "\n",
    "#subs = np.abs(predict - y_test[:,0])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "subplots_adjust(right = 2, wspace=0.2)\n",
    "axs[0].hist(y_train[:,1])\n",
    "axs[0].set_title(\"True value in train data\\n\" + \n",
    "                 \"avg:\" + str(np.average(y_train[:,0])) + \n",
    "                 \"\\nstd:\" + str(np.std(y_train[:,0])))\n",
    "axs[1].hist(y_test[:,0])\n",
    "axs[1].set_title(\"True value in test data\\n\" + \n",
    "                 \"avg:\" + str(np.average(y_test[:,0])) + \n",
    "                 \"\\nstd:\" + str(np.std(y_test[:,0])))\n",
    "axs[1].set_ylim([0,400])\n",
    "                        \n",
    "axs[2].hist(predict) \n",
    "axs[2].set_title(\"Prediction\\n\" +\n",
    "                \"avg:\" + str(np.average(predict)) + \n",
    "                 \"\\nstd:\" + str(np.std(predict)))\n",
    "                \n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
