{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries and Define constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Training based on features of audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#Constant\n",
    "EMOTION_ANNOTATORS = {'anger': 0, 'happiness' : 1, 'sadness' : 2, 'neutral' : 3, 'frustration' : 4, 'excited': 5,\n",
    "           'fear' : 6,'surprise' : 7,'disgust' : 8, 'other' : 9}\n",
    "\n",
    "EMOTION = {'ang': 0, 'hap' : 1, 'sad' : 2, 'neu' : 3, 'fru' : 4, 'exc': 5,\n",
    "           'fea' : 6,'sur' : 7,'dis' : 8, 'oth' : 9, 'xxx':10}\n",
    "\n",
    "#EMOTION = {'ang': 0, 'hap' : 1, 'sad' : 2}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output: 7513 ,  7513\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##Loading  data from files\n",
    "filehandlerInput = open('processed-data/input.obj', 'rb')\n",
    "filehandlerOutput = open('processed-data/output.obj', 'rb')\n",
    "input = pickle.load(filehandlerInput)\n",
    "output = pickle.load(filehandlerOutput)\n",
    "print(\"Size input, output:\", len(input),\", \", len(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nan values in each features in all sample: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0]\n",
      "Size filteres input, output:  7506 ,  7506\n",
      "{'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'fru': 4, 'exc': 5, 'fea': 6, 'sur': 7, 'dis': 8, 'oth': 9, 'xxx': 10}\n"
     ]
    }
   ],
   "source": [
    "feature_name= ['energy', \n",
    "               'f0', 'intensity', 'f1', 'f2', 'f3','f1-bw','f2-bw','f3-bw' ,\n",
    "               'f2-f1', 'f3-f1', \n",
    "               'jitter', 'shimmer', 'duration',\n",
    "              'unvoiced_percent', 'breaks_degree', 'max_dur_pause', 'average_dur_pause']\n",
    "\n",
    "num_feas = len(input[0])\n",
    "\n",
    "numNan = [np.count_nonzero(np.isnan(input[:,i]))   for i in range (0, num_feas)]\n",
    "print(\"Number of Nan values in each features in all sample:\", numNan)\n",
    "\n",
    "# index_fea_contain_Nan = [i for i in range(0, len(numNan)) if numNan[i] != 0]\n",
    "# print(\"Index of features containing Nan values: \", index_fea_contain_Nan)\n",
    "\n",
    "# fea_contain_Nan = [feature_name[index] for index in index_fea_contain_Nan]\n",
    "# print(\"Name of features containing Nan values: \", fea_contain_Nan)\n",
    "\n",
    "# Filter samples containing Nan values\n",
    "input_filtered = input[~np.any(np.isnan(input), axis=1)]\n",
    "output_filtered = output[~np.any(np.isnan(input), axis=1)]\n",
    "print(\"Size filteres input, output: \", len(input_filtered), \", \", len(output_filtered))\n",
    "\n",
    "#Normalize input\n",
    "input_filtered = (input_filtered - input_filtered.min(axis=0)) / (input_filtered.max(axis=0) - input_filtered.min(axis=0))\n",
    "\n",
    "print(EMOTION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exc -> hap. Positive\n",
    "output_filtered[output_filtered == 5] = 1\n",
    "\n",
    "#sad -> ang. Negative\n",
    "#output_filtered[output_filtered == 2] = 0\n",
    "#output_filtered[output_filtered == 4] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION_ANNOTATE:  {'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'fru': 4, 'exc': 5, 'fea': 6, 'sur': 7, 'dis': 8, 'oth': 9, 'xxx': 10}\n",
      "\n",
      "The quantity of each label:  [(0, 1102), (1, 1633), (2, 1076)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def printQuantitySample(output):\n",
    "    y = np.bincount(output)\n",
    "    ii = np.nonzero(y)[0]\n",
    "    a = list(zip(ii, y[ii]))\n",
    "    print(\"EMOTION_ANNOTATE: \", EMOTION)\n",
    "    print(\"\\nThe quantity of each label: \", a, \"\\n\")\n",
    "    \n",
    "def filterLabels(input, output, labels=['ang','hap','sad']):\n",
    "    labels_int = [EMOTION[l] for l in labels]\n",
    "    condition = [out in labels_int for out in output]\n",
    "    input = input[condition]\n",
    "    output = output[condition]\n",
    "    return input, output\n",
    "    \n",
    "    \n",
    "# Remove labels that have small quantity.\n",
    "input_filtered, output_filtered = filterLabels(input_filtered, output_filtered, ['ang', 'sad', 'hap' ])\n",
    "printQuantitySample(output_filtered)\n",
    "\n",
    "#Shuffer\n",
    "c = list(zip(input_filtered, output_filtered))\n",
    "random.shuffle(c)\n",
    "input_filtered, output_filtered = zip( * c)\n",
    "input_filtered = np.array(input_filtered)\n",
    "output_filtered = np.array(output_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training, testing set:  3048 ,  763\n",
      "[1 1 1 ... 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_filtered, output_filtered, test_size=0.2, random_state=300)\n",
    "print(\"Size training, testing set: \", len(X_train), \", \", len(X_test))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search best parameters for RandomForest model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 466, 733, 1000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 15, 20, 25, 30, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 4)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 30, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "#Tuninng parameter\n",
    "# rf = RandomForestRegressor()\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# rf_random.fit(X_train, y_train)\n",
    "# print(rf_random.best_params_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search model playing around**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss:  [-0.63353026 -0.74091926 -0.65012047 -0.65834728 -0.76786972]\n",
      "Accuracy:  [0.71214953 0.71535581 0.70356473 0.69230769 0.68667917]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf =  MLPClassifier(solver='lbfgs', alpha=1e-7,\n",
    "                 hidden_layer_sizes=(300), random_state=200)\n",
    "\n",
    "log_loss = cross_val_score(clf, X_train, y_train, scoring= 'neg_log_loss', cv = 5) \n",
    "accs = cross_val_score(clf, X_train, y_train, scoring= 'accuracy', cv = 5) \n",
    "print(\"Log-loss: \", log_loss)\n",
    "print(\"Accuracy: \",  accs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of training set:  0.8039381153305204\n",
      "Score of validation set:  0.7016393442622951\n",
      "Score of training set:  0.8011299435028248\n",
      "Score of validation set:  0.7245901639344262\n",
      "Score of training set:  0.8090395480225989\n",
      "Score of validation set:  0.6786885245901639\n",
      "Score of training set:  0.8020477815699659\n",
      "Score of validation set:  0.6688524590163935\n",
      "Score of training set:  0.8028248587570621\n",
      "Score of validation set:  0.6786885245901639\n",
      "Score of training set:  0.8159577425632472\n",
      "Score of validation set:  0.7311475409836066\n",
      "Score of training set:  0.8154960981047937\n",
      "Score of validation set:  0.7180327868852459\n",
      "Score of training set:  0.8168067226890756\n",
      "Score of validation set:  0.6983606557377049\n",
      "Score of training set:  0.806325216904562\n",
      "Score of validation set:  0.7105263157894737\n",
      "Score of training set:  0.8009531819456126\n",
      "Score of validation set:  0.7203947368421053\n",
      "Average accuracy training set, std: 0.8074519209390264   0.006111647806578266\n",
      "Average accuracy validation set, std: 0.7030921052631578   0.020530627162130843\n",
      "EMOTION_ANNOTATE:  {'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'fru': 4, 'exc': 5, 'fea': 6, 'sur': 7, 'dis': 8, 'oth': 9, 'xxx': 10}\n",
      "\n",
      "The quantity of each label:  [(0, 1318), (1, 1318), (2, 1318)] \n",
      "\n",
      "Saved model into file\n"
     ]
    }
   ],
   "source": [
    "def training(X_train, y_train):    \n",
    "    sm = SMOTE(random_state=42)\n",
    "    kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "    i_fold = 0\n",
    "    accuracy_train_results = []\n",
    "    accuracy_valid_results = []\n",
    "\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        x_train_sub, x_valid_sub = X_train[train_index], X_train[valid_index]\n",
    "        y_train_sub, y_valid_sub = y_train[train_index], y_train[valid_index]\n",
    "        \n",
    "        #clf = RandomForestClassifier(n_estimators = 300)\n",
    "        clf = MLPClassifier(solver='lbfgs', alpha=1e-7,\n",
    "                 hidden_layer_sizes=(300), random_state=200)\n",
    "        \n",
    "       #print(clf.get_params())\n",
    "        #Upsampling train data\n",
    "        x_train_sub, y_train_sub = sm.fit_sample(x_train_sub, y_train_sub)\n",
    "        clf.fit(x_train_sub, y_train_sub)\n",
    "        \n",
    "        score = clf.score(x_train_sub, y_train_sub)\n",
    "        score1 = clf.score(x_valid_sub, y_valid_sub)\n",
    "        accuracy_train_results.append(score)\n",
    "        accuracy_valid_results.append(score1)\n",
    "        \n",
    "        print(\"Score of training set: \", score)\n",
    "        print(\"Score of validation set: \", score1)\n",
    "     \n",
    "       \n",
    "    \n",
    "    avg_accuracy_train_result = np.sum(accuracy_train_results) / len(accuracy_train_results)\n",
    "    avg_accuracy_valid_result = np.sum(accuracy_valid_results) / len(accuracy_valid_results)\n",
    "    print(\"Average accuracy training set, std:\", avg_accuracy_train_result, \" \",\\\n",
    "          np.std(accuracy_train_results))\n",
    "    print(\"Average accuracy validation set, std:\", avg_accuracy_valid_result,\" \", \\\n",
    "          np.std(accuracy_valid_results))     \n",
    "    \n",
    "    \n",
    "    #Upsampling data\n",
    "    X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    printQuantitySample(y_train)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return clf\n",
    "   \n",
    "# Training \n",
    "clf = training(X_train, y_train)\n",
    "\n",
    "#Save model  into files\n",
    "filename = 'model/model1.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "print(\"Saved model into file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score for test set:  0.7339449541284404\n",
      "\n",
      "Confusion matrix:..................... \n",
      " [[153  45   9]\n",
      " [ 65 204  46]\n",
      " [ 11  27 203]]\n",
      "\n",
      "TP:  [153, 204, 203] \n",
      "\n",
      "FP:  [76, 72, 55] \n",
      "\n",
      "FN:  [54, 111, 38] \n",
      "\n",
      "\n",
      "Precision:  [0.6681222707423581, 0.7391304347826086, 0.7868217054263565] \n",
      "\n",
      "Recall:  [0.7391304347826086, 0.6476190476190476, 0.8423236514522822] \n",
      "\n",
      "F1_scrore:  [0.7018348623853211, 0.6903553299492385, 0.8136272545090181] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(clf, X_test, y_test):\n",
    "    predicts = clf.predict(X_test)\n",
    "   # pro = clf.predict_proba(X_test)\n",
    "\n",
    "    score_test = clf.score(X_test, y_test)\n",
    "    print(\"\\nScore for test set: \", score_test)\n",
    "    \n",
    "    matrix = confusion_matrix(y_test, predicts)\n",
    "    print (\"\\nConfusion matrix:..................... \\n\",matrix)\n",
    "    \n",
    "    sum_colum = np.sum(matrix, axis = 0)\n",
    "    sum_row = np.sum(matrix, axis = 1)\n",
    "\n",
    "    TP = [matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"\\nTP: \", TP,\"\\n\")   \n",
    "    FP = [sum_colum[i] - matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"FP: \", FP,\"\\n\")\n",
    "    FN = [sum_row[i] - matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"FN: \", FN,\"\\n\")\n",
    "    Presision = [TP[i] /(TP[i] + FP[i])  for i in range(0, len(matrix))]\n",
    "    Recall = [TP[i] /(TP[i] + FN[i])  for i in range(0, len(matrix))]\n",
    "    F1_score = [2 * Presision[i] * Recall[i] /(Presision[i] + Recall[i])  for i in range(0, len(matrix))]\n",
    "    \n",
    "    print(\"\\nPrecision: \", Presision,\"\\n\")\n",
    "    print(\"Recall: \", Recall,\"\\n\")\n",
    "    print(\"F1_scrore: \", F1_score, \"\\n\")\n",
    "\n",
    "test(clf, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of cositent prediction:  0.7378768020969856\n",
      "[[9.90556762e-01 9.44282693e-03 4.11136213e-07]\n",
      " [7.56307833e-02 6.14036932e-01 3.10332285e-01]\n",
      " [2.98540608e-03 1.71411268e-02 9.79873467e-01]\n",
      " [9.70173380e-01 2.94957693e-02 3.30850611e-04]]\n",
      "[1 2 2 0]\n",
      "[0 1 2 0]\n",
      "Log loss value: 0.6819651252836908\n",
      "Prediciton accuracy based on log loss:  0.5056224040701159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "predicts = clf.predict(X_test)\n",
    "pro = clf.predict_proba(X_test)\n",
    "\n",
    "theshold_consitent = 0.70\n",
    "consitent_predictions = pro[[np.any(p> theshold_consitent)  for p in pro]]\n",
    "print(\"Ratio of cositent prediction: \", len(consitent_predictions) / len (pro))\n",
    "print(pro[0:4])\n",
    "print(y_test[0:4])\n",
    "print(predicts[0:4])\n",
    "    \n",
    "log_loss_value = log_loss(y_test, pro, labels=[0,1,2])\n",
    "print(\"Log loss value:\", log_loss_value)\n",
    "print(\"Prediciton accuracy based on log loss: \", np.exp(-log_loss_value))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Training on keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Train on 2668 samples, validate on 667 samples\n",
      "Epoch 1/100\n",
      "2668/2668 [==============================] - 1s 285us/step - loss: 0.6596 - acc: 0.5971 - val_loss: 0.6125 - val_acc: 0.6582\n",
      "Epoch 2/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.6016 - acc: 0.6724 - val_loss: 0.6131 - val_acc: 0.6507\n",
      "Epoch 3/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.5873 - acc: 0.6852 - val_loss: 0.5869 - val_acc: 0.6807\n",
      "Epoch 4/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.5729 - acc: 0.7069 - val_loss: 0.5787 - val_acc: 0.6732\n",
      "Epoch 5/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.5653 - acc: 0.7076 - val_loss: 0.5774 - val_acc: 0.6837\n",
      "Epoch 6/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.5593 - acc: 0.7151 - val_loss: 0.6262 - val_acc: 0.6597\n",
      "Epoch 7/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.5571 - acc: 0.7121 - val_loss: 0.5827 - val_acc: 0.6822\n",
      "Epoch 8/100\n",
      "2668/2668 [==============================] - 0s 108us/step - loss: 0.5525 - acc: 0.7166 - val_loss: 0.5808 - val_acc: 0.6777\n",
      "Epoch 9/100\n",
      "2668/2668 [==============================] - 0s 108us/step - loss: 0.5502 - acc: 0.7208 - val_loss: 0.5742 - val_acc: 0.6987\n",
      "Epoch 10/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.5443 - acc: 0.7264 - val_loss: 0.5682 - val_acc: 0.7031\n",
      "Epoch 11/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.5428 - acc: 0.7253 - val_loss: 0.5918 - val_acc: 0.6867\n",
      "Epoch 12/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.5374 - acc: 0.7249 - val_loss: 0.5707 - val_acc: 0.6987\n",
      "Epoch 13/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.5360 - acc: 0.7298 - val_loss: 0.5734 - val_acc: 0.7001\n",
      "Epoch 14/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.5354 - acc: 0.7234 - val_loss: 0.5630 - val_acc: 0.7046\n",
      "Epoch 15/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.5320 - acc: 0.7268 - val_loss: 0.5705 - val_acc: 0.6987\n",
      "Epoch 16/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.5258 - acc: 0.7346 - val_loss: 0.5721 - val_acc: 0.7001\n",
      "Epoch 17/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.5274 - acc: 0.7316 - val_loss: 0.6198 - val_acc: 0.6777\n",
      "Epoch 18/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.5252 - acc: 0.7358 - val_loss: 0.5577 - val_acc: 0.7196\n",
      "Epoch 19/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.5214 - acc: 0.7346 - val_loss: 0.5857 - val_acc: 0.6762\n",
      "Epoch 20/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.5205 - acc: 0.7380 - val_loss: 0.5671 - val_acc: 0.7076\n",
      "Epoch 21/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.5146 - acc: 0.7463 - val_loss: 0.6019 - val_acc: 0.6927\n",
      "Epoch 22/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.5106 - acc: 0.7421 - val_loss: 0.5630 - val_acc: 0.7136\n",
      "Epoch 23/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.5100 - acc: 0.7440 - val_loss: 0.5655 - val_acc: 0.7016\n",
      "Epoch 24/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.5076 - acc: 0.7448 - val_loss: 0.5709 - val_acc: 0.7016\n",
      "Epoch 25/100\n",
      "2668/2668 [==============================] - 0s 106us/step - loss: 0.5046 - acc: 0.7485 - val_loss: 0.5540 - val_acc: 0.7181\n",
      "Epoch 26/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.5023 - acc: 0.7564 - val_loss: 0.5572 - val_acc: 0.7166\n",
      "Epoch 27/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.4978 - acc: 0.7552 - val_loss: 0.5864 - val_acc: 0.6987\n",
      "Epoch 28/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.4993 - acc: 0.7489 - val_loss: 0.5876 - val_acc: 0.6987\n",
      "Epoch 29/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.4943 - acc: 0.7552 - val_loss: 0.5533 - val_acc: 0.7136\n",
      "Epoch 30/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.4960 - acc: 0.7470 - val_loss: 0.5521 - val_acc: 0.7211\n",
      "Epoch 31/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.4877 - acc: 0.7620 - val_loss: 0.5664 - val_acc: 0.7136\n",
      "Epoch 32/100\n",
      "2668/2668 [==============================] - 0s 106us/step - loss: 0.4859 - acc: 0.7571 - val_loss: 0.5612 - val_acc: 0.7061\n",
      "Epoch 33/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.4813 - acc: 0.7639 - val_loss: 0.5681 - val_acc: 0.7121\n",
      "Epoch 34/100\n",
      "2668/2668 [==============================] - 0s 117us/step - loss: 0.4807 - acc: 0.7654 - val_loss: 0.6224 - val_acc: 0.6507\n",
      "Epoch 35/100\n",
      "2668/2668 [==============================] - 0s 119us/step - loss: 0.4778 - acc: 0.7680 - val_loss: 0.5539 - val_acc: 0.7076\n",
      "Epoch 36/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.4771 - acc: 0.7691 - val_loss: 0.5549 - val_acc: 0.7151\n",
      "Epoch 37/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.4751 - acc: 0.7669 - val_loss: 0.5685 - val_acc: 0.7091\n",
      "Epoch 38/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.4705 - acc: 0.7657 - val_loss: 0.5933 - val_acc: 0.7151\n",
      "Epoch 39/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.4673 - acc: 0.7729 - val_loss: 0.5750 - val_acc: 0.7121\n",
      "Epoch 40/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.4629 - acc: 0.7740 - val_loss: 0.5916 - val_acc: 0.7136\n",
      "Epoch 41/100\n",
      "2668/2668 [==============================] - 0s 103us/step - loss: 0.4606 - acc: 0.7747 - val_loss: 0.5673 - val_acc: 0.6912\n",
      "Epoch 42/100\n",
      "2668/2668 [==============================] - 0s 108us/step - loss: 0.4584 - acc: 0.7789 - val_loss: 0.5683 - val_acc: 0.7241\n",
      "Epoch 43/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.4589 - acc: 0.7736 - val_loss: 0.5635 - val_acc: 0.7121\n",
      "Epoch 44/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.4541 - acc: 0.7826 - val_loss: 0.5555 - val_acc: 0.7241\n",
      "Epoch 45/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.4492 - acc: 0.7822 - val_loss: 0.5621 - val_acc: 0.7151\n",
      "Epoch 46/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.4473 - acc: 0.7856 - val_loss: 0.5476 - val_acc: 0.7136\n",
      "Epoch 47/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.4457 - acc: 0.7882 - val_loss: 0.6476 - val_acc: 0.6837\n",
      "Epoch 48/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.4430 - acc: 0.7912 - val_loss: 0.5647 - val_acc: 0.7106\n",
      "Epoch 49/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.4371 - acc: 0.7924 - val_loss: 0.5902 - val_acc: 0.6927\n",
      "Epoch 50/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.4367 - acc: 0.7980 - val_loss: 0.6994 - val_acc: 0.6822\n",
      "Epoch 51/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.4330 - acc: 0.7950 - val_loss: 0.5570 - val_acc: 0.7151\n",
      "Epoch 52/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.4315 - acc: 0.7837 - val_loss: 0.5714 - val_acc: 0.7226\n",
      "Epoch 53/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.4300 - acc: 0.7924 - val_loss: 0.6110 - val_acc: 0.7061\n",
      "Epoch 54/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.4256 - acc: 0.8017 - val_loss: 0.5851 - val_acc: 0.7166\n",
      "Epoch 55/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.4243 - acc: 0.8006 - val_loss: 0.5986 - val_acc: 0.6882\n",
      "Epoch 56/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.4212 - acc: 0.7969 - val_loss: 0.5753 - val_acc: 0.7106\n",
      "Epoch 57/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.4179 - acc: 0.8025 - val_loss: 0.5541 - val_acc: 0.7226\n",
      "Epoch 58/100\n",
      "2668/2668 [==============================] - 0s 106us/step - loss: 0.4099 - acc: 0.8115 - val_loss: 0.5707 - val_acc: 0.7121\n",
      "Epoch 59/100\n",
      "2668/2668 [==============================] - 0s 108us/step - loss: 0.4083 - acc: 0.8100 - val_loss: 0.5668 - val_acc: 0.7211\n",
      "Epoch 60/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.4036 - acc: 0.8088 - val_loss: 0.5630 - val_acc: 0.7211\n",
      "Epoch 61/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.4091 - acc: 0.8077 - val_loss: 0.6545 - val_acc: 0.6702\n",
      "Epoch 62/100\n",
      "2668/2668 [==============================] - 0s 105us/step - loss: 0.4017 - acc: 0.8193 - val_loss: 0.5912 - val_acc: 0.7211\n",
      "Epoch 63/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.3972 - acc: 0.8148 - val_loss: 0.5869 - val_acc: 0.7196\n",
      "Epoch 64/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.3934 - acc: 0.8163 - val_loss: 0.6435 - val_acc: 0.6762\n",
      "Epoch 65/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.3936 - acc: 0.8216 - val_loss: 0.6945 - val_acc: 0.6747\n",
      "Epoch 66/100\n",
      "2668/2668 [==============================] - 0s 108us/step - loss: 0.3926 - acc: 0.8220 - val_loss: 0.6608 - val_acc: 0.6972\n",
      "Epoch 67/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.3857 - acc: 0.8336 - val_loss: 0.5961 - val_acc: 0.7031\n",
      "Epoch 68/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.3850 - acc: 0.8265 - val_loss: 0.5979 - val_acc: 0.7181\n",
      "Epoch 69/100\n",
      "2668/2668 [==============================] - 0s 119us/step - loss: 0.3777 - acc: 0.8272 - val_loss: 0.6111 - val_acc: 0.7301\n",
      "Epoch 70/100\n",
      "2668/2668 [==============================] - 0s 116us/step - loss: 0.3840 - acc: 0.8212 - val_loss: 0.6040 - val_acc: 0.7226\n",
      "Epoch 71/100\n",
      "2668/2668 [==============================] - 0s 115us/step - loss: 0.3735 - acc: 0.8317 - val_loss: 0.6120 - val_acc: 0.7181\n",
      "Epoch 72/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.3683 - acc: 0.8302 - val_loss: 0.6356 - val_acc: 0.6972\n",
      "Epoch 73/100\n",
      "2668/2668 [==============================] - 0s 113us/step - loss: 0.3675 - acc: 0.8328 - val_loss: 0.6154 - val_acc: 0.7166\n",
      "Epoch 74/100\n",
      "2668/2668 [==============================] - 0s 117us/step - loss: 0.3653 - acc: 0.8321 - val_loss: 0.6341 - val_acc: 0.6852\n",
      "Epoch 75/100\n",
      "2668/2668 [==============================] - 0s 115us/step - loss: 0.3640 - acc: 0.8358 - val_loss: 0.6596 - val_acc: 0.7016\n",
      "Epoch 76/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.3600 - acc: 0.8407 - val_loss: 0.6774 - val_acc: 0.7046\n",
      "Epoch 77/100\n",
      "2668/2668 [==============================] - 0s 116us/step - loss: 0.3610 - acc: 0.8400 - val_loss: 0.6207 - val_acc: 0.7166\n",
      "Epoch 78/100\n",
      "2668/2668 [==============================] - 0s 115us/step - loss: 0.3527 - acc: 0.8463 - val_loss: 0.6737 - val_acc: 0.7031\n",
      "Epoch 79/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.3571 - acc: 0.8400 - val_loss: 0.6323 - val_acc: 0.7151\n",
      "Epoch 80/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.3536 - acc: 0.8445 - val_loss: 0.6462 - val_acc: 0.7196\n",
      "Epoch 81/100\n",
      "2668/2668 [==============================] - 0s 116us/step - loss: 0.3461 - acc: 0.8478 - val_loss: 0.6892 - val_acc: 0.7031\n",
      "Epoch 82/100\n",
      "2668/2668 [==============================] - 0s 114us/step - loss: 0.3418 - acc: 0.8527 - val_loss: 0.6360 - val_acc: 0.7121\n",
      "Epoch 83/100\n",
      "2668/2668 [==============================] - 0s 108us/step - loss: 0.3407 - acc: 0.8508 - val_loss: 0.6604 - val_acc: 0.7196\n",
      "Epoch 84/100\n",
      "2668/2668 [==============================] - 0s 105us/step - loss: 0.3381 - acc: 0.8490 - val_loss: 0.6983 - val_acc: 0.7151\n",
      "Epoch 85/100\n",
      "2668/2668 [==============================] - 0s 105us/step - loss: 0.3401 - acc: 0.8512 - val_loss: 0.6673 - val_acc: 0.6957\n",
      "Epoch 86/100\n",
      "2668/2668 [==============================] - 0s 101us/step - loss: 0.3364 - acc: 0.8471 - val_loss: 0.6457 - val_acc: 0.7151\n",
      "Epoch 87/100\n",
      "2668/2668 [==============================] - 0s 104us/step - loss: 0.3297 - acc: 0.8531 - val_loss: 0.6601 - val_acc: 0.7166\n",
      "Epoch 88/100\n",
      "2668/2668 [==============================] - 0s 104us/step - loss: 0.3310 - acc: 0.8531 - val_loss: 0.6338 - val_acc: 0.7211\n",
      "Epoch 89/100\n",
      "2668/2668 [==============================] - 0s 101us/step - loss: 0.3229 - acc: 0.8564 - val_loss: 0.6402 - val_acc: 0.7316\n",
      "Epoch 90/100\n",
      "2668/2668 [==============================] - 0s 108us/step - loss: 0.3222 - acc: 0.8617 - val_loss: 0.6426 - val_acc: 0.7226\n",
      "Epoch 91/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.3197 - acc: 0.8557 - val_loss: 0.6552 - val_acc: 0.7406\n",
      "Epoch 92/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.3152 - acc: 0.8564 - val_loss: 0.6677 - val_acc: 0.7211\n",
      "Epoch 93/100\n",
      "2668/2668 [==============================] - 0s 111us/step - loss: 0.3131 - acc: 0.8632 - val_loss: 0.7198 - val_acc: 0.7046\n",
      "Epoch 94/100\n",
      "2668/2668 [==============================] - 0s 112us/step - loss: 0.3149 - acc: 0.8647 - val_loss: 0.6899 - val_acc: 0.7226\n",
      "Epoch 95/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.3095 - acc: 0.8602 - val_loss: 0.7562 - val_acc: 0.6807\n",
      "Epoch 96/100\n",
      "2668/2668 [==============================] - 0s 110us/step - loss: 0.3079 - acc: 0.8636 - val_loss: 0.6651 - val_acc: 0.7301\n",
      "Epoch 97/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.3068 - acc: 0.8621 - val_loss: 0.6832 - val_acc: 0.7031\n",
      "Epoch 98/100\n",
      "2668/2668 [==============================] - 0s 109us/step - loss: 0.3048 - acc: 0.8658 - val_loss: 0.7014 - val_acc: 0.6867\n",
      "Epoch 99/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.2985 - acc: 0.8688 - val_loss: 0.6908 - val_acc: 0.7181\n",
      "Epoch 100/100\n",
      "2668/2668 [==============================] - 0s 107us/step - loss: 0.2978 - acc: 0.8681 - val_loss: 0.6693 - val_acc: 0.7241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72246ab7f0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=None)\n",
    "y_test_cat = to_categorical(y_test, num_classes=None)\n",
    "\n",
    "print(y_train_cat)\n",
    "model = Sequential([\n",
    "    Dense(128, input_shape=(88,)),\n",
    "    Activation('relu'),\n",
    "    Dense(64),\n",
    "    Activation('relu'),\n",
    "    Dense(48),\n",
    "    Activation('relu'),\n",
    "    Dense(24),\n",
    "    Activation('relu'),\n",
    "    Dense(2),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "model.compile(optimizer='Adagrad',\n",
    "              loss='category_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_cat, validation_data = (X_test, y_test_cat), epochs = 100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7076461769115442\n"
     ]
    }
   ],
   "source": [
    "predicts = model.predict_classes(X_test)\n",
    "#print(predicts)\n",
    "#print(y_test)\n",
    "acc = predicts == y_test\n",
    "print(np.count_nonzero(acc) / len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3.6",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
