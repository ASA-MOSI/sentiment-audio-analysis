{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries and Define constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#### Training based on features of audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots_adjust\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.5,1,1,1,1.5])\n",
    "print(a)\n",
    "[ e >1 for e in a ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output: 10018 ,  10018\n"
     ]
    }
   ],
   "source": [
    "##Loading  data from files\n",
    "filehandlerInput = open('processed-data/input_VAD.obj', 'rb')\n",
    "filehandlerOutput = open('processed-data/output_VAD.obj', 'rb')\n",
    "input = pickle.load(filehandlerInput)\n",
    "output = pickle.load(filehandlerOutput)\n",
    "print(\"Size input, output:\", len(input),\", \", len(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data and Analyze data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct output:  {0.5, 1.5, 2.5, 3.5, 3.0, 4.0, 2.0, 4.5, 5.0, 1.0, 2.3333, 2.6667, 4.3333, 2.25, 2.75, 3.25, 3.75, 4.75, 4.25, 5.5, 1.3333, 3.3333, 4.6667, 1.6667, 3.6667}\n",
      "Average labels: 2.7782824016769814\n",
      "Std labels: 0.8970523348000802\n",
      "\n",
      "Distinct output:  {0, 1, 2, 3, 4, 5, 6}\n",
      "Average labels: 2.7481533240167697\n",
      "Std labels: 0.9153419863285778\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x576 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# removing_label = np.array([0.5, 1.5, 2.5, 3.5])\n",
    "# remove_condition = np.array([True for o in output if o in removing_label])\n",
    "                            \n",
    "# print(remove_condition)\n",
    "# input = input[~remove_condition]\n",
    "# output = output[~remove_condition]\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(5,8))\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "\n",
    "\n",
    "## Draw distribution of raw output\n",
    "flattened_out = output.flatten()\n",
    "print(\"Distinct output: \", set(flattened_out));\n",
    "print(\"Average labels:\", np.average(output[:,0]))\n",
    "print(\"Std labels:\", np.std(output[:,0]))\n",
    "axs[0][0].hist(output[:,0])\n",
    "axs[0][0].set_title(\"Distribution Valance\")\n",
    "axs[0][1].hist(output[:,1])\n",
    "axs[0][1].set_title(\"Distribution Arouse\")\n",
    "axs[0][2].hist(output[:,2])\n",
    "axs[0][2].set_title(\"Distribution Dominance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Around output into integer.\n",
    "for i in range(0, output.size):\n",
    "    output.flat[i] = int(np.round(output.flat[i]))\n",
    "output = output.astype(int)\n",
    "\n",
    "flattened_out = output.flatten()\n",
    "print(\"\\nDistinct output: \", set(flattened_out));\n",
    "print(\"Average labels:\", np.average(output[:,0]))\n",
    "print(\"Std labels:\", np.std(output[:,0]))\n",
    "axs[1][0].hist(output[:,0])\n",
    "axs[1][0].set_title(\"Distribution Valance\")\n",
    "axs[1][1].hist(output[:,1])\n",
    "axs[1][1].set_title(\"Distribution Arouse\")\n",
    "axs[1][2].hist(output[:,2])\n",
    "axs[1][2].set_title(\"Distribution Dominance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output after remove Nan values  10009 ,  10009\n",
      "(10009, 3)\n",
      "Normalize input and categorize output \n",
      "10009\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "input_filtered = input[~np.any(np.isnan(input), axis=1)]\n",
    "input_filtered = input_filtered[:,0:88]\n",
    "output_filtered = output[~np.any(np.isnan(input), axis=1)]\n",
    "print(\"Size input, output after remove Nan values \", len(input_filtered), \", \", len(output_filtered))\n",
    "\n",
    "#Normalize input\n",
    "input_filtered = (input_filtered - input_filtered.min(axis=0)) / (input_filtered.max(axis=0) - input_filtered.min(axis=0))\n",
    "\n",
    "#Normalize output\n",
    "def categorize_output(output):\n",
    "    if (output <= 2.5):\n",
    "        return 0\n",
    "    elif  output >= 4:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# def categorize_output(output):\n",
    "#     return np.around(output)\n",
    "   \n",
    "print(output_filtered.shape)\n",
    "\n",
    "output_filtered = np.array([list(map(lambda x: categorize_output(x), col)) for col in output_filtered])\n",
    "#pprint(output_filtered)\n",
    "#output_filtered = [to_categorical(out, 6) for out in output_filtered]\n",
    "#output_filtered = (output_filtered - output_filtered.min(axis=0)) / (output_filtered.max(axis=0) - output_filtered.min(axis=0))\n",
    "print(\"Normalize input and categorize output \")\n",
    "print(len(output_filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffer data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 2]\n",
      " [0 2 2]\n",
      " [0 1 0]\n",
      " ...\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "#Shuffer\n",
    "c = list(zip(input_filtered, output_filtered))\n",
    "random.shuffle(c)\n",
    "input_filtered, output_filtered = zip( * c)\n",
    "input_filtered = np.array(input_filtered)\n",
    "output_filtered = np.array(output_filtered)\n",
    "print(output_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training, testing set:  8007 ,  2002\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_filtered, output_filtered, test_size=0.2, random_state=300)\n",
    "print(\"Size training, testing set: \", len(X_train), \", \", len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter only integer values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filter_condition_int(arr):\n",
    "    return [int(el) - el == 0 for el in arr]\n",
    "\n",
    "filter_condition_arouse_train = create_filter_condition_int(y_train[:,1])\n",
    "filter_condition_arouse_test = create_filter_condition_int(y_test[:,1])\n",
    "X_train = X_train[filter_condition_arouse_train]\n",
    "y_train = y_train[filter_condition_arouse_train]\n",
    "X_test = X_test[filter_condition_arouse_test]\n",
    "y_test = y_test[filter_condition_arouse_test]\n",
    "\n",
    "print('Size train and test after filter integer values ', len(X_train), len(X_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Training on keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8007 samples, validate on 2002 samples\n",
      "Epoch 1/50\n",
      "8007/8007 [==============================] - ETA: 2:29 - loss: 1.098 - ETA: 3s - loss: 1.0953  - ETA: 1s - loss: 1.088 - ETA: 0s - loss: 1.076 - ETA: 0s - loss: 1.060 - ETA: 0s - loss: 1.037 - ETA: 0s - loss: 1.013 - 1s 133us/step - loss: 1.0090 - val_loss: 0.9012\n",
      "Epoch 2/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.052 - ETA: 0s - loss: 0.868 - ETA: 0s - loss: 0.880 - ETA: 0s - loss: 0.898 - ETA: 0s - loss: 0.894 - ETA: 0s - loss: 0.894 - 0s 40us/step - loss: 0.8941 - val_loss: 0.8889\n",
      "Epoch 3/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.797 - ETA: 0s - loss: 0.875 - ETA: 0s - loss: 0.873 - ETA: 0s - loss: 0.876 - ETA: 0s - loss: 0.876 - ETA: 0s - loss: 0.878 - 0s 38us/step - loss: 0.8805 - val_loss: 0.8838\n",
      "Epoch 4/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.911 - ETA: 0s - loss: 0.890 - ETA: 0s - loss: 0.879 - ETA: 0s - loss: 0.875 - ETA: 0s - loss: 0.878 - ETA: 0s - loss: 0.875 - 0s 40us/step - loss: 0.8743 - val_loss: 0.8793\n",
      "Epoch 5/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.879 - ETA: 0s - loss: 0.872 - ETA: 0s - loss: 0.867 - ETA: 0s - loss: 0.863 - ETA: 0s - loss: 0.863 - 0s 41us/step - loss: 0.8685 - val_loss: 0.8850\n",
      "Epoch 6/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.052 - ETA: 0s - loss: 0.898 - ETA: 0s - loss: 0.888 - ETA: 0s - loss: 0.871 - ETA: 0s - loss: 0.869 - ETA: 0s - loss: 0.871 - ETA: 0s - loss: 0.867 - 0s 44us/step - loss: 0.8662 - val_loss: 0.8799\n",
      "Epoch 7/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.951 - ETA: 0s - loss: 0.890 - ETA: 0s - loss: 0.877 - ETA: 0s - loss: 0.873 - ETA: 0s - loss: 0.872 - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.862 - 0s 46us/step - loss: 0.8663 - val_loss: 0.8749\n",
      "Epoch 8/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.913 - ETA: 0s - loss: 0.856 - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.857 - ETA: 0s - loss: 0.855 - ETA: 0s - loss: 0.861 - ETA: 0s - loss: 0.861 - 0s 43us/step - loss: 0.8621 - val_loss: 0.8754\n",
      "Epoch 9/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.929 - ETA: 0s - loss: 0.877 - ETA: 0s - loss: 0.884 - ETA: 0s - loss: 0.868 - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.860 - 0s 51us/step - loss: 0.8619 - val_loss: 0.8706\n",
      "Epoch 10/50\n",
      "8007/8007 [==============================] - ETA: 1s - loss: 0.815 - ETA: 0s - loss: 0.848 - ETA: 0s - loss: 0.848 - ETA: 0s - loss: 0.855 - ETA: 0s - loss: 0.855 - ETA: 0s - loss: 0.858 - ETA: 0s - loss: 0.856 - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.856 - 0s 56us/step - loss: 0.8570 - val_loss: 0.9137\n",
      "Epoch 11/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.875 - ETA: 0s - loss: 0.873 - ETA: 0s - loss: 0.872 - ETA: 0s - loss: 0.864 - ETA: 0s - loss: 0.867 - ETA: 0s - loss: 0.864 - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.864 - 0s 54us/step - loss: 0.8642 - val_loss: 0.8706\n",
      "Epoch 12/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.876 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.852 - ETA: 0s - loss: 0.862 - ETA: 0s - loss: 0.863 - ETA: 0s - loss: 0.862 - ETA: 0s - loss: 0.860 - 0s 42us/step - loss: 0.8597 - val_loss: 0.8745\n",
      "Epoch 13/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.978 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.855 - ETA: 0s - loss: 0.855 - 0s 44us/step - loss: 0.8558 - val_loss: 0.8652\n",
      "Epoch 14/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.843 - ETA: 0s - loss: 0.812 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.857 - ETA: 0s - loss: 0.856 - 0s 47us/step - loss: 0.8562 - val_loss: 0.8633\n",
      "Epoch 15/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.853 - 0s 49us/step - loss: 0.8567 - val_loss: 0.8730\n",
      "Epoch 16/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.780 - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.864 - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.859 - ETA: 0s - loss: 0.858 - 0s 41us/step - loss: 0.8578 - val_loss: 0.8629\n",
      "Epoch 17/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.934 - ETA: 0s - loss: 0.887 - ETA: 0s - loss: 0.864 - ETA: 0s - loss: 0.858 - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.850 - 0s 44us/step - loss: 0.8535 - val_loss: 0.8693\n",
      "Epoch 18/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.814 - ETA: 0s - loss: 0.877 - ETA: 0s - loss: 0.865 - ETA: 0s - loss: 0.861 - ETA: 0s - loss: 0.859 - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.856 - 0s 43us/step - loss: 0.8559 - val_loss: 0.8671\n",
      "Epoch 19/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.836 - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.856 - ETA: 0s - loss: 0.846 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.849 - 0s 42us/step - loss: 0.8492 - val_loss: 0.8661\n",
      "Epoch 20/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.077 - ETA: 0s - loss: 0.872 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.849 - 0s 42us/step - loss: 0.8508 - val_loss: 0.8656\n",
      "Epoch 21/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.028 - ETA: 0s - loss: 0.839 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.848 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.852 - ETA: 0s - loss: 0.849 - 0s 43us/step - loss: 0.8500 - val_loss: 0.8780\n",
      "Epoch 22/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.724 - ETA: 0s - loss: 0.857 - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.853 - ETA: 0s - loss: 0.853 - 0s 44us/step - loss: 0.8504 - val_loss: 0.8743\n",
      "Epoch 23/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.941 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.837 - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.862 - ETA: 0s - loss: 0.856 - ETA: 0s - loss: 0.854 - 0s 43us/step - loss: 0.8536 - val_loss: 0.8704\n",
      "Epoch 24/50\n",
      "8007/8007 [==============================] - ETA: 1s - loss: 0.886 - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.858 - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.849 - 0s 41us/step - loss: 0.8491 - val_loss: 0.8701\n",
      "Epoch 25/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.872 - ETA: 0s - loss: 0.855 - ETA: 0s - loss: 0.846 - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.848 - 0s 39us/step - loss: 0.8477 - val_loss: 0.8967\n",
      "Epoch 26/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.869 - ETA: 0s - loss: 0.843 - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.849 - 0s 43us/step - loss: 0.8487 - val_loss: 0.8618\n",
      "Epoch 27/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.836 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.838 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.845 - 0s 40us/step - loss: 0.8460 - val_loss: 0.8582\n",
      "Epoch 28/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.812 - ETA: 0s - loss: 0.858 - ETA: 0s - loss: 0.852 - ETA: 0s - loss: 0.857 - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.847 - 0s 43us/step - loss: 0.8473 - val_loss: 0.8602\n",
      "Epoch 29/50\n",
      "8007/8007 [==============================] - ETA: 1s - loss: 0.707 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.856 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.839 - ETA: 0s - loss: 0.843 - ETA: 0s - loss: 0.843 - 0s 44us/step - loss: 0.8441 - val_loss: 0.8686\n",
      "Epoch 30/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 0.872 - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.846 - ETA: 0s - loss: 0.842 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.846 - 0s 45us/step - loss: 0.8490 - val_loss: 0.8571\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8007/8007 [==============================] - ETA: 0s - loss: 0.680 - ETA: 0s - loss: 0.848 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.842 - ETA: 0s - loss: 0.842 - 0s 37us/step - loss: 0.8438 - val_loss: 0.8774\n",
      "Epoch 32/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.892 - ETA: 0s - loss: 0.843 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.841 - 0s 39us/step - loss: 0.8428 - val_loss: 0.8575\n",
      "Epoch 33/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.863 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.846 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.841 - 0s 52us/step - loss: 0.8430 - val_loss: 0.8605\n",
      "Epoch 34/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.902 - ETA: 0s - loss: 0.832 - ETA: 0s - loss: 0.849 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.839 - ETA: 0s - loss: 0.838 - ETA: 0s - loss: 0.843 - 0s 48us/step - loss: 0.8430 - val_loss: 0.8592\n",
      "Epoch 35/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.677 - ETA: 0s - loss: 0.848 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.838 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.845 - ETA: 0s - loss: 0.844 - 0s 49us/step - loss: 0.8443 - val_loss: 0.8604\n",
      "Epoch 36/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.763 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.828 - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.839 - 0s 44us/step - loss: 0.8385 - val_loss: 0.8661\n",
      "Epoch 37/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.843 - ETA: 0s - loss: 0.832 - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.838 - ETA: 0s - loss: 0.839 - 0s 47us/step - loss: 0.8408 - val_loss: 0.8624\n",
      "Epoch 38/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.918 - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.840 - ETA: 0s - loss: 0.839 - 0s 47us/step - loss: 0.8400 - val_loss: 0.8569\n",
      "Epoch 39/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.850 - ETA: 0s - loss: 0.880 - ETA: 0s - loss: 0.851 - ETA: 0s - loss: 0.852 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.837 - 0s 46us/step - loss: 0.8367 - val_loss: 0.8581\n",
      "Epoch 40/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.975 - ETA: 0s - loss: 0.860 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.836 - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.834 - 0s 47us/step - loss: 0.8360 - val_loss: 0.8623\n",
      "Epoch 41/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.740 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.826 - ETA: 0s - loss: 0.830 - ETA: 0s - loss: 0.833 - 0s 43us/step - loss: 0.8345 - val_loss: 0.8572\n",
      "Epoch 42/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.998 - ETA: 0s - loss: 0.796 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.818 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.838 - 0s 44us/step - loss: 0.8395 - val_loss: 0.8600\n",
      "Epoch 43/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.975 - ETA: 0s - loss: 0.818 - ETA: 0s - loss: 0.821 - ETA: 0s - loss: 0.832 - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.826 - ETA: 0s - loss: 0.828 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.833 - 0s 60us/step - loss: 0.8346 - val_loss: 0.8580\n",
      "Epoch 44/50\n",
      "8007/8007 [==============================] - ETA: 1s - loss: 0.718 - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.838 - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.836 - ETA: 0s - loss: 0.836 - 0s 51us/step - loss: 0.8368 - val_loss: 0.8554\n",
      "Epoch 45/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.701 - ETA: 0s - loss: 0.832 - ETA: 0s - loss: 0.816 - ETA: 0s - loss: 0.815 - ETA: 0s - loss: 0.822 - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.834 - 0s 52us/step - loss: 0.8336 - val_loss: 0.8597\n",
      "Epoch 46/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.861 - ETA: 0s - loss: 0.846 - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.844 - ETA: 0s - loss: 0.842 - ETA: 0s - loss: 0.836 - 0s 45us/step - loss: 0.8358 - val_loss: 0.8735\n",
      "Epoch 47/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.763 - ETA: 0s - loss: 0.843 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.825 - ETA: 0s - loss: 0.830 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.832 - 0s 43us/step - loss: 0.8345 - val_loss: 0.8618\n",
      "Epoch 48/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.686 - ETA: 0s - loss: 0.816 - ETA: 0s - loss: 0.817 - ETA: 0s - loss: 0.826 - ETA: 0s - loss: 0.830 - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.832 - 0s 45us/step - loss: 0.8319 - val_loss: 0.8550\n",
      "Epoch 49/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.834 - ETA: 0s - loss: 0.847 - ETA: 0s - loss: 0.842 - ETA: 0s - loss: 0.832 - ETA: 0s - loss: 0.834 - 0s 40us/step - loss: 0.8343 - val_loss: 0.8571\n",
      "Epoch 50/50\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.788 - ETA: 0s - loss: 0.833 - ETA: 0s - loss: 0.831 - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.827 - ETA: 0s - loss: 0.826 - ETA: 0s - loss: 0.831 - 0s 42us/step - loss: 0.8301 - val_loss: 0.8715\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "#     model = Sequential([\n",
    "#         Dense(64, input_shape=(20,), kernel_initializer='normal'),\n",
    "#        Activation('relu'),\n",
    "#         Dense(32, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(16,kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(8, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(1,kernel_initializer='normal'),\n",
    "#     ])\n",
    "#     model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#     model.fit(X_train, y_train[:,0], validation_data=(X_test, y_test[:,0]), epochs = 20)\n",
    "    #model.fit(X_train[0:2], y_train[0:2,0], validation_data=(X_test[0:2], y_test[0:2,0]), epochs = 1000)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    model1 = Sequential([\n",
    "        Dense(64, input_shape=(88,), kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(32, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(16,kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(8, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(3),\n",
    "        Activation('softmax'),\n",
    "\n",
    "    ])\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    model1.fit(X_train, y_train[:,1], validation_data=(X_test, y_test[:,1]), epochs = 50)\n",
    "\n",
    "\n",
    "#     model2 = Sequential([\n",
    "#         Dense(64, input_shape=(20,), kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(32, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(16,kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(8, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(1),\n",
    "#     ])\n",
    "#     model2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#     model2.fit(X_train, y_train[:,2], validation_data=(X_test, y_test[:,2]), epochs = 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinal logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (3!=1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-eb7345672dfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mclf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_AT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_AT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean Absolute Error of LogisticAT %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_AT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \"\"\"\n\u001b[0;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 170\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    171\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n\u001b[0;32m    172\u001b[0m                                weights=sample_weight, axis=0)\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         raise ValueError(\"y_true and y_pred have different number of output \"\n\u001b[1;32m---> 87\u001b[1;33m                          \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred have different number of output (3!=1)"
     ]
    }
   ],
   "source": [
    "import mord\n",
    "from sklearn import linear_model, metrics, preprocessing\n",
    "\n",
    "clf2 = mord.LogisticAT(alpha=1)\n",
    "#clf2 = mord.LogisticIT(alpha=1)\n",
    "#clf2 = mord.LAD()\n",
    "#clf2 = mord.OrdinalRidge()\n",
    "#clf2 = mord.MulticlassLogistic()\n",
    "\n",
    "y_train_AT = y_train \n",
    "y_train_AT = y_train_AT.astype(int)\n",
    "y_test_AT = y_test.astype(int)\n",
    "\n",
    "clf2.fit(X_train, y_train_AT[:,1] )\n",
    "\n",
    "print(metrics.mean_absolute_error(model1.predict(X_test), y_test_AT[:,1]))\n",
    "print('Mean Absolute Error of LogisticAT %s' % metrics.mean_absolute_error(clf2.predict(X_test), y_test_AT[:,1]))\n",
    "\n",
    "print(clf2.predict(X_test)[0:3])\n",
    "#print(clf2.predict_proba(X_test)[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient of Determination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#predict = model.predict(X_test)\n",
    "predict1 = model1.predict(X_test)\n",
    "#predict2 = model2.predict(X_test)\n",
    "\n",
    "# a = r2_score(y_test[:,0], predict)\n",
    "# print(\"The R2 for Valance: \" ,a)\n",
    "\n",
    "a1 = r2_score(y_test[:,1], predict1)\n",
    "print(\"The R2 for Activation: \" ,a1)\n",
    "\n",
    "# a2 = r2_score(y_test[:,2], predict2)\n",
    "# print(\"The R2 for Dominance: \" ,a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of err**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = np.linspace(0, 1, 10)\n",
    "data = np.abs(predict1 - y_test[:,1])\n",
    "print(len(data))\n",
    "hist, bin_edges = np.histogram(data,bins) # make the histogram\n",
    "print(hist)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Plot the histogram heights against integers on the x axis\n",
    "ax.bar(range(len(hist)),hist,width=1) \n",
    "\n",
    "# # Set the ticks to the middle of the bars\n",
    "# ax.set_xticks([0.5+i for i,j in enumerate(hist)])\n",
    "\n",
    "# # # Set the xticklabels to a string that tells us what the bin edges were\n",
    "# # ax.set_xticklabels(['{} - {}'.format(bins[i],bins[i+1]) for i,j in enumerate(hist)])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#predict = model.predict(X_test)\n",
    "\n",
    "#plt.hist(subs, bins =bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard residual plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.pyplot import subplots_adjust\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "subplots_adjust(right = 2, wspace=0.2)\n",
    "predict = model.predict(X_test)\n",
    "predict = np.array([p[0] for p in predict])\n",
    "axs[0] = sns.residplot(y_test[:,0], predict, lowess=True, color=\"g\", ax=axs[0])\n",
    "axs[0].set_ylim([-0.4,0.4])\n",
    "axs[0].set_title(\"Residual for Valance\") \n",
    "\n",
    "\n",
    "predict1 = model1.predict(X_test)\n",
    "predict1 = np.array([p[0] for p in predict1])\n",
    "axs[1] = sns.residplot(y_test[:,1], predict1, lowess=True, color=\"r\", ax=axs[1])\n",
    "axs[1].set_ylim([-0.4,0.4])\n",
    "axs[1].set_title(\"Residual for Activation\")\n",
    "\n",
    "predict2 = model2.predict(X_test)\n",
    "predict2 = np.array([p[0] for p in predict2])\n",
    "axs[2] = sns.residplot(y_test[:,2], predict2, lowess=True, color=\"b\", ax=axs[2])\n",
    "axs[2].set_ylim([-0.5,0.5])\n",
    "axs[2].set_title(\"Residual for Dominance\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual plot show that the error followed random pattern. If the residual plot is not enough random, then the model would be likely to lose something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing around with prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict = model.predict(X_test)\n",
    "#print(predict)\n",
    "\n",
    "subs = np.abs(predict - y_test[:,0])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "subplots_adjust(right = 2, wspace=0.2)\n",
    "axs[0].hist(y_train[:,0])\n",
    "axs[0].set_title(\"True value in train data\\n\" + \n",
    "                 \"avg:\" + str(np.average(y_train[:,0])) + \n",
    "                 \"\\nstd:\" + str(np.std(y_train[:,0])))\n",
    "axs[1].hist(y_test[:,0])\n",
    "axs[1].set_title(\"True value in test data\\n\" + \n",
    "                 \"avg:\" + str(np.average(y_test[:,0])) + \n",
    "                 \"\\nstd:\" + str(np.std(y_test[:,0])))\n",
    "axs[1].set_ylim([0,400])\n",
    "                        \n",
    "axs[2].hist(predict) \n",
    "axs[2].set_title(\"Prediction\\n\" +\n",
    "                \"avg:\" + str(np.average(predict)) + \n",
    "                 \"\\nstd:\" + str(np.std(predict)))\n",
    "                \n",
    "# axs[2].hist(subs, bins='auto')\n",
    "# axs[2].set_title(\"error on predictions\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.99):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "predict = model1.predict(X_test)\n",
    "bins = np.linspace(0, 1, 6)\n",
    "bin_ranges = [bins[i-1: i+1] for i in range (1,len(bins))]\n",
    "bin_ranges =  np.array(bin_ranges)\n",
    "\n",
    "# y = y_test[:,1]\n",
    "# subs = np.abs(y-predict)\n",
    "# print(\"ave, std, per90:\", np.average(subs), \", \", np.std(subs), \",\", np.percentile(subs, 60))\n",
    "    \n",
    "for r in bin_ranges:\n",
    "    # Get valance\n",
    "    y = y_test[:,1]\n",
    "    condition1 = [r[0] <= e <= r[1]for e in y]\n",
    "    \n",
    "    predict = predict.flatten()\n",
    "    condition2 = [r[0] <= e <= r[1]for e in predict]\n",
    "    \n",
    "    y = y[condition1 or condition2]\n",
    "    pre = predict[condition1 or condition2]\n",
    "    #pre= pre.flatten()\n",
    "    #print(pre)\n",
    "    subs = np.abs(y-pre)\n",
    "   # print(subs)\n",
    "    try:\n",
    "        #print(subs)\n",
    "        print(\"\\nIn range \" , r, \n",
    "              \"\\nNum samples: \", len(subs),\n",
    "              \"\\nAverage err: \", np.average(subs), \n",
    "              \"\\nStd err: \", np.std(subs),\n",
    "              \"\\nMedian err: \", np.median(subs), \n",
    "             \"\\nPercentile: \", np.percentile(subs, 90),\n",
    "              \"\\nConfident Interval: \", mean_confidence_interval(subs)\n",
    "             )\n",
    "    except:\n",
    "        print(\"null array\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.99):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "predict = clf2.predict(X_test)\n",
    "bins = np.linspace(0, 6, 7)\n",
    "bin_ranges = [bins[i-1: i+1] for i in range (1,len(bins))]\n",
    "bin_ranges =  np.array(bin_ranges)\n",
    "\n",
    "\n",
    "for r in bin_ranges:\n",
    "    # Get valance\n",
    "    y = y_test[:,1]\n",
    "    condition1 = [r[0] <= e <= r[1]for e in y]\n",
    "    \n",
    "    predict = predict.flatten()\n",
    "    condition2 = [r[0] <= e <= r[1]for e in predict]\n",
    "    \n",
    "    y = y[condition1 or condition2]\n",
    "    pre = predict[condition1 or condition2]\n",
    "    #pre= pre.flatten()\n",
    "    print(pre)\n",
    "    subs = np.abs(y-pre)\n",
    "   # print(subs)\n",
    "    try:\n",
    "        print(subs)\n",
    "        print(\"\\nIn range \" , r, \n",
    "              \"\\nNum samples: \", len(subs),\n",
    "              \"\\nAverage err: \", np.average(subs), \n",
    "              \"\\nStd err: \", np.std(subs),\n",
    "              \"\\nMedian err: \", np.median(subs), \n",
    "             \"\\nPercentile: \", np.percentile(subs, 95),\n",
    "              \"\\nConfident Interval: \", mean_confidence_interval(subs)\n",
    "             )\n",
    "    except:\n",
    "        print(\"null array\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "predict = model.predict(X_test)\n",
    "predict = np.array([p[0] for p in predict])\n",
    "\n",
    "predict1 = model1.predict(X_test)\n",
    "predict1 = np.array([p[0] for p in predict1])\n",
    "\n",
    "predict2 = model2.predict(X_test)\n",
    "predict2 = np.array([p[0] for p in predict2])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test[:,1], predict1, edgecolors=(0, 0, 0))\n",
    "#ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "\n",
    "# ax[1].scatter(y_test[:,1], predict, edgecolors=(0, 0, 0))\n",
    "# ax[1].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "# ax[1].set_xlabel('Actual')\n",
    "# ax[1].set_ylabel('Predicted')\n",
    "\n",
    "# ax[2].scatter(y_test[:,2], predict, edgecolors=(0, 0, 0))\n",
    "# ax[2].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "# ax[2].set_xlabel('Actual')\n",
    "# ax[2].set_ylabel('Predicted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing around **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "predict = model2.predict(X_test)\n",
    "\n",
    "def reducer(acc, val):\n",
    "    if (val > 0.5):\n",
    "        acc = acc + 1\n",
    "    return acc\n",
    "\n",
    "num = reduce(lambda acc,val: acc+1 if val >1 else acc, predict.flatten(), 0) \n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence in interval for Arouse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 ... 2 1 0]\n",
      "Confusion matrix: \n",
      " [[335 252  40]\n",
      " [105 349 151]\n",
      " [ 29 269 472]]\n",
      "Num samples: 1232\n",
      "Accurcy in range:  [0. 1.] :  0.5547445255474452\n",
      "Num samples: 1375\n",
      "Accurcy in range:  [1. 2.] :  0.596656976744186\n",
      "Num samples: 770\n",
      "Accurcy in range:  [2. 3.] :  0.6121919584954605\n",
      "Num samples: 0\n",
      "Accurcy in range:  [3. 4.] :  0.0\n",
      "Num samples: 0\n",
      "Accurcy in range:  [4. 5.] :  0.0\n",
      "{0, 1, 2}\n",
      "2002\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict = clf2.predict(X_test)\n",
    "print(predict)\n",
    "#predict = clf2.predict(X_test)\n",
    "#print(predict)\n",
    "#predict = np.array([p[0] for p in predict])\n",
    "y = y_test[:, 1]\n",
    "y1=y\n",
    "#y1 = np.argmax(y,1)\n",
    "cm = confusion_matrix(y1, predict)\n",
    "print(\"Confusion matrix: \\n\", cm)\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 5, 6)\n",
    "bin_ranges = [bins[i-1: i+1] for i in range (1,len(bins))]\n",
    "bin_ranges =  np.array(bin_ranges)\n",
    "\n",
    "for r in bin_ranges:\n",
    "     condition = [r[0] <= e <= r[1] for e in y]\n",
    "     y1 = y[condition]\n",
    "     pre = predict.flatten()\n",
    "     pre = pre[condition]\n",
    "     num_true = 0\n",
    "     for i in range (0, len(y1)):\n",
    "        if (np.abs(y1[i] - pre[i]) ==0 ):\n",
    "            num_true += 1\n",
    "     print(\"Num samples:\", len(pre))\n",
    "     print(\"Accurcy in range: \", r, \": \", num_true / (len(pre) +1 ) )\n",
    "            \n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "y_int = y.astype(int)\n",
    "\n",
    "ax[0].hist(y)\n",
    "ax[0].set_ylim([0,650])\n",
    "ax[1].hist(y_int)\n",
    "ax[1].set_ylim([0,650])\n",
    "ax[2].hist(predict)\n",
    "ax[2].set_ylim([0,650])\n",
    "        \n",
    "print(set(predict))\n",
    "print(len(predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
