{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries and Define constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#### Training based on features of audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots_adjust\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output, transcripts:  10018 ,  10018 , 10018\n"
     ]
    }
   ],
   "source": [
    "##Loading  data from files\n",
    "filehandlerInput = open('processed-data/input_VAD.obj', 'rb')\n",
    "filehandlerOutput = open('processed-data/output_VAD.obj', 'rb')\n",
    "filehandlerTranscript = open('processed-data/transcript_VAD.obj', 'rb')\n",
    "input = pickle.load(filehandlerInput)\n",
    "output = pickle.load(filehandlerOutput)\n",
    "texts = pickle.load(filehandlerTranscript)\n",
    "print(\"Size input, output, transcripts: \", len(input),\", \", len(output), \",\", len(texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data and Analyze data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct output:  {0.5, 1.5, 2.5, 3.5, 3.0, 4.0, 2.0, 4.5, 5.0, 1.0, 2.3333, 2.6667, 4.3333, 2.25, 2.75, 3.25, 3.75, 4.75, 4.25, 5.5, 1.3333, 3.3333, 4.6667, 1.6667, 3.6667}\n",
      "Average labels: 2.7782824016769814\n",
      "Std labels: 0.8970523348000802\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x576 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3,3, figsize=(5,8))\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "\n",
    "\n",
    "## Draw distribution of raw output\n",
    "flattened_out = output.flatten()\n",
    "print(\"Distinct output: \", set(flattened_out));\n",
    "print(\"Average labels:\", np.average(output[:,0]))\n",
    "print(\"Std labels:\", np.std(output[:,0]))\n",
    "axs[0][0].hist(output[:,0])\n",
    "axs[0][0].set_title(\"Distribution Valance\")\n",
    "axs[0][1].hist(output[:,1])\n",
    "axs[0][1].set_title(\"Distribution Arouse\")\n",
    "axs[0][2].hist(output[:,2])\n",
    "axs[0][2].set_title(\"Distribution Dominance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Around output into integer.\n",
    "# for i in range(0, output.size):\n",
    "#     output.flat[i] = int(np.round(output.flat[i]))\n",
    "# output = output.astype(int)\n",
    "\n",
    "# flattened_out = output.flatten()\n",
    "# print(\"\\nDistinct output: \", set(flattened_out));\n",
    "# print(\"Average labels:\", np.average(output[:,0]))\n",
    "# print(\"Std labels:\", np.std(output[:,0]))\n",
    "# axs[1][0].hist(output[:,0])\n",
    "# axs[1][0].set_title(\"Distribution Valance\")\n",
    "# axs[1][1].hist(output[:,1])\n",
    "# axs[1][1].set_title(\"Distribution Arouse\")\n",
    "# axs[1][2].hist(output[:,2])\n",
    "# axs[1][2].set_title(\"Distribution Dominance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size input, output, texts after remove Nan values  10009 ,  10009 ,  10009\n",
      "(10009, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize input and categorize output \n",
      "10009\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "input_filtered = input[~np.any(np.isnan(input), axis=1)]\n",
    "input_filtered = input_filtered[:,0:88]\n",
    "output_filtered = output[~np.any(np.isnan(input), axis=1)]\n",
    "texts_filtered = texts[~np.any(np.isnan(input), axis=1)]\n",
    "print(\"Size input, output, texts after remove Nan values \", len(input_filtered), \", \", len(output_filtered), \", \", len(texts_filtered))\n",
    "\n",
    "#Normalize input\n",
    "input_filtered = (input_filtered - input_filtered.min(axis=0)) / (input_filtered.max(axis=0) - input_filtered.min(axis=0))\n",
    "\n",
    "#Normalize output\n",
    "def categorize_output(output):\n",
    "    if (output <= 2.5):\n",
    "        return 0\n",
    "    elif  output >= 4:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# def categorize_output(output):\n",
    "#     return np.around(output)\n",
    "   \n",
    "print(output_filtered.shape)\n",
    "\n",
    "output_filtered = np.array([list(map(lambda x: categorize_output(x), col)) for col in output_filtered])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "axs[0].hist(output_filtered[:,0])\n",
    "axs[0].set_title(\"Distribution Valance\")\n",
    "axs[1].hist(output_filtered[:,1])\n",
    "axs[1].set_title(\"Distribution Arouse\")\n",
    "axs[2].hist(output_filtered[:,2])\n",
    "axs[2].set_title(\"Distribution Dominance\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pprint(output_filtered)\n",
    "output_filtered = [to_categorical(out, 3) for out in output_filtered]\n",
    "#output_filtered = (output_filtered - output_filtered.min(axis=0)) / (output_filtered.max(axis=0) - output_filtered.min(axis=0))\n",
    "print(\"Normalize input and categorize output \")\n",
    "print(len(output_filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffer data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#Shuffer\n",
    "c = list(zip(input_filtered, output_filtered, texts_filtered))\n",
    "random.shuffle(c)\n",
    "input_filtered, output_filtered, texts_filtered= zip( * c)\n",
    "input_filtered = np.array(input_filtered)\n",
    "output_filtered = np.array(output_filtered)\n",
    "text_filtered = np.array(texts_filtered)\n",
    "print(output_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acoustic Features Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "num_features = 88\n",
    "run = False\n",
    "\n",
    "\n",
    "if run:\n",
    "    estimator = SVR(kernel=\"linear\")\n",
    "    selector = RFE(estimator, 5, step=1)\n",
    "    output_int = np.argmax(output_filtered[:,1], axis = 1)\n",
    "    selector = selector.fit(input_filtered, output_int)\n",
    "    print(\"selection support: \", selector.support_) \n",
    "    print(\"selection ranking: \", selector.ranking_)\n",
    "\n",
    "\n",
    "selection_ranking = np.array([1, 66, 11, 73, 4, 1, 7, 47, 8, 24, 1, 23, 27, 28, 40, 41, \n",
    " 6, 78, 30, 77, 16, 84, 10, 67, 3, 54, 76, 68, 56, 65, 31, 9, 55, 81, 21, 61, 38, \n",
    " 14, 17, 39, 51, 70, 62, 79, 43, 44, 57, 22, 69, 53, 34, 83, 35, 15, 80, 42, 18, 72,\n",
    " 12, 60, 25, 52, 1, 58, 59, 64, 75, 74, 6, 33, 20, 45, 5, 29, 49, 48, 13, 32, 82, 19, 63, 36, \n",
    " 1, 2, 46, 37, 50, 71])\n",
    "\n",
    "## Get high-rank features\n",
    "input_filtered = np.array(list(map(lambda x: x[selection_ranking <= num_features], input_filtered)))\n",
    "\n",
    "num_features = len(input_filtered[0])\n",
    "print(len(input_filtered[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Get Textual Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "VALANCE_TYPE = ['compound', 'neg', 'neu', 'pos']\n",
    "\n",
    "def get_textual_features(text):\n",
    "    ss = sid.polarity_scores(text)\n",
    "    return [ss['compound'], ss['neg'], ss['neu'], ss['pos']]\n",
    "    \n",
    "\n",
    "textual_fearures = np.array([get_textual_features(el) for el in text_filtered])\n",
    "\n",
    "#print(textual_fearures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze textual and output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Compund values distribution of Hight valance')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_valance_int = np.argmax(output_filtered[:,0], axis=1)\n",
    "compounds = textual_fearures[:,0]\n",
    "\n",
    "\n",
    "low_valance = compounds[output_valance_int == 0]\n",
    "moderate_valance = compounds[output_valance_int == 1]\n",
    "high_valance = compounds[output_valance_int == 2]\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "axs[0].hist(low_valance)\n",
    "axs[0].set_title(\"Compund values distribution of Low valance\")\n",
    "\n",
    "axs[1].hist(moderate_valance)\n",
    "axs[1].set_title(\"Compund values distribution of Moderate valance\")\n",
    "\n",
    "axs[2].hist(high_valance)\n",
    "axs[2].set_title(\"Compund values distribution of Hight valance\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Combine acoustic and textual **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10009, 88)\n",
      "(10009, 3)\n",
      "Length input_filtered after combination:  91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acoustic_features = input_filtered\n",
    "print(input_filtered.shape)\n",
    "print(textual_fearures.shape)\n",
    "\n",
    "input_filtered = np.column_stack((acoustic_features, textual_fearures))\n",
    "\n",
    "print(\"Length input_filtered after combination: \", len(input_filtered[0]))\n",
    "\n",
    "#input_filtered= input_filtered[:,-1]\n",
    "num_features = len(input_filtered[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training, testing set:  8007 ,  2002\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_filtered, output_filtered, test_size=0.2, random_state=300)\n",
    "print(\"Size training, testing set: \", len(X_train), \", \", len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Training on keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8007 samples, validate on 2002 samples\n",
      "Epoch 1/100\n",
      "8007/8007 [==============================] - ETA: 3:12 - loss: 1.0986 - acc: 0.281 - ETA: 5s - loss: 1.0933 - acc: 0.5076  - ETA: 2s - loss: 1.0867 - acc: 0.522 - ETA: 1s - loss: 1.0736 - acc: 0.530 - ETA: 1s - loss: 1.0601 - acc: 0.525 - ETA: 0s - loss: 1.0416 - acc: 0.531 - ETA: 0s - loss: 1.0341 - acc: 0.533 - ETA: 0s - loss: 1.0263 - acc: 0.536 - 1s 154us/step - loss: 1.0220 - acc: 0.5380 - val_loss: 1.0279 - val_acc: 0.5060\n",
      "Epoch 2/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0021 - acc: 0.531 - ETA: 0s - loss: 0.9901 - acc: 0.540 - ETA: 0s - loss: 0.9878 - acc: 0.544 - ETA: 0s - loss: 0.9941 - acc: 0.538 - ETA: 0s - loss: 0.9968 - acc: 0.536 - ETA: 0s - loss: 0.9963 - acc: 0.538 - ETA: 0s - loss: 0.9915 - acc: 0.543 - ETA: 0s - loss: 0.9944 - acc: 0.540 - 0s 49us/step - loss: 0.9943 - acc: 0.5402 - val_loss: 1.0212 - val_acc: 0.5060\n",
      "Epoch 3/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0230 - acc: 0.500 - ETA: 0s - loss: 1.0092 - acc: 0.527 - ETA: 0s - loss: 1.0012 - acc: 0.532 - ETA: 0s - loss: 0.9960 - acc: 0.536 - ETA: 0s - loss: 0.9963 - acc: 0.534 - ETA: 0s - loss: 0.9917 - acc: 0.538 - ETA: 0s - loss: 0.9897 - acc: 0.539 - ETA: 0s - loss: 0.9908 - acc: 0.539 - 0s 54us/step - loss: 0.9894 - acc: 0.5402 - val_loss: 1.0182 - val_acc: 0.5060\n",
      "Epoch 4/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9013 - acc: 0.625 - ETA: 0s - loss: 0.9668 - acc: 0.560 - ETA: 0s - loss: 0.9846 - acc: 0.534 - ETA: 0s - loss: 0.9841 - acc: 0.535 - ETA: 0s - loss: 0.9850 - acc: 0.534 - ETA: 0s - loss: 0.9831 - acc: 0.536 - ETA: 0s - loss: 0.9826 - acc: 0.537 - ETA: 0s - loss: 0.9806 - acc: 0.539 - 0s 51us/step - loss: 0.9797 - acc: 0.5402 - val_loss: 1.0012 - val_acc: 0.5060\n",
      "Epoch 5/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0248 - acc: 0.531 - ETA: 0s - loss: 0.9608 - acc: 0.543 - ETA: 0s - loss: 0.9576 - acc: 0.548 - ETA: 0s - loss: 0.9624 - acc: 0.543 - ETA: 0s - loss: 0.9637 - acc: 0.542 - ETA: 0s - loss: 0.9656 - acc: 0.542 - ETA: 0s - loss: 0.9650 - acc: 0.544 - ETA: 0s - loss: 0.9593 - acc: 0.547 - 0s 50us/step - loss: 0.9594 - acc: 0.5473 - val_loss: 0.9712 - val_acc: 0.5360\n",
      "Epoch 6/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9489 - acc: 0.531 - ETA: 0s - loss: 0.9161 - acc: 0.589 - ETA: 0s - loss: 0.9360 - acc: 0.575 - ETA: 0s - loss: 0.9390 - acc: 0.571 - ETA: 0s - loss: 0.9400 - acc: 0.567 - ETA: 0s - loss: 0.9391 - acc: 0.570 - ETA: 0s - loss: 0.9380 - acc: 0.571 - ETA: 0s - loss: 0.9386 - acc: 0.570 - 0s 54us/step - loss: 0.9427 - acc: 0.5669 - val_loss: 0.9599 - val_acc: 0.5405\n",
      "Epoch 7/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0531 - acc: 0.500 - ETA: 0s - loss: 0.9466 - acc: 0.567 - ETA: 0s - loss: 0.9393 - acc: 0.577 - ETA: 0s - loss: 0.9452 - acc: 0.566 - ETA: 0s - loss: 0.9406 - acc: 0.568 - ETA: 0s - loss: 0.9385 - acc: 0.570 - ETA: 0s - loss: 0.9379 - acc: 0.569 - 0s 47us/step - loss: 0.9376 - acc: 0.5696 - val_loss: 0.9558 - val_acc: 0.5440\n",
      "Epoch 8/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8986 - acc: 0.593 - ETA: 0s - loss: 0.9314 - acc: 0.584 - ETA: 0s - loss: 0.9329 - acc: 0.579 - ETA: 0s - loss: 0.9309 - acc: 0.578 - ETA: 0s - loss: 0.9302 - acc: 0.576 - ETA: 0s - loss: 0.9334 - acc: 0.571 - ETA: 0s - loss: 0.9303 - acc: 0.574 - ETA: 0s - loss: 0.9335 - acc: 0.572 - 0s 54us/step - loss: 0.9332 - acc: 0.5725 - val_loss: 0.9615 - val_acc: 0.5420\n",
      "Epoch 9/100\n",
      "8007/8007 [==============================] - ETA: 1s - loss: 0.9552 - acc: 0.562 - ETA: 0s - loss: 0.9224 - acc: 0.582 - ETA: 0s - loss: 0.9244 - acc: 0.585 - ETA: 0s - loss: 0.9290 - acc: 0.577 - ETA: 0s - loss: 0.9285 - acc: 0.576 - ETA: 0s - loss: 0.9293 - acc: 0.573 - ETA: 0s - loss: 0.9302 - acc: 0.572 - ETA: 0s - loss: 0.9313 - acc: 0.572 - 0s 54us/step - loss: 0.9312 - acc: 0.5727 - val_loss: 0.9513 - val_acc: 0.5460\n",
      "Epoch 10/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8383 - acc: 0.687 - ETA: 0s - loss: 0.9320 - acc: 0.583 - ETA: 0s - loss: 0.9327 - acc: 0.571 - ETA: 0s - loss: 0.9227 - acc: 0.578 - ETA: 0s - loss: 0.9250 - acc: 0.574 - ETA: 0s - loss: 0.9320 - acc: 0.570 - ETA: 0s - loss: 0.9288 - acc: 0.573 - ETA: 0s - loss: 0.9295 - acc: 0.572 - 0s 52us/step - loss: 0.9284 - acc: 0.5737 - val_loss: 0.9476 - val_acc: 0.5509\n",
      "Epoch 11/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8395 - acc: 0.687 - ETA: 0s - loss: 0.9185 - acc: 0.587 - ETA: 0s - loss: 0.9287 - acc: 0.578 - ETA: 0s - loss: 0.9260 - acc: 0.579 - ETA: 0s - loss: 0.9296 - acc: 0.577 - ETA: 0s - loss: 0.9261 - acc: 0.578 - ETA: 0s - loss: 0.9285 - acc: 0.577 - ETA: 0s - loss: 0.9311 - acc: 0.573 - ETA: 0s - loss: 0.9293 - acc: 0.573 - 0s 59us/step - loss: 0.9273 - acc: 0.5745 - val_loss: 0.9512 - val_acc: 0.5495\n",
      "Epoch 12/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8216 - acc: 0.687 - ETA: 0s - loss: 0.9231 - acc: 0.581 - ETA: 0s - loss: 0.9228 - acc: 0.586 - ETA: 0s - loss: 0.9213 - acc: 0.584 - ETA: 0s - loss: 0.9203 - acc: 0.584 - ETA: 0s - loss: 0.9180 - acc: 0.581 - ETA: 0s - loss: 0.9229 - acc: 0.579 - ETA: 0s - loss: 0.9228 - acc: 0.577 - ETA: 0s - loss: 0.9230 - acc: 0.577 - 0s 59us/step - loss: 0.9242 - acc: 0.5762 - val_loss: 0.9467 - val_acc: 0.5539\n",
      "Epoch 13/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9246 - acc: 0.593 - ETA: 0s - loss: 0.9394 - acc: 0.559 - ETA: 0s - loss: 0.9200 - acc: 0.573 - ETA: 0s - loss: 0.9110 - acc: 0.580 - ETA: 0s - loss: 0.9118 - acc: 0.582 - ETA: 0s - loss: 0.9161 - acc: 0.578 - ETA: 0s - loss: 0.9170 - acc: 0.579 - ETA: 0s - loss: 0.9223 - acc: 0.575 - ETA: 0s - loss: 0.9212 - acc: 0.576 - 0s 58us/step - loss: 0.9222 - acc: 0.5756 - val_loss: 0.9446 - val_acc: 0.5514\n",
      "Epoch 14/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8962 - acc: 0.562 - ETA: 0s - loss: 0.9329 - acc: 0.565 - ETA: 0s - loss: 0.9233 - acc: 0.576 - ETA: 0s - loss: 0.9317 - acc: 0.572 - ETA: 0s - loss: 0.9307 - acc: 0.573 - ETA: 0s - loss: 0.9276 - acc: 0.573 - ETA: 0s - loss: 0.9245 - acc: 0.574 - 0s 47us/step - loss: 0.9214 - acc: 0.5779 - val_loss: 0.9419 - val_acc: 0.5564\n",
      "Epoch 15/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8490 - acc: 0.562 - ETA: 0s - loss: 0.9210 - acc: 0.567 - ETA: 0s - loss: 0.9159 - acc: 0.582 - ETA: 0s - loss: 0.9203 - acc: 0.579 - ETA: 0s - loss: 0.9218 - acc: 0.578 - ETA: 0s - loss: 0.9191 - acc: 0.579 - ETA: 0s - loss: 0.9222 - acc: 0.576 - 0s 47us/step - loss: 0.9201 - acc: 0.5790 - val_loss: 0.9427 - val_acc: 0.5509\n",
      "Epoch 16/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8905 - acc: 0.625 - ETA: 0s - loss: 0.9035 - acc: 0.604 - ETA: 0s - loss: 0.9154 - acc: 0.587 - ETA: 0s - loss: 0.9177 - acc: 0.578 - ETA: 0s - loss: 0.9227 - acc: 0.575 - ETA: 0s - loss: 0.9207 - acc: 0.577 - ETA: 0s - loss: 0.9189 - acc: 0.577 - 0s 47us/step - loss: 0.9184 - acc: 0.5786 - val_loss: 0.9407 - val_acc: 0.5544\n",
      "Epoch 17/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9682 - acc: 0.531 - ETA: 0s - loss: 0.9229 - acc: 0.564 - ETA: 0s - loss: 0.9057 - acc: 0.588 - ETA: 0s - loss: 0.9175 - acc: 0.576 - ETA: 0s - loss: 0.9168 - acc: 0.577 - ETA: 0s - loss: 0.9149 - acc: 0.575 - ETA: 0s - loss: 0.9162 - acc: 0.576 - 0s 47us/step - loss: 0.9186 - acc: 0.5756 - val_loss: 0.9502 - val_acc: 0.5490\n",
      "Epoch 18/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0196 - acc: 0.437 - ETA: 0s - loss: 0.8934 - acc: 0.599 - ETA: 0s - loss: 0.9133 - acc: 0.581 - ETA: 0s - loss: 0.9164 - acc: 0.579 - ETA: 0s - loss: 0.9137 - acc: 0.578 - ETA: 0s - loss: 0.9163 - acc: 0.577 - ETA: 0s - loss: 0.9185 - acc: 0.576 - ETA: 0s - loss: 0.9172 - acc: 0.576 - 0s 54us/step - loss: 0.9186 - acc: 0.5764 - val_loss: 0.9428 - val_acc: 0.5475\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7505 - acc: 0.750 - ETA: 0s - loss: 0.9390 - acc: 0.573 - ETA: 0s - loss: 0.9244 - acc: 0.574 - ETA: 0s - loss: 0.9326 - acc: 0.567 - ETA: 0s - loss: 0.9122 - acc: 0.579 - ETA: 0s - loss: 0.9084 - acc: 0.582 - ETA: 0s - loss: 0.9132 - acc: 0.577 - ETA: 0s - loss: 0.9161 - acc: 0.577 - 0s 49us/step - loss: 0.9168 - acc: 0.5769 - val_loss: 0.9394 - val_acc: 0.5539\n",
      "Epoch 20/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0423 - acc: 0.437 - ETA: 0s - loss: 0.9096 - acc: 0.575 - ETA: 0s - loss: 0.9105 - acc: 0.583 - ETA: 0s - loss: 0.9154 - acc: 0.580 - ETA: 0s - loss: 0.9152 - acc: 0.579 - ETA: 0s - loss: 0.9173 - acc: 0.578 - ETA: 0s - loss: 0.9144 - acc: 0.581 - ETA: 0s - loss: 0.9159 - acc: 0.579 - 0s 52us/step - loss: 0.9162 - acc: 0.5790 - val_loss: 0.9368 - val_acc: 0.5564\n",
      "Epoch 21/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9697 - acc: 0.531 - ETA: 0s - loss: 0.9313 - acc: 0.556 - ETA: 0s - loss: 0.9320 - acc: 0.561 - ETA: 0s - loss: 0.9251 - acc: 0.567 - ETA: 0s - loss: 0.9173 - acc: 0.577 - ETA: 0s - loss: 0.9181 - acc: 0.575 - ETA: 0s - loss: 0.9136 - acc: 0.581 - ETA: 0s - loss: 0.9143 - acc: 0.579 - ETA: 0s - loss: 0.9151 - acc: 0.578 - 0s 57us/step - loss: 0.9150 - acc: 0.5791 - val_loss: 0.9481 - val_acc: 0.5480\n",
      "Epoch 22/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9347 - acc: 0.593 - ETA: 0s - loss: 0.9226 - acc: 0.564 - ETA: 0s - loss: 0.9220 - acc: 0.565 - ETA: 0s - loss: 0.9326 - acc: 0.558 - ETA: 0s - loss: 0.9238 - acc: 0.567 - ETA: 0s - loss: 0.9179 - acc: 0.572 - ETA: 0s - loss: 0.9190 - acc: 0.575 - ETA: 0s - loss: 0.9139 - acc: 0.579 - 0s 52us/step - loss: 0.9149 - acc: 0.5785 - val_loss: 0.9354 - val_acc: 0.5559\n",
      "Epoch 23/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0082 - acc: 0.468 - ETA: 0s - loss: 0.9107 - acc: 0.579 - ETA: 0s - loss: 0.9378 - acc: 0.561 - ETA: 0s - loss: 0.9284 - acc: 0.568 - ETA: 0s - loss: 0.9345 - acc: 0.564 - ETA: 0s - loss: 0.9250 - acc: 0.568 - ETA: 0s - loss: 0.9167 - acc: 0.574 - ETA: 0s - loss: 0.9189 - acc: 0.575 - ETA: 0s - loss: 0.9177 - acc: 0.576 - 0s 59us/step - loss: 0.9157 - acc: 0.5784 - val_loss: 0.9409 - val_acc: 0.5500\n",
      "Epoch 24/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0328 - acc: 0.437 - ETA: 0s - loss: 0.9134 - acc: 0.572 - ETA: 0s - loss: 0.9117 - acc: 0.590 - ETA: 0s - loss: 0.9154 - acc: 0.580 - ETA: 0s - loss: 0.9133 - acc: 0.581 - ETA: 0s - loss: 0.9158 - acc: 0.578 - ETA: 0s - loss: 0.9182 - acc: 0.576 - ETA: 0s - loss: 0.9161 - acc: 0.576 - 0s 53us/step - loss: 0.9147 - acc: 0.5786 - val_loss: 0.9360 - val_acc: 0.5524\n",
      "Epoch 25/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8837 - acc: 0.625 - ETA: 0s - loss: 0.9032 - acc: 0.579 - ETA: 0s - loss: 0.9050 - acc: 0.582 - ETA: 0s - loss: 0.9108 - acc: 0.578 - ETA: 0s - loss: 0.9091 - acc: 0.579 - ETA: 0s - loss: 0.9131 - acc: 0.580 - ETA: 0s - loss: 0.9099 - acc: 0.584 - ETA: 0s - loss: 0.9106 - acc: 0.584 - 0s 51us/step - loss: 0.9128 - acc: 0.5815 - val_loss: 0.9339 - val_acc: 0.5534\n",
      "Epoch 26/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7930 - acc: 0.593 - ETA: 0s - loss: 0.9070 - acc: 0.578 - ETA: 0s - loss: 0.9170 - acc: 0.564 - ETA: 0s - loss: 0.9021 - acc: 0.578 - ETA: 0s - loss: 0.9039 - acc: 0.580 - ETA: 0s - loss: 0.9115 - acc: 0.576 - ETA: 0s - loss: 0.9089 - acc: 0.579 - ETA: 0s - loss: 0.9105 - acc: 0.579 - 0s 53us/step - loss: 0.9117 - acc: 0.5792 - val_loss: 0.9332 - val_acc: 0.5539\n",
      "Epoch 27/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9621 - acc: 0.593 - ETA: 0s - loss: 0.9256 - acc: 0.566 - ETA: 0s - loss: 0.9151 - acc: 0.576 - ETA: 0s - loss: 0.9103 - acc: 0.579 - ETA: 0s - loss: 0.9161 - acc: 0.574 - ETA: 0s - loss: 0.9152 - acc: 0.573 - ETA: 0s - loss: 0.9147 - acc: 0.574 - ETA: 0s - loss: 0.9156 - acc: 0.575 - 0s 55us/step - loss: 0.9123 - acc: 0.5780 - val_loss: 0.9331 - val_acc: 0.5544\n",
      "Epoch 28/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8960 - acc: 0.531 - ETA: 0s - loss: 0.9115 - acc: 0.579 - ETA: 0s - loss: 0.9096 - acc: 0.578 - ETA: 0s - loss: 0.9052 - acc: 0.583 - ETA: 0s - loss: 0.9145 - acc: 0.575 - ETA: 0s - loss: 0.9172 - acc: 0.574 - ETA: 0s - loss: 0.9164 - acc: 0.574 - ETA: 0s - loss: 0.9114 - acc: 0.581 - ETA: 0s - loss: 0.9123 - acc: 0.580 - 0s 58us/step - loss: 0.9110 - acc: 0.5811 - val_loss: 0.9324 - val_acc: 0.5514\n",
      "Epoch 29/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9848 - acc: 0.500 - ETA: 0s - loss: 0.8925 - acc: 0.577 - ETA: 0s - loss: 0.9108 - acc: 0.571 - ETA: 0s - loss: 0.9164 - acc: 0.573 - ETA: 0s - loss: 0.9207 - acc: 0.570 - ETA: 0s - loss: 0.9163 - acc: 0.574 - ETA: 0s - loss: 0.9122 - acc: 0.578 - ETA: 0s - loss: 0.9117 - acc: 0.578 - ETA: 0s - loss: 0.9103 - acc: 0.580 - 0s 56us/step - loss: 0.9102 - acc: 0.5804 - val_loss: 0.9315 - val_acc: 0.5539\n",
      "Epoch 30/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8113 - acc: 0.625 - ETA: 0s - loss: 0.9215 - acc: 0.573 - ETA: 0s - loss: 0.9248 - acc: 0.564 - ETA: 0s - loss: 0.9215 - acc: 0.570 - ETA: 0s - loss: 0.9166 - acc: 0.575 - ETA: 0s - loss: 0.9072 - acc: 0.583 - ETA: 0s - loss: 0.9055 - acc: 0.584 - ETA: 0s - loss: 0.9060 - acc: 0.583 - 0s 53us/step - loss: 0.9088 - acc: 0.5820 - val_loss: 0.9306 - val_acc: 0.5539\n",
      "Epoch 31/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8501 - acc: 0.593 - ETA: 0s - loss: 0.9195 - acc: 0.561 - ETA: 0s - loss: 0.9040 - acc: 0.581 - ETA: 0s - loss: 0.9122 - acc: 0.578 - ETA: 0s - loss: 0.9141 - acc: 0.577 - ETA: 0s - loss: 0.9130 - acc: 0.578 - ETA: 0s - loss: 0.9132 - acc: 0.576 - ETA: 0s - loss: 0.9094 - acc: 0.581 - ETA: 0s - loss: 0.9122 - acc: 0.580 - 0s 58us/step - loss: 0.9089 - acc: 0.5825 - val_loss: 0.9353 - val_acc: 0.5485\n",
      "Epoch 32/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.1715 - acc: 0.437 - ETA: 0s - loss: 0.9184 - acc: 0.574 - ETA: 0s - loss: 0.9100 - acc: 0.580 - ETA: 0s - loss: 0.9115 - acc: 0.581 - ETA: 0s - loss: 0.9063 - acc: 0.585 - ETA: 0s - loss: 0.9077 - acc: 0.584 - ETA: 0s - loss: 0.9061 - acc: 0.584 - ETA: 0s - loss: 0.9065 - acc: 0.582 - 0s 54us/step - loss: 0.9085 - acc: 0.5805 - val_loss: 0.9436 - val_acc: 0.5509\n",
      "Epoch 33/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9270 - acc: 0.656 - ETA: 0s - loss: 0.9017 - acc: 0.584 - ETA: 0s - loss: 0.8966 - acc: 0.588 - ETA: 0s - loss: 0.9071 - acc: 0.579 - ETA: 0s - loss: 0.9070 - acc: 0.576 - ETA: 0s - loss: 0.9073 - acc: 0.576 - ETA: 0s - loss: 0.9066 - acc: 0.578 - ETA: 0s - loss: 0.9096 - acc: 0.577 - 0s 52us/step - loss: 0.9079 - acc: 0.5801 - val_loss: 0.9294 - val_acc: 0.5500\n",
      "Epoch 34/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9532 - acc: 0.531 - ETA: 0s - loss: 0.8974 - acc: 0.589 - ETA: 0s - loss: 0.9093 - acc: 0.580 - ETA: 0s - loss: 0.9049 - acc: 0.584 - ETA: 0s - loss: 0.9062 - acc: 0.583 - ETA: 0s - loss: 0.9027 - acc: 0.585 - ETA: 0s - loss: 0.9042 - acc: 0.584 - ETA: 0s - loss: 0.9055 - acc: 0.583 - 0s 51us/step - loss: 0.9074 - acc: 0.5821 - val_loss: 0.9290 - val_acc: 0.5519\n",
      "Epoch 35/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7742 - acc: 0.593 - ETA: 0s - loss: 0.9003 - acc: 0.574 - ETA: 0s - loss: 0.8983 - acc: 0.576 - ETA: 0s - loss: 0.8947 - acc: 0.584 - ETA: 0s - loss: 0.9029 - acc: 0.580 - ETA: 0s - loss: 0.9028 - acc: 0.582 - ETA: 0s - loss: 0.9113 - acc: 0.575 - ETA: 0s - loss: 0.9057 - acc: 0.580 - 0s 49us/step - loss: 0.9056 - acc: 0.5800 - val_loss: 0.9307 - val_acc: 0.5509\n",
      "Epoch 36/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9433 - acc: 0.562 - ETA: 0s - loss: 0.9109 - acc: 0.573 - ETA: 0s - loss: 0.9118 - acc: 0.569 - ETA: 0s - loss: 0.9072 - acc: 0.569 - ETA: 0s - loss: 0.9105 - acc: 0.568 - ETA: 0s - loss: 0.9068 - acc: 0.574 - ETA: 0s - loss: 0.9061 - acc: 0.575 - ETA: 0s - loss: 0.9067 - acc: 0.575 - ETA: 0s - loss: 0.9023 - acc: 0.581 - 0s 57us/step - loss: 0.9028 - acc: 0.5811 - val_loss: 0.9285 - val_acc: 0.5509\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8891 - acc: 0.531 - ETA: 0s - loss: 0.8955 - acc: 0.582 - ETA: 0s - loss: 0.9102 - acc: 0.574 - ETA: 0s - loss: 0.9066 - acc: 0.574 - ETA: 0s - loss: 0.9041 - acc: 0.578 - ETA: 0s - loss: 0.9028 - acc: 0.578 - ETA: 0s - loss: 0.9005 - acc: 0.581 - ETA: 0s - loss: 0.8979 - acc: 0.582 - 0s 49us/step - loss: 0.8987 - acc: 0.5826 - val_loss: 0.9228 - val_acc: 0.5509\n",
      "Epoch 38/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9071 - acc: 0.625 - ETA: 0s - loss: 0.8785 - acc: 0.596 - ETA: 0s - loss: 0.8937 - acc: 0.584 - ETA: 0s - loss: 0.8954 - acc: 0.584 - ETA: 0s - loss: 0.8918 - acc: 0.584 - ETA: 0s - loss: 0.8917 - acc: 0.586 - ETA: 0s - loss: 0.8953 - acc: 0.582 - 0s 44us/step - loss: 0.8952 - acc: 0.5835 - val_loss: 0.9263 - val_acc: 0.5534\n",
      "Epoch 39/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8529 - acc: 0.625 - ETA: 0s - loss: 0.8962 - acc: 0.562 - ETA: 0s - loss: 0.8950 - acc: 0.578 - ETA: 0s - loss: 0.8922 - acc: 0.581 - ETA: 0s - loss: 0.8914 - acc: 0.582 - ETA: 0s - loss: 0.8909 - acc: 0.582 - ETA: 0s - loss: 0.8958 - acc: 0.576 - ETA: 0s - loss: 0.8972 - acc: 0.579 - 0s 54us/step - loss: 0.8934 - acc: 0.5812 - val_loss: 0.9185 - val_acc: 0.5574\n",
      "Epoch 40/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9356 - acc: 0.531 - ETA: 0s - loss: 0.8829 - acc: 0.591 - ETA: 0s - loss: 0.8799 - acc: 0.586 - ETA: 0s - loss: 0.8962 - acc: 0.576 - ETA: 0s - loss: 0.8980 - acc: 0.577 - ETA: 0s - loss: 0.8978 - acc: 0.576 - ETA: 0s - loss: 0.8949 - acc: 0.577 - 0s 48us/step - loss: 0.8909 - acc: 0.5820 - val_loss: 0.9197 - val_acc: 0.5574\n",
      "Epoch 41/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8650 - acc: 0.625 - ETA: 0s - loss: 0.8712 - acc: 0.596 - ETA: 0s - loss: 0.8795 - acc: 0.589 - ETA: 0s - loss: 0.8890 - acc: 0.588 - ETA: 0s - loss: 0.8924 - acc: 0.583 - ETA: 0s - loss: 0.8999 - acc: 0.578 - ETA: 0s - loss: 0.8956 - acc: 0.581 - ETA: 0s - loss: 0.8947 - acc: 0.579 - ETA: 0s - loss: 0.8911 - acc: 0.581 - 0s 58us/step - loss: 0.8906 - acc: 0.5826 - val_loss: 0.9143 - val_acc: 0.5629\n",
      "Epoch 42/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8983 - acc: 0.562 - ETA: 0s - loss: 0.8998 - acc: 0.584 - ETA: 0s - loss: 0.8828 - acc: 0.584 - ETA: 0s - loss: 0.8865 - acc: 0.583 - ETA: 0s - loss: 0.8824 - acc: 0.586 - ETA: 0s - loss: 0.8916 - acc: 0.581 - ETA: 0s - loss: 0.8899 - acc: 0.583 - 0s 47us/step - loss: 0.8893 - acc: 0.5835 - val_loss: 0.9203 - val_acc: 0.5604\n",
      "Epoch 43/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8908 - acc: 0.625 - ETA: 0s - loss: 0.8801 - acc: 0.595 - ETA: 0s - loss: 0.8779 - acc: 0.590 - ETA: 0s - loss: 0.8845 - acc: 0.583 - ETA: 0s - loss: 0.8850 - acc: 0.583 - ETA: 0s - loss: 0.8874 - acc: 0.584 - ETA: 0s - loss: 0.8875 - acc: 0.583 - 0s 45us/step - loss: 0.8880 - acc: 0.5835 - val_loss: 0.9154 - val_acc: 0.5544\n",
      "Epoch 44/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8615 - acc: 0.687 - ETA: 0s - loss: 0.8967 - acc: 0.577 - ETA: 0s - loss: 0.8876 - acc: 0.580 - ETA: 0s - loss: 0.8813 - acc: 0.585 - ETA: 0s - loss: 0.8803 - acc: 0.591 - ETA: 0s - loss: 0.8815 - acc: 0.591 - ETA: 0s - loss: 0.8844 - acc: 0.587 - 0s 46us/step - loss: 0.8862 - acc: 0.5862 - val_loss: 0.9171 - val_acc: 0.5619\n",
      "Epoch 45/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8690 - acc: 0.593 - ETA: 0s - loss: 0.8901 - acc: 0.582 - ETA: 0s - loss: 0.8989 - acc: 0.580 - ETA: 0s - loss: 0.8862 - acc: 0.585 - ETA: 0s - loss: 0.8821 - acc: 0.585 - ETA: 0s - loss: 0.8875 - acc: 0.583 - ETA: 0s - loss: 0.8895 - acc: 0.583 - 0s 46us/step - loss: 0.8870 - acc: 0.5845 - val_loss: 0.9158 - val_acc: 0.5579\n",
      "Epoch 46/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0195 - acc: 0.468 - ETA: 0s - loss: 0.8676 - acc: 0.593 - ETA: 0s - loss: 0.8914 - acc: 0.581 - ETA: 0s - loss: 0.8854 - acc: 0.585 - ETA: 0s - loss: 0.8874 - acc: 0.583 - ETA: 0s - loss: 0.8869 - acc: 0.585 - ETA: 0s - loss: 0.8882 - acc: 0.581 - 0s 47us/step - loss: 0.8858 - acc: 0.5824 - val_loss: 0.9153 - val_acc: 0.5639\n",
      "Epoch 47/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8339 - acc: 0.625 - ETA: 0s - loss: 0.8731 - acc: 0.592 - ETA: 0s - loss: 0.8918 - acc: 0.573 - ETA: 0s - loss: 0.8822 - acc: 0.577 - ETA: 0s - loss: 0.8856 - acc: 0.576 - ETA: 0s - loss: 0.8852 - acc: 0.575 - ETA: 0s - loss: 0.8853 - acc: 0.578 - ETA: 0s - loss: 0.8886 - acc: 0.575 - ETA: 0s - loss: 0.8819 - acc: 0.582 - 0s 57us/step - loss: 0.8858 - acc: 0.5799 - val_loss: 0.9130 - val_acc: 0.5624\n",
      "Epoch 48/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9327 - acc: 0.625 - ETA: 0s - loss: 0.8947 - acc: 0.567 - ETA: 0s - loss: 0.8812 - acc: 0.584 - ETA: 0s - loss: 0.8842 - acc: 0.585 - ETA: 0s - loss: 0.8846 - acc: 0.585 - ETA: 0s - loss: 0.8836 - acc: 0.590 - ETA: 0s - loss: 0.8870 - acc: 0.585 - 0s 48us/step - loss: 0.8855 - acc: 0.5854 - val_loss: 0.9154 - val_acc: 0.5614\n",
      "Epoch 49/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8582 - acc: 0.562 - ETA: 0s - loss: 0.8527 - acc: 0.608 - ETA: 0s - loss: 0.8895 - acc: 0.581 - ETA: 0s - loss: 0.8747 - acc: 0.587 - ETA: 0s - loss: 0.8768 - acc: 0.584 - ETA: 0s - loss: 0.8799 - acc: 0.585 - ETA: 0s - loss: 0.8845 - acc: 0.582 - ETA: 0s - loss: 0.8834 - acc: 0.583 - 0s 54us/step - loss: 0.8849 - acc: 0.5826 - val_loss: 0.9123 - val_acc: 0.5634\n",
      "Epoch 50/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8736 - acc: 0.625 - ETA: 0s - loss: 0.8631 - acc: 0.596 - ETA: 0s - loss: 0.8907 - acc: 0.588 - ETA: 0s - loss: 0.8866 - acc: 0.586 - ETA: 0s - loss: 0.8818 - acc: 0.587 - ETA: 0s - loss: 0.8811 - acc: 0.587 - ETA: 0s - loss: 0.8818 - acc: 0.586 - ETA: 0s - loss: 0.8809 - acc: 0.586 - 0s 54us/step - loss: 0.8840 - acc: 0.5854 - val_loss: 0.9135 - val_acc: 0.5599\n",
      "Epoch 51/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8809 - acc: 0.500 - ETA: 0s - loss: 0.9127 - acc: 0.563 - ETA: 0s - loss: 0.8944 - acc: 0.576 - ETA: 0s - loss: 0.8937 - acc: 0.581 - ETA: 0s - loss: 0.8922 - acc: 0.581 - ETA: 0s - loss: 0.8891 - acc: 0.584 - ETA: 0s - loss: 0.8900 - acc: 0.579 - ETA: 0s - loss: 0.8880 - acc: 0.582 - ETA: 0s - loss: 0.8828 - acc: 0.584 - 0s 56us/step - loss: 0.8830 - acc: 0.5841 - val_loss: 0.9144 - val_acc: 0.5634\n",
      "Epoch 52/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9501 - acc: 0.531 - ETA: 0s - loss: 0.8460 - acc: 0.607 - ETA: 0s - loss: 0.8732 - acc: 0.590 - ETA: 0s - loss: 0.8721 - acc: 0.590 - ETA: 0s - loss: 0.8822 - acc: 0.585 - ETA: 0s - loss: 0.8776 - acc: 0.588 - ETA: 0s - loss: 0.8833 - acc: 0.584 - ETA: 0s - loss: 0.8855 - acc: 0.584 - 0s 54us/step - loss: 0.8830 - acc: 0.5847 - val_loss: 0.9124 - val_acc: 0.5609\n",
      "Epoch 53/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8903 - acc: 0.593 - ETA: 0s - loss: 0.8465 - acc: 0.600 - ETA: 0s - loss: 0.8695 - acc: 0.587 - ETA: 0s - loss: 0.8787 - acc: 0.577 - ETA: 0s - loss: 0.8752 - acc: 0.583 - ETA: 0s - loss: 0.8802 - acc: 0.583 - ETA: 0s - loss: 0.8787 - acc: 0.588 - ETA: 0s - loss: 0.8806 - acc: 0.586 - ETA: 0s - loss: 0.8826 - acc: 0.584 - 0s 59us/step - loss: 0.8828 - acc: 0.5825 - val_loss: 0.9125 - val_acc: 0.5604\n",
      "Epoch 54/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8079 - acc: 0.593 - ETA: 0s - loss: 0.9138 - acc: 0.564 - ETA: 0s - loss: 0.9092 - acc: 0.561 - ETA: 0s - loss: 0.8992 - acc: 0.563 - ETA: 0s - loss: 0.8952 - acc: 0.573 - ETA: 0s - loss: 0.8938 - acc: 0.580 - ETA: 0s - loss: 0.8923 - acc: 0.580 - ETA: 0s - loss: 0.8833 - acc: 0.583 - 0s 52us/step - loss: 0.8813 - acc: 0.5840 - val_loss: 0.9196 - val_acc: 0.5624\n",
      "Epoch 55/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0865 - acc: 0.406 - ETA: 0s - loss: 0.8979 - acc: 0.559 - ETA: 0s - loss: 0.9003 - acc: 0.565 - ETA: 0s - loss: 0.9043 - acc: 0.562 - ETA: 0s - loss: 0.8922 - acc: 0.577 - ETA: 0s - loss: 0.8796 - acc: 0.584 - ETA: 0s - loss: 0.8809 - acc: 0.581 - ETA: 0s - loss: 0.8843 - acc: 0.581 - ETA: 0s - loss: 0.8825 - acc: 0.582 - 0s 59us/step - loss: 0.8821 - acc: 0.5837 - val_loss: 0.9118 - val_acc: 0.5609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8287 - acc: 0.656 - ETA: 0s - loss: 0.8604 - acc: 0.590 - ETA: 0s - loss: 0.8763 - acc: 0.591 - ETA: 0s - loss: 0.8831 - acc: 0.585 - ETA: 0s - loss: 0.8856 - acc: 0.585 - ETA: 0s - loss: 0.8880 - acc: 0.581 - ETA: 0s - loss: 0.8918 - acc: 0.581 - ETA: 0s - loss: 0.8889 - acc: 0.582 - ETA: 0s - loss: 0.8847 - acc: 0.584 - 0s 60us/step - loss: 0.8817 - acc: 0.5860 - val_loss: 0.9145 - val_acc: 0.5634\n",
      "Epoch 57/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7755 - acc: 0.687 - ETA: 0s - loss: 0.8902 - acc: 0.581 - ETA: 0s - loss: 0.8758 - acc: 0.586 - ETA: 0s - loss: 0.8747 - acc: 0.591 - ETA: 0s - loss: 0.8792 - acc: 0.591 - ETA: 0s - loss: 0.8810 - acc: 0.590 - ETA: 0s - loss: 0.8799 - acc: 0.591 - ETA: 0s - loss: 0.8820 - acc: 0.589 - ETA: 0s - loss: 0.8846 - acc: 0.587 - ETA: 0s - loss: 0.8805 - acc: 0.587 - 1s 64us/step - loss: 0.8817 - acc: 0.5871 - val_loss: 0.9145 - val_acc: 0.5624\n",
      "Epoch 58/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.2110 - acc: 0.500 - ETA: 0s - loss: 0.8977 - acc: 0.572 - ETA: 0s - loss: 0.8893 - acc: 0.571 - ETA: 0s - loss: 0.8852 - acc: 0.576 - ETA: 0s - loss: 0.8750 - acc: 0.588 - ETA: 0s - loss: 0.8779 - acc: 0.589 - ETA: 0s - loss: 0.8785 - acc: 0.589 - ETA: 0s - loss: 0.8811 - acc: 0.586 - ETA: 0s - loss: 0.8827 - acc: 0.585 - ETA: 0s - loss: 0.8829 - acc: 0.585 - 1s 65us/step - loss: 0.8813 - acc: 0.5855 - val_loss: 0.9214 - val_acc: 0.5574\n",
      "Epoch 59/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8795 - acc: 0.593 - ETA: 0s - loss: 0.8730 - acc: 0.595 - ETA: 0s - loss: 0.8829 - acc: 0.587 - ETA: 0s - loss: 0.8865 - acc: 0.581 - ETA: 0s - loss: 0.8885 - acc: 0.580 - ETA: 0s - loss: 0.8824 - acc: 0.584 - ETA: 0s - loss: 0.8776 - acc: 0.584 - ETA: 0s - loss: 0.8864 - acc: 0.581 - ETA: 0s - loss: 0.8851 - acc: 0.581 - ETA: 0s - loss: 0.8834 - acc: 0.581 - ETA: 0s - loss: 0.8822 - acc: 0.581 - 1s 70us/step - loss: 0.8808 - acc: 0.5822 - val_loss: 0.9169 - val_acc: 0.5644\n",
      "Epoch 60/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9940 - acc: 0.593 - ETA: 0s - loss: 0.8627 - acc: 0.602 - ETA: 0s - loss: 0.8886 - acc: 0.578 - ETA: 0s - loss: 0.8872 - acc: 0.581 - ETA: 0s - loss: 0.8883 - acc: 0.581 - ETA: 0s - loss: 0.8886 - acc: 0.580 - ETA: 0s - loss: 0.8785 - acc: 0.587 - ETA: 0s - loss: 0.8805 - acc: 0.584 - 0s 54us/step - loss: 0.8802 - acc: 0.5840 - val_loss: 0.9159 - val_acc: 0.5594\n",
      "Epoch 61/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0349 - acc: 0.531 - ETA: 0s - loss: 0.8963 - acc: 0.572 - ETA: 0s - loss: 0.8936 - acc: 0.575 - ETA: 0s - loss: 0.8850 - acc: 0.581 - ETA: 0s - loss: 0.8782 - acc: 0.584 - ETA: 0s - loss: 0.8783 - acc: 0.586 - ETA: 0s - loss: 0.8794 - acc: 0.588 - ETA: 0s - loss: 0.8807 - acc: 0.586 - 0s 50us/step - loss: 0.8815 - acc: 0.5866 - val_loss: 0.9097 - val_acc: 0.5609\n",
      "Epoch 62/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9668 - acc: 0.593 - ETA: 0s - loss: 0.8493 - acc: 0.611 - ETA: 0s - loss: 0.8818 - acc: 0.592 - ETA: 0s - loss: 0.8695 - acc: 0.592 - ETA: 0s - loss: 0.8736 - acc: 0.588 - ETA: 0s - loss: 0.8808 - acc: 0.584 - ETA: 0s - loss: 0.8787 - acc: 0.587 - ETA: 0s - loss: 0.8768 - acc: 0.587 - 0s 52us/step - loss: 0.8789 - acc: 0.5874 - val_loss: 0.9090 - val_acc: 0.5639\n",
      "Epoch 63/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7744 - acc: 0.687 - ETA: 0s - loss: 0.8990 - acc: 0.568 - ETA: 0s - loss: 0.8872 - acc: 0.579 - ETA: 0s - loss: 0.8817 - acc: 0.589 - ETA: 0s - loss: 0.8821 - acc: 0.587 - ETA: 0s - loss: 0.8846 - acc: 0.587 - ETA: 0s - loss: 0.8793 - acc: 0.589 - ETA: 0s - loss: 0.8808 - acc: 0.588 - 0s 52us/step - loss: 0.8792 - acc: 0.5885 - val_loss: 0.9178 - val_acc: 0.5634\n",
      "Epoch 64/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7737 - acc: 0.687 - ETA: 0s - loss: 0.8959 - acc: 0.584 - ETA: 0s - loss: 0.8680 - acc: 0.591 - ETA: 0s - loss: 0.8849 - acc: 0.578 - ETA: 0s - loss: 0.8803 - acc: 0.579 - ETA: 0s - loss: 0.8802 - acc: 0.580 - ETA: 0s - loss: 0.8830 - acc: 0.580 - 0s 48us/step - loss: 0.8790 - acc: 0.5832 - val_loss: 0.9141 - val_acc: 0.5644\n",
      "Epoch 65/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9140 - acc: 0.593 - ETA: 0s - loss: 0.9104 - acc: 0.583 - ETA: 0s - loss: 0.9065 - acc: 0.583 - ETA: 0s - loss: 0.9014 - acc: 0.577 - ETA: 0s - loss: 0.8936 - acc: 0.582 - ETA: 0s - loss: 0.8879 - acc: 0.584 - ETA: 0s - loss: 0.8823 - acc: 0.588 - ETA: 0s - loss: 0.8802 - acc: 0.588 - 0s 51us/step - loss: 0.8798 - acc: 0.5900 - val_loss: 0.9088 - val_acc: 0.5624\n",
      "Epoch 66/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8985 - acc: 0.625 - ETA: 0s - loss: 0.8606 - acc: 0.611 - ETA: 0s - loss: 0.8604 - acc: 0.605 - ETA: 0s - loss: 0.8718 - acc: 0.596 - ETA: 0s - loss: 0.8747 - acc: 0.594 - ETA: 0s - loss: 0.8745 - acc: 0.591 - ETA: 0s - loss: 0.8794 - acc: 0.592 - ETA: 0s - loss: 0.8812 - acc: 0.590 - ETA: 0s - loss: 0.8797 - acc: 0.590 - 0s 58us/step - loss: 0.8796 - acc: 0.5899 - val_loss: 0.9089 - val_acc: 0.5609\n",
      "Epoch 67/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9302 - acc: 0.562 - ETA: 0s - loss: 0.9131 - acc: 0.570 - ETA: 0s - loss: 0.8912 - acc: 0.585 - ETA: 0s - loss: 0.8890 - acc: 0.579 - ETA: 0s - loss: 0.8937 - acc: 0.571 - ETA: 0s - loss: 0.8938 - acc: 0.574 - ETA: 0s - loss: 0.8905 - acc: 0.576 - ETA: 0s - loss: 0.8850 - acc: 0.582 - ETA: 0s - loss: 0.8789 - acc: 0.587 - 0s 58us/step - loss: 0.8785 - acc: 0.5875 - val_loss: 0.9307 - val_acc: 0.5619\n",
      "Epoch 68/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9514 - acc: 0.500 - ETA: 0s - loss: 0.9126 - acc: 0.544 - ETA: 0s - loss: 0.8834 - acc: 0.578 - ETA: 0s - loss: 0.8885 - acc: 0.581 - ETA: 0s - loss: 0.8890 - acc: 0.577 - ETA: 0s - loss: 0.8847 - acc: 0.580 - ETA: 0s - loss: 0.8869 - acc: 0.579 - ETA: 0s - loss: 0.8828 - acc: 0.582 - ETA: 0s - loss: 0.8804 - acc: 0.585 - 0s 61us/step - loss: 0.8785 - acc: 0.5871 - val_loss: 0.9110 - val_acc: 0.5634\n",
      "Epoch 69/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7797 - acc: 0.656 - ETA: 0s - loss: 0.9093 - acc: 0.574 - ETA: 0s - loss: 0.8945 - acc: 0.579 - ETA: 0s - loss: 0.8809 - acc: 0.587 - ETA: 0s - loss: 0.8840 - acc: 0.586 - ETA: 0s - loss: 0.8854 - acc: 0.583 - ETA: 0s - loss: 0.8865 - acc: 0.582 - ETA: 0s - loss: 0.8789 - acc: 0.586 - 0s 53us/step - loss: 0.8793 - acc: 0.5866 - val_loss: 0.9113 - val_acc: 0.5624\n",
      "Epoch 70/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9171 - acc: 0.625 - ETA: 0s - loss: 0.8741 - acc: 0.585 - ETA: 0s - loss: 0.8800 - acc: 0.579 - ETA: 0s - loss: 0.8818 - acc: 0.577 - ETA: 0s - loss: 0.8812 - acc: 0.579 - ETA: 0s - loss: 0.8795 - acc: 0.583 - ETA: 0s - loss: 0.8797 - acc: 0.585 - ETA: 0s - loss: 0.8807 - acc: 0.585 - ETA: 0s - loss: 0.8784 - acc: 0.585 - 0s 56us/step - loss: 0.8784 - acc: 0.5857 - val_loss: 0.9097 - val_acc: 0.5629\n",
      "Epoch 71/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7949 - acc: 0.593 - ETA: 0s - loss: 0.8954 - acc: 0.571 - ETA: 0s - loss: 0.8681 - acc: 0.590 - ETA: 0s - loss: 0.8645 - acc: 0.591 - ETA: 0s - loss: 0.8709 - acc: 0.585 - ETA: 0s - loss: 0.8727 - acc: 0.586 - ETA: 0s - loss: 0.8777 - acc: 0.583 - ETA: 0s - loss: 0.8797 - acc: 0.582 - ETA: 0s - loss: 0.8775 - acc: 0.586 - 0s 58us/step - loss: 0.8774 - acc: 0.5879 - val_loss: 0.9196 - val_acc: 0.5604\n",
      "Epoch 72/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0564 - acc: 0.500 - ETA: 0s - loss: 0.8867 - acc: 0.590 - ETA: 0s - loss: 0.8746 - acc: 0.592 - ETA: 0s - loss: 0.8661 - acc: 0.593 - ETA: 0s - loss: 0.8690 - acc: 0.592 - ETA: 0s - loss: 0.8772 - acc: 0.585 - ETA: 0s - loss: 0.8826 - acc: 0.582 - ETA: 0s - loss: 0.8804 - acc: 0.587 - ETA: 0s - loss: 0.8771 - acc: 0.590 - 0s 61us/step - loss: 0.8767 - acc: 0.5907 - val_loss: 0.9097 - val_acc: 0.5619\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8411 - acc: 0.625 - ETA: 0s - loss: 0.8594 - acc: 0.598 - ETA: 0s - loss: 0.8687 - acc: 0.596 - ETA: 0s - loss: 0.8727 - acc: 0.590 - ETA: 0s - loss: 0.8726 - acc: 0.590 - ETA: 0s - loss: 0.8706 - acc: 0.590 - ETA: 0s - loss: 0.8682 - acc: 0.592 - ETA: 0s - loss: 0.8716 - acc: 0.589 - ETA: 0s - loss: 0.8742 - acc: 0.586 - ETA: 0s - loss: 0.8778 - acc: 0.584 - ETA: 0s - loss: 0.8772 - acc: 0.586 - 1s 70us/step - loss: 0.8773 - acc: 0.5861 - val_loss: 0.9161 - val_acc: 0.5619\n",
      "Epoch 74/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9952 - acc: 0.500 - ETA: 0s - loss: 0.8884 - acc: 0.564 - ETA: 0s - loss: 0.8867 - acc: 0.575 - ETA: 0s - loss: 0.8835 - acc: 0.577 - ETA: 0s - loss: 0.8788 - acc: 0.582 - ETA: 0s - loss: 0.8792 - acc: 0.582 - ETA: 0s - loss: 0.8802 - acc: 0.584 - ETA: 0s - loss: 0.8750 - acc: 0.585 - ETA: 0s - loss: 0.8762 - acc: 0.586 - ETA: 0s - loss: 0.8772 - acc: 0.587 - 1s 63us/step - loss: 0.8774 - acc: 0.5875 - val_loss: 0.9219 - val_acc: 0.5599\n",
      "Epoch 75/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7471 - acc: 0.625 - ETA: 0s - loss: 0.8771 - acc: 0.565 - ETA: 0s - loss: 0.8799 - acc: 0.577 - ETA: 0s - loss: 0.8777 - acc: 0.579 - ETA: 0s - loss: 0.8746 - acc: 0.584 - ETA: 0s - loss: 0.8777 - acc: 0.583 - ETA: 0s - loss: 0.8763 - acc: 0.586 - ETA: 0s - loss: 0.8774 - acc: 0.586 - ETA: 0s - loss: 0.8756 - acc: 0.588 - ETA: 0s - loss: 0.8770 - acc: 0.587 - ETA: 0s - loss: 0.8771 - acc: 0.586 - 1s 72us/step - loss: 0.8772 - acc: 0.5861 - val_loss: 0.9102 - val_acc: 0.5604\n",
      "Epoch 76/100\n",
      "8007/8007 [==============================] - ETA: 1s - loss: 0.9117 - acc: 0.562 - ETA: 0s - loss: 0.8676 - acc: 0.600 - ETA: 0s - loss: 0.8647 - acc: 0.589 - ETA: 0s - loss: 0.8686 - acc: 0.591 - ETA: 0s - loss: 0.8745 - acc: 0.588 - ETA: 0s - loss: 0.8682 - acc: 0.594 - ETA: 0s - loss: 0.8737 - acc: 0.591 - ETA: 0s - loss: 0.8768 - acc: 0.587 - ETA: 0s - loss: 0.8774 - acc: 0.585 - ETA: 0s - loss: 0.8758 - acc: 0.589 - ETA: 0s - loss: 0.8781 - acc: 0.587 - 1s 72us/step - loss: 0.8775 - acc: 0.5867 - val_loss: 0.9076 - val_acc: 0.5624\n",
      "Epoch 77/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9419 - acc: 0.531 - ETA: 0s - loss: 0.8916 - acc: 0.556 - ETA: 0s - loss: 0.9013 - acc: 0.563 - ETA: 0s - loss: 0.8893 - acc: 0.575 - ETA: 0s - loss: 0.8783 - acc: 0.587 - ETA: 0s - loss: 0.8774 - acc: 0.588 - ETA: 0s - loss: 0.8735 - acc: 0.594 - ETA: 0s - loss: 0.8754 - acc: 0.591 - ETA: 0s - loss: 0.8727 - acc: 0.591 - 0s 59us/step - loss: 0.8762 - acc: 0.5909 - val_loss: 0.9103 - val_acc: 0.5589\n",
      "Epoch 78/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8636 - acc: 0.625 - ETA: 0s - loss: 0.8639 - acc: 0.590 - ETA: 0s - loss: 0.8742 - acc: 0.585 - ETA: 0s - loss: 0.8670 - acc: 0.590 - ETA: 0s - loss: 0.8684 - acc: 0.594 - ETA: 0s - loss: 0.8699 - acc: 0.594 - ETA: 0s - loss: 0.8717 - acc: 0.594 - ETA: 0s - loss: 0.8707 - acc: 0.593 - ETA: 0s - loss: 0.8749 - acc: 0.589 - ETA: 0s - loss: 0.8754 - acc: 0.589 - 1s 66us/step - loss: 0.8765 - acc: 0.5886 - val_loss: 0.9078 - val_acc: 0.5604\n",
      "Epoch 79/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9440 - acc: 0.500 - ETA: 0s - loss: 0.8438 - acc: 0.613 - ETA: 0s - loss: 0.8611 - acc: 0.592 - ETA: 0s - loss: 0.8757 - acc: 0.594 - ETA: 0s - loss: 0.8751 - acc: 0.593 - ETA: 0s - loss: 0.8755 - acc: 0.592 - ETA: 0s - loss: 0.8761 - acc: 0.588 - ETA: 0s - loss: 0.8780 - acc: 0.587 - ETA: 0s - loss: 0.8812 - acc: 0.585 - 0s 61us/step - loss: 0.8783 - acc: 0.5891 - val_loss: 0.9127 - val_acc: 0.5584\n",
      "Epoch 80/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8366 - acc: 0.718 - ETA: 0s - loss: 0.8789 - acc: 0.584 - ETA: 0s - loss: 0.8873 - acc: 0.580 - ETA: 0s - loss: 0.8781 - acc: 0.580 - ETA: 0s - loss: 0.8646 - acc: 0.591 - ETA: 0s - loss: 0.8706 - acc: 0.587 - ETA: 0s - loss: 0.8736 - acc: 0.584 - ETA: 0s - loss: 0.8727 - acc: 0.585 - ETA: 0s - loss: 0.8748 - acc: 0.586 - ETA: 0s - loss: 0.8751 - acc: 0.588 - 1s 63us/step - loss: 0.8753 - acc: 0.5877 - val_loss: 0.9097 - val_acc: 0.5684\n",
      "Epoch 81/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9903 - acc: 0.500 - ETA: 0s - loss: 0.8720 - acc: 0.590 - ETA: 0s - loss: 0.8635 - acc: 0.608 - ETA: 0s - loss: 0.8784 - acc: 0.589 - ETA: 0s - loss: 0.8779 - acc: 0.587 - ETA: 0s - loss: 0.8812 - acc: 0.584 - ETA: 0s - loss: 0.8791 - acc: 0.586 - ETA: 0s - loss: 0.8825 - acc: 0.584 - ETA: 0s - loss: 0.8781 - acc: 0.586 - 0s 60us/step - loss: 0.8763 - acc: 0.5877 - val_loss: 0.9141 - val_acc: 0.5619\n",
      "Epoch 82/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8999 - acc: 0.593 - ETA: 0s - loss: 0.8543 - acc: 0.622 - ETA: 0s - loss: 0.8668 - acc: 0.604 - ETA: 0s - loss: 0.8680 - acc: 0.596 - ETA: 0s - loss: 0.8694 - acc: 0.595 - ETA: 0s - loss: 0.8628 - acc: 0.601 - ETA: 0s - loss: 0.8729 - acc: 0.592 - ETA: 0s - loss: 0.8744 - acc: 0.592 - ETA: 0s - loss: 0.8770 - acc: 0.589 - 0s 59us/step - loss: 0.8763 - acc: 0.5900 - val_loss: 0.9119 - val_acc: 0.5644\n",
      "Epoch 83/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8366 - acc: 0.781 - ETA: 0s - loss: 0.8646 - acc: 0.590 - ETA: 0s - loss: 0.8763 - acc: 0.583 - ETA: 0s - loss: 0.8757 - acc: 0.583 - ETA: 0s - loss: 0.8755 - acc: 0.585 - ETA: 0s - loss: 0.8748 - acc: 0.587 - ETA: 0s - loss: 0.8719 - acc: 0.590 - ETA: 0s - loss: 0.8737 - acc: 0.592 - ETA: 0s - loss: 0.8759 - acc: 0.588 - 0s 57us/step - loss: 0.8765 - acc: 0.5886 - val_loss: 0.9091 - val_acc: 0.5654\n",
      "Epoch 84/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9371 - acc: 0.531 - ETA: 0s - loss: 0.8883 - acc: 0.572 - ETA: 0s - loss: 0.8733 - acc: 0.584 - ETA: 0s - loss: 0.8811 - acc: 0.582 - ETA: 0s - loss: 0.8753 - acc: 0.585 - ETA: 0s - loss: 0.8764 - acc: 0.583 - ETA: 0s - loss: 0.8801 - acc: 0.582 - ETA: 0s - loss: 0.8767 - acc: 0.585 - ETA: 0s - loss: 0.8756 - acc: 0.586 - 0s 57us/step - loss: 0.8754 - acc: 0.5866 - val_loss: 0.9118 - val_acc: 0.5629\n",
      "Epoch 85/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9371 - acc: 0.593 - ETA: 0s - loss: 0.8314 - acc: 0.618 - ETA: 0s - loss: 0.8465 - acc: 0.605 - ETA: 0s - loss: 0.8580 - acc: 0.600 - ETA: 0s - loss: 0.8745 - acc: 0.589 - ETA: 0s - loss: 0.8731 - acc: 0.589 - ETA: 0s - loss: 0.8769 - acc: 0.586 - ETA: 0s - loss: 0.8741 - acc: 0.590 - 0s 54us/step - loss: 0.8757 - acc: 0.5906 - val_loss: 0.9139 - val_acc: 0.5634\n",
      "Epoch 86/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7238 - acc: 0.687 - ETA: 0s - loss: 0.8894 - acc: 0.591 - ETA: 0s - loss: 0.8881 - acc: 0.588 - ETA: 0s - loss: 0.8877 - acc: 0.586 - ETA: 0s - loss: 0.8938 - acc: 0.583 - ETA: 0s - loss: 0.8881 - acc: 0.586 - ETA: 0s - loss: 0.8843 - acc: 0.587 - ETA: 0s - loss: 0.8808 - acc: 0.587 - 0s 55us/step - loss: 0.8779 - acc: 0.5864 - val_loss: 0.9086 - val_acc: 0.5634\n",
      "Epoch 87/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8099 - acc: 0.625 - ETA: 0s - loss: 0.8837 - acc: 0.587 - ETA: 0s - loss: 0.8869 - acc: 0.575 - ETA: 0s - loss: 0.8854 - acc: 0.578 - ETA: 0s - loss: 0.8856 - acc: 0.578 - ETA: 0s - loss: 0.8815 - acc: 0.582 - ETA: 0s - loss: 0.8824 - acc: 0.581 - ETA: 0s - loss: 0.8773 - acc: 0.588 - ETA: 0s - loss: 0.8762 - acc: 0.589 - 0s 57us/step - loss: 0.8762 - acc: 0.5896 - val_loss: 0.9110 - val_acc: 0.5619\n",
      "Epoch 88/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8673 - acc: 0.687 - ETA: 0s - loss: 0.8602 - acc: 0.607 - ETA: 0s - loss: 0.8751 - acc: 0.594 - ETA: 0s - loss: 0.8783 - acc: 0.592 - ETA: 0s - loss: 0.8771 - acc: 0.592 - ETA: 0s - loss: 0.8775 - acc: 0.592 - ETA: 0s - loss: 0.8729 - acc: 0.593 - ETA: 0s - loss: 0.8716 - acc: 0.594 - ETA: 0s - loss: 0.8754 - acc: 0.590 - 0s 56us/step - loss: 0.8763 - acc: 0.5900 - val_loss: 0.9088 - val_acc: 0.5619\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9795 - acc: 0.468 - ETA: 0s - loss: 0.8881 - acc: 0.579 - ETA: 0s - loss: 0.8698 - acc: 0.592 - ETA: 0s - loss: 0.8646 - acc: 0.595 - ETA: 0s - loss: 0.8688 - acc: 0.591 - ETA: 0s - loss: 0.8741 - acc: 0.593 - ETA: 0s - loss: 0.8743 - acc: 0.596 - ETA: 0s - loss: 0.8777 - acc: 0.592 - ETA: 0s - loss: 0.8761 - acc: 0.591 - 0s 58us/step - loss: 0.8764 - acc: 0.5905 - val_loss: 0.9082 - val_acc: 0.5634\n",
      "Epoch 90/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9729 - acc: 0.562 - ETA: 0s - loss: 0.8790 - acc: 0.593 - ETA: 0s - loss: 0.8906 - acc: 0.579 - ETA: 0s - loss: 0.8910 - acc: 0.578 - ETA: 0s - loss: 0.8785 - acc: 0.586 - ETA: 0s - loss: 0.8701 - acc: 0.588 - ETA: 0s - loss: 0.8747 - acc: 0.588 - ETA: 0s - loss: 0.8761 - acc: 0.587 - 0s 53us/step - loss: 0.8761 - acc: 0.5892 - val_loss: 0.9117 - val_acc: 0.5629\n",
      "Epoch 91/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9769 - acc: 0.437 - ETA: 0s - loss: 0.9347 - acc: 0.553 - ETA: 0s - loss: 0.9060 - acc: 0.567 - ETA: 0s - loss: 0.8947 - acc: 0.578 - ETA: 0s - loss: 0.8897 - acc: 0.584 - ETA: 0s - loss: 0.8863 - acc: 0.585 - ETA: 0s - loss: 0.8759 - acc: 0.591 - ETA: 0s - loss: 0.8751 - acc: 0.591 - 0s 49us/step - loss: 0.8755 - acc: 0.5909 - val_loss: 0.9193 - val_acc: 0.5629\n",
      "Epoch 92/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8325 - acc: 0.593 - ETA: 0s - loss: 0.8999 - acc: 0.572 - ETA: 0s - loss: 0.8818 - acc: 0.584 - ETA: 0s - loss: 0.8779 - acc: 0.584 - ETA: 0s - loss: 0.8782 - acc: 0.582 - ETA: 0s - loss: 0.8815 - acc: 0.580 - ETA: 0s - loss: 0.8760 - acc: 0.588 - ETA: 0s - loss: 0.8755 - acc: 0.589 - 0s 51us/step - loss: 0.8766 - acc: 0.5889 - val_loss: 0.9096 - val_acc: 0.5654\n",
      "Epoch 93/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8041 - acc: 0.656 - ETA: 0s - loss: 0.8682 - acc: 0.605 - ETA: 0s - loss: 0.8674 - acc: 0.605 - ETA: 0s - loss: 0.8710 - acc: 0.597 - ETA: 0s - loss: 0.8712 - acc: 0.594 - ETA: 0s - loss: 0.8743 - acc: 0.592 - ETA: 0s - loss: 0.8737 - acc: 0.592 - ETA: 0s - loss: 0.8714 - acc: 0.594 - 0s 52us/step - loss: 0.8744 - acc: 0.5937 - val_loss: 0.9100 - val_acc: 0.5634\n",
      "Epoch 94/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7364 - acc: 0.781 - ETA: 0s - loss: 0.8813 - acc: 0.576 - ETA: 0s - loss: 0.8680 - acc: 0.592 - ETA: 0s - loss: 0.8759 - acc: 0.594 - ETA: 0s - loss: 0.8777 - acc: 0.591 - ETA: 0s - loss: 0.8761 - acc: 0.591 - ETA: 0s - loss: 0.8749 - acc: 0.591 - 0s 48us/step - loss: 0.8754 - acc: 0.5897 - val_loss: 0.9151 - val_acc: 0.5634\n",
      "Epoch 95/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9392 - acc: 0.625 - ETA: 0s - loss: 0.8833 - acc: 0.596 - ETA: 0s - loss: 0.8629 - acc: 0.592 - ETA: 0s - loss: 0.8696 - acc: 0.592 - ETA: 0s - loss: 0.8726 - acc: 0.587 - ETA: 0s - loss: 0.8792 - acc: 0.587 - ETA: 0s - loss: 0.8762 - acc: 0.589 - ETA: 0s - loss: 0.8757 - acc: 0.591 - ETA: 0s - loss: 0.8774 - acc: 0.590 - 0s 61us/step - loss: 0.8757 - acc: 0.5909 - val_loss: 0.9132 - val_acc: 0.5644\n",
      "Epoch 96/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8129 - acc: 0.562 - ETA: 0s - loss: 0.8734 - acc: 0.601 - ETA: 0s - loss: 0.8625 - acc: 0.601 - ETA: 0s - loss: 0.8625 - acc: 0.595 - ETA: 0s - loss: 0.8727 - acc: 0.590 - ETA: 0s - loss: 0.8767 - acc: 0.589 - ETA: 0s - loss: 0.8808 - acc: 0.584 - ETA: 0s - loss: 0.8758 - acc: 0.586 - ETA: 0s - loss: 0.8774 - acc: 0.588 - ETA: 0s - loss: 0.8757 - acc: 0.590 - 1s 64us/step - loss: 0.8751 - acc: 0.5909 - val_loss: 0.9143 - val_acc: 0.5619\n",
      "Epoch 97/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8348 - acc: 0.656 - ETA: 0s - loss: 0.8571 - acc: 0.578 - ETA: 0s - loss: 0.8807 - acc: 0.565 - ETA: 0s - loss: 0.8786 - acc: 0.577 - ETA: 0s - loss: 0.8795 - acc: 0.580 - ETA: 0s - loss: 0.8800 - acc: 0.582 - ETA: 0s - loss: 0.8757 - acc: 0.587 - ETA: 0s - loss: 0.8762 - acc: 0.588 - 0s 50us/step - loss: 0.8744 - acc: 0.5895 - val_loss: 0.9408 - val_acc: 0.5544\n",
      "Epoch 98/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8709 - acc: 0.562 - ETA: 0s - loss: 0.8697 - acc: 0.574 - ETA: 0s - loss: 0.8676 - acc: 0.582 - ETA: 0s - loss: 0.8730 - acc: 0.584 - ETA: 0s - loss: 0.8715 - acc: 0.586 - ETA: 0s - loss: 0.8764 - acc: 0.586 - ETA: 0s - loss: 0.8758 - acc: 0.587 - ETA: 0s - loss: 0.8740 - acc: 0.588 - 0s 51us/step - loss: 0.8749 - acc: 0.5874 - val_loss: 0.9077 - val_acc: 0.5614\n",
      "Epoch 99/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8961 - acc: 0.625 - ETA: 0s - loss: 0.8831 - acc: 0.578 - ETA: 0s - loss: 0.8688 - acc: 0.587 - ETA: 0s - loss: 0.8636 - acc: 0.593 - ETA: 0s - loss: 0.8703 - acc: 0.597 - ETA: 0s - loss: 0.8697 - acc: 0.598 - ETA: 0s - loss: 0.8717 - acc: 0.598 - ETA: 0s - loss: 0.8749 - acc: 0.593 - 0s 51us/step - loss: 0.8753 - acc: 0.5935 - val_loss: 0.9121 - val_acc: 0.5649\n",
      "Epoch 100/100\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.1730 - acc: 0.375 - ETA: 0s - loss: 0.8763 - acc: 0.572 - ETA: 0s - loss: 0.8742 - acc: 0.580 - ETA: 0s - loss: 0.8730 - acc: 0.588 - ETA: 0s - loss: 0.8821 - acc: 0.582 - ETA: 0s - loss: 0.8770 - acc: 0.585 - ETA: 0s - loss: 0.8744 - acc: 0.587 - ETA: 0s - loss: 0.8752 - acc: 0.586 - 0s 50us/step - loss: 0.8749 - acc: 0.5865 - val_loss: 0.9233 - val_acc: 0.5589\n",
      "Train on 8007 samples, validate on 2002 samples\n",
      "Epoch 1/20\n",
      "8007/8007 [==============================] - ETA: 1:42 - loss: 1.0988 - acc: 0.156 - ETA: 3s - loss: 1.0913 - acc: 0.4627  - ETA: 1s - loss: 1.0804 - acc: 0.396 - ETA: 1s - loss: 1.0571 - acc: 0.414 - ETA: 0s - loss: 1.0237 - acc: 0.451 - ETA: 0s - loss: 0.9912 - acc: 0.476 - ETA: 0s - loss: 0.9648 - acc: 0.500 - ETA: 0s - loss: 0.9450 - acc: 0.516 - ETA: 0s - loss: 0.9294 - acc: 0.529 - 1s 113us/step - loss: 0.9226 - acc: 0.5345 - val_loss: 0.8093 - val_acc: 0.6174\n",
      "Epoch 2/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7764 - acc: 0.656 - ETA: 0s - loss: 0.8024 - acc: 0.616 - ETA: 0s - loss: 0.7884 - acc: 0.625 - ETA: 0s - loss: 0.7945 - acc: 0.623 - ETA: 0s - loss: 0.7968 - acc: 0.623 - ETA: 0s - loss: 0.7965 - acc: 0.625 - ETA: 0s - loss: 0.7973 - acc: 0.622 - ETA: 0s - loss: 0.7969 - acc: 0.622 - ETA: 0s - loss: 0.7947 - acc: 0.622 - 0s 58us/step - loss: 0.7947 - acc: 0.6232 - val_loss: 0.7966 - val_acc: 0.6224\n",
      "Epoch 3/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.6464 - acc: 0.750 - ETA: 0s - loss: 0.7786 - acc: 0.656 - ETA: 0s - loss: 0.7795 - acc: 0.648 - ETA: 0s - loss: 0.7727 - acc: 0.643 - ETA: 0s - loss: 0.7695 - acc: 0.645 - ETA: 0s - loss: 0.7765 - acc: 0.638 - ETA: 0s - loss: 0.7754 - acc: 0.641 - ETA: 0s - loss: 0.7773 - acc: 0.641 - ETA: 0s - loss: 0.7750 - acc: 0.641 - 0s 59us/step - loss: 0.7782 - acc: 0.6416 - val_loss: 0.8039 - val_acc: 0.6179\n",
      "Epoch 4/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 1.0476 - acc: 0.531 - ETA: 0s - loss: 0.7884 - acc: 0.627 - ETA: 0s - loss: 0.7782 - acc: 0.631 - ETA: 0s - loss: 0.7785 - acc: 0.628 - ETA: 0s - loss: 0.7773 - acc: 0.629 - ETA: 0s - loss: 0.7766 - acc: 0.633 - ETA: 0s - loss: 0.7781 - acc: 0.634 - ETA: 0s - loss: 0.7780 - acc: 0.634 - ETA: 0s - loss: 0.7748 - acc: 0.636 - 0s 62us/step - loss: 0.7796 - acc: 0.6356 - val_loss: 0.7949 - val_acc: 0.6264\n",
      "Epoch 5/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7538 - acc: 0.656 - ETA: 0s - loss: 0.7716 - acc: 0.633 - ETA: 0s - loss: 0.7756 - acc: 0.630 - ETA: 0s - loss: 0.7679 - acc: 0.641 - ETA: 0s - loss: 0.7694 - acc: 0.640 - ETA: 0s - loss: 0.7658 - acc: 0.639 - ETA: 0s - loss: 0.7718 - acc: 0.636 - ETA: 0s - loss: 0.7721 - acc: 0.639 - ETA: 0s - loss: 0.7701 - acc: 0.641 - ETA: 0s - loss: 0.7693 - acc: 0.642 - 1s 63us/step - loss: 0.7702 - acc: 0.6411 - val_loss: 0.7955 - val_acc: 0.6189\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8007/8007 [==============================] - ETA: 0s - loss: 0.6874 - acc: 0.625 - ETA: 0s - loss: 0.7498 - acc: 0.657 - ETA: 0s - loss: 0.7728 - acc: 0.631 - ETA: 0s - loss: 0.7662 - acc: 0.637 - ETA: 0s - loss: 0.7613 - acc: 0.642 - ETA: 0s - loss: 0.7661 - acc: 0.639 - ETA: 0s - loss: 0.7637 - acc: 0.640 - ETA: 0s - loss: 0.7649 - acc: 0.640 - ETA: 0s - loss: 0.7697 - acc: 0.640 - ETA: 0s - loss: 0.7693 - acc: 0.639 - 1s 63us/step - loss: 0.7685 - acc: 0.6392 - val_loss: 0.7931 - val_acc: 0.6164\n",
      "Epoch 7/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7059 - acc: 0.656 - ETA: 0s - loss: 0.7566 - acc: 0.644 - ETA: 0s - loss: 0.7568 - acc: 0.657 - ETA: 0s - loss: 0.7713 - acc: 0.641 - ETA: 0s - loss: 0.7633 - acc: 0.647 - ETA: 0s - loss: 0.7631 - acc: 0.643 - ETA: 0s - loss: 0.7623 - acc: 0.643 - ETA: 0s - loss: 0.7678 - acc: 0.642 - ETA: 0s - loss: 0.7666 - acc: 0.643 - 0s 57us/step - loss: 0.7663 - acc: 0.6433 - val_loss: 0.7915 - val_acc: 0.6294\n",
      "Epoch 8/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7657 - acc: 0.562 - ETA: 0s - loss: 0.7646 - acc: 0.658 - ETA: 0s - loss: 0.7553 - acc: 0.646 - ETA: 0s - loss: 0.7588 - acc: 0.651 - ETA: 0s - loss: 0.7672 - acc: 0.641 - ETA: 0s - loss: 0.7705 - acc: 0.640 - ETA: 0s - loss: 0.7710 - acc: 0.638 - ETA: 0s - loss: 0.7687 - acc: 0.637 - ETA: 0s - loss: 0.7625 - acc: 0.642 - 0s 57us/step - loss: 0.7642 - acc: 0.6426 - val_loss: 0.7873 - val_acc: 0.6344\n",
      "Epoch 9/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8546 - acc: 0.500 - ETA: 0s - loss: 0.7645 - acc: 0.632 - ETA: 0s - loss: 0.7629 - acc: 0.644 - ETA: 0s - loss: 0.7510 - acc: 0.649 - ETA: 0s - loss: 0.7546 - acc: 0.645 - ETA: 0s - loss: 0.7514 - acc: 0.646 - ETA: 0s - loss: 0.7537 - acc: 0.643 - ETA: 0s - loss: 0.7585 - acc: 0.643 - 0s 52us/step - loss: 0.7609 - acc: 0.6409 - val_loss: 0.7833 - val_acc: 0.6264\n",
      "Epoch 10/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.6522 - acc: 0.718 - ETA: 0s - loss: 0.7713 - acc: 0.631 - ETA: 0s - loss: 0.7628 - acc: 0.636 - ETA: 0s - loss: 0.7610 - acc: 0.635 - ETA: 0s - loss: 0.7593 - acc: 0.635 - ETA: 0s - loss: 0.7593 - acc: 0.638 - ETA: 0s - loss: 0.7571 - acc: 0.639 - ETA: 0s - loss: 0.7572 - acc: 0.640 - ETA: 0s - loss: 0.7598 - acc: 0.640 - 0s 57us/step - loss: 0.7593 - acc: 0.6411 - val_loss: 0.8122 - val_acc: 0.6104\n",
      "Epoch 11/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7592 - acc: 0.656 - ETA: 0s - loss: 0.7588 - acc: 0.655 - ETA: 0s - loss: 0.7568 - acc: 0.655 - ETA: 0s - loss: 0.7641 - acc: 0.646 - ETA: 0s - loss: 0.7609 - acc: 0.646 - ETA: 0s - loss: 0.7594 - acc: 0.645 - ETA: 0s - loss: 0.7601 - acc: 0.645 - ETA: 0s - loss: 0.7591 - acc: 0.646 - ETA: 0s - loss: 0.7606 - acc: 0.643 - 0s 61us/step - loss: 0.7615 - acc: 0.6453 - val_loss: 0.7798 - val_acc: 0.6294\n",
      "Epoch 12/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7530 - acc: 0.625 - ETA: 0s - loss: 0.7605 - acc: 0.643 - ETA: 0s - loss: 0.7634 - acc: 0.645 - ETA: 0s - loss: 0.7719 - acc: 0.643 - ETA: 0s - loss: 0.7712 - acc: 0.645 - ETA: 0s - loss: 0.7693 - acc: 0.645 - ETA: 0s - loss: 0.7592 - acc: 0.654 - ETA: 0s - loss: 0.7557 - acc: 0.655 - ETA: 0s - loss: 0.7591 - acc: 0.651 - ETA: 0s - loss: 0.7563 - acc: 0.650 - 1s 64us/step - loss: 0.7553 - acc: 0.6493 - val_loss: 0.7884 - val_acc: 0.6344\n",
      "Epoch 13/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.6215 - acc: 0.750 - ETA: 0s - loss: 0.7444 - acc: 0.658 - ETA: 0s - loss: 0.7553 - acc: 0.651 - ETA: 0s - loss: 0.7529 - acc: 0.647 - ETA: 0s - loss: 0.7533 - acc: 0.649 - ETA: 0s - loss: 0.7474 - acc: 0.649 - ETA: 0s - loss: 0.7512 - acc: 0.649 - ETA: 0s - loss: 0.7536 - acc: 0.648 - ETA: 0s - loss: 0.7577 - acc: 0.645 - 0s 57us/step - loss: 0.7574 - acc: 0.6459 - val_loss: 0.8050 - val_acc: 0.6184\n",
      "Epoch 14/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8912 - acc: 0.625 - ETA: 0s - loss: 0.7601 - acc: 0.655 - ETA: 0s - loss: 0.7655 - acc: 0.642 - ETA: 0s - loss: 0.7489 - acc: 0.651 - ETA: 0s - loss: 0.7553 - acc: 0.646 - ETA: 0s - loss: 0.7578 - acc: 0.649 - ETA: 0s - loss: 0.7592 - acc: 0.648 - ETA: 0s - loss: 0.7542 - acc: 0.651 - ETA: 0s - loss: 0.7525 - acc: 0.651 - 0s 59us/step - loss: 0.7530 - acc: 0.6506 - val_loss: 0.8156 - val_acc: 0.6099\n",
      "Epoch 15/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.9987 - acc: 0.593 - ETA: 0s - loss: 0.7708 - acc: 0.632 - ETA: 0s - loss: 0.7755 - acc: 0.634 - ETA: 0s - loss: 0.7582 - acc: 0.647 - ETA: 0s - loss: 0.7544 - acc: 0.644 - ETA: 0s - loss: 0.7598 - acc: 0.642 - ETA: 0s - loss: 0.7583 - acc: 0.644 - ETA: 0s - loss: 0.7573 - acc: 0.645 - ETA: 0s - loss: 0.7565 - acc: 0.645 - 0s 58us/step - loss: 0.7545 - acc: 0.6467 - val_loss: 0.7929 - val_acc: 0.6324\n",
      "Epoch 16/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.8158 - acc: 0.625 - ETA: 0s - loss: 0.7805 - acc: 0.632 - ETA: 0s - loss: 0.7631 - acc: 0.643 - ETA: 0s - loss: 0.7581 - acc: 0.644 - ETA: 0s - loss: 0.7496 - acc: 0.650 - ETA: 0s - loss: 0.7498 - acc: 0.648 - ETA: 0s - loss: 0.7484 - acc: 0.647 - ETA: 0s - loss: 0.7518 - acc: 0.645 - ETA: 0s - loss: 0.7536 - acc: 0.644 - 0s 59us/step - loss: 0.7533 - acc: 0.6442 - val_loss: 0.7815 - val_acc: 0.6354\n",
      "Epoch 17/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7444 - acc: 0.625 - ETA: 0s - loss: 0.7672 - acc: 0.637 - ETA: 0s - loss: 0.7571 - acc: 0.639 - ETA: 0s - loss: 0.7531 - acc: 0.648 - ETA: 0s - loss: 0.7489 - acc: 0.651 - ETA: 0s - loss: 0.7511 - acc: 0.648 - ETA: 0s - loss: 0.7509 - acc: 0.646 - ETA: 0s - loss: 0.7505 - acc: 0.649 - 0s 54us/step - loss: 0.7478 - acc: 0.6508 - val_loss: 0.7886 - val_acc: 0.6264\n",
      "Epoch 18/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7433 - acc: 0.625 - ETA: 0s - loss: 0.7415 - acc: 0.644 - ETA: 0s - loss: 0.7183 - acc: 0.660 - ETA: 0s - loss: 0.7331 - acc: 0.655 - ETA: 0s - loss: 0.7364 - acc: 0.652 - ETA: 0s - loss: 0.7379 - acc: 0.654 - ETA: 0s - loss: 0.7406 - acc: 0.653 - ETA: 0s - loss: 0.7415 - acc: 0.648 - ETA: 0s - loss: 0.7470 - acc: 0.647 - 0s 60us/step - loss: 0.7492 - acc: 0.6463 - val_loss: 0.7759 - val_acc: 0.6349\n",
      "Epoch 19/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7278 - acc: 0.656 - ETA: 0s - loss: 0.7549 - acc: 0.646 - ETA: 0s - loss: 0.7449 - acc: 0.652 - ETA: 0s - loss: 0.7430 - acc: 0.650 - ETA: 0s - loss: 0.7565 - acc: 0.640 - ETA: 0s - loss: 0.7591 - acc: 0.640 - ETA: 0s - loss: 0.7508 - acc: 0.646 - ETA: 0s - loss: 0.7526 - acc: 0.644 - ETA: 0s - loss: 0.7537 - acc: 0.645 - 0s 59us/step - loss: 0.7512 - acc: 0.6473 - val_loss: 0.7750 - val_acc: 0.6324\n",
      "Epoch 20/20\n",
      "8007/8007 [==============================] - ETA: 0s - loss: 0.7378 - acc: 0.625 - ETA: 0s - loss: 0.7509 - acc: 0.658 - ETA: 0s - loss: 0.7618 - acc: 0.645 - ETA: 0s - loss: 0.7656 - acc: 0.644 - ETA: 0s - loss: 0.7571 - acc: 0.647 - ETA: 0s - loss: 0.7511 - acc: 0.650 - ETA: 0s - loss: 0.7487 - acc: 0.652 - ETA: 0s - loss: 0.7462 - acc: 0.652 - 0s 54us/step - loss: 0.7457 - acc: 0.6531 - val_loss: 0.7956 - val_acc: 0.6299\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    model = Sequential([\n",
    "        Dense(3, input_shape=(num_features,), kernel_initializer='normal'),\n",
    "       Activation('relu'),\n",
    "        Dense(32, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(16,kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(8, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(3,kernel_initializer='normal'),\n",
    "         Activation('softmax'),\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    model.fit(X_train, y_train[:,0], validation_data=(X_test, y_test[:,0]), epochs = 100)\n",
    "    #model.fit(X_train[0:2], y_train[0:2,0], validation_data=(X_test[0:2], y_test[0:2,0]), epochs = 1000)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    model1 = Sequential([\n",
    "        Dense(64, input_shape=(num_features,), kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(32, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(16,kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(8, kernel_initializer='normal'),\n",
    "        Activation('relu'),\n",
    "        Dense(3),\n",
    "        Activation('softmax'),\n",
    "\n",
    "    ])\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    \n",
    "    model1.fit(X_train, y_train[:,1], validation_data=(X_test, y_test[:,1]), epochs = 20)\n",
    "\n",
    "\n",
    "#     model2 = Sequential([\n",
    "#         Dense(64, input_shape=(20,), kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(32, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(16,kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(8, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(1),\n",
    "#     ])\n",
    "#     model2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#     model2.fit(X_train, y_train[:,2], validation_data=(X_test, y_test[:,2]), epochs = 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[944  42  27]\n",
      " [470  59  35]\n",
      " [249  60 116]] \n",
      "\n",
      "[[413 202   3]\n",
      " [264 755  30]\n",
      " [ 12 230  93]]\n",
      "[0.40053058 0.5704209  0.02904855] ,  0\n",
      "[0.0140562  0.4327929  0.55315095] ,  2\n",
      "[0.29926488 0.6475718  0.05316335] ,  1\n",
      "[0.26691413 0.6610558  0.07202996] ,  1\n",
      "[8.4274352e-01 1.5647501e-01 7.8145257e-04] ,  0\n",
      "[0.51666224 0.46577373 0.01756407] ,  1\n",
      "[0.3015513  0.6419258  0.05652288] ,  0\n",
      "[0.6074382  0.38008502 0.0124768 ] ,  0\n",
      "[8.3340734e-01 1.6577919e-01 8.1345841e-04] ,  1\n",
      "[0.22864468 0.679418   0.09193728] ,  1\n",
      "[0.25440872 0.6673801  0.0782112 ] ,  2\n",
      "[0.02013795 0.47608697 0.5037751 ] ,  1\n",
      "[0.23987187 0.6749087  0.08521948] ,  1\n",
      "[0.30883783 0.62986505 0.06129718] ,  1\n",
      "[0.00306397 0.25540805 0.741528  ] ,  2\n",
      "[0.5422972  0.44796008 0.00974272] ,  1\n",
      "[0.29282123 0.6372891  0.0698896 ] ,  0\n",
      "[0.31405944 0.6494018  0.03653873] ,  1\n",
      "[0.620581   0.36807388 0.01134521] ,  1\n",
      "[0.7837961  0.21421109 0.0019929 ] ,  0\n",
      "Log_loss: 0.7955506107547543\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-388a72a2b2af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Log_loss:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0my_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "pre = model.predict_classes(X_test)\n",
    "y_int = np.argmax(y_test[:,0], axis=1)\n",
    "print(confusion_matrix(y_int, pre), \"\\n\")\n",
    "\n",
    "\n",
    "pre = model1.predict_classes(X_test)\n",
    "#pre = model1.predict(X_test)\n",
    "#pre = np.argmax(pre, axis =1)\n",
    "y_int = np.argmax(y_test[:,1], axis=1)\n",
    "print(confusion_matrix(y_int, pre))\n",
    "prob =  model1.predict(X_test)\n",
    "for i in range(0,20):\n",
    "    print(prob[i],\", \" ,y_int[i])\n",
    "print(\"Log_loss:\",log_loss(y_int, prob))\n",
    "\n",
    "pre = model2.predict_classes(X_test)\n",
    "y_int = np.argmax(y_test[:,2], axis=1)\n",
    "print(\"\\n\",confusion_matrix(y_int, pre))\n",
    "\n",
    "\n",
    "# [[1001    0   62]\n",
    "#  [ 462    0   87]\n",
    "#  [ 261    0  129]] \n",
    "\n",
    "# [[383 240   4]\n",
    "#  [197 813  40]\n",
    "#  [  8 200 117]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinal logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mord\n",
    "from sklearn import linear_model, metrics, preprocessing\n",
    "\n",
    "clf2 = mord.LogisticAT(alpha=1)\n",
    "#clf2 = mord.LogisticIT(alpha=1)\n",
    "#clf2 = mord.LAD()\n",
    "#clf2 = mord.OrdinalRidge()\n",
    "#clf2 = mord.MulticlassLogistic()\n",
    "\n",
    "y_train_AT = y_train \n",
    "y_train_AT = y_train_AT.astype(int)\n",
    "y_test_AT = y_test.astype(int)\n",
    "\n",
    "clf2.fit(X_train, y_train_AT[:,1] )\n",
    "\n",
    "print(metrics.mean_absolute_error(model1.predict(X_test), y_test_AT[:,1]))\n",
    "print('Mean Absolute Error of LogisticAT %s' % metrics.mean_absolute_error(clf2.predict(X_test), y_test_AT[:,1]))\n",
    "\n",
    "print(clf2.predict(X_test)[0:3])\n",
    "#print(clf2.predict_proba(X_test)[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficient of Determination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#predict = model.predict(X_test)\n",
    "predict1 = model1.predict(X_test)\n",
    "#predict2 = model2.predict(X_test)\n",
    "\n",
    "# a = r2_score(y_test[:,0], predict)\n",
    "# print(\"The R2 for Valance: \" ,a)\n",
    "\n",
    "a1 = r2_score(y_test[:,1], predict1)\n",
    "print(\"The R2 for Activation: \" ,a1)\n",
    "\n",
    "# a2 = r2_score(y_test[:,2], predict2)\n",
    "# print(\"The R2 for Dominance: \" ,a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of err**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = np.linspace(0, 1, 10)\n",
    "data = np.abs(predict1 - y_test[:,1])\n",
    "print(len(data))\n",
    "hist, bin_edges = np.histogram(data,bins) # make the histogram\n",
    "print(hist)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Plot the histogram heights against integers on the x axis\n",
    "ax.bar(range(len(hist)),hist,width=1) \n",
    "\n",
    "# # Set the ticks to the middle of the bars\n",
    "# ax.set_xticks([0.5+i for i,j in enumerate(hist)])\n",
    "\n",
    "# # # Set the xticklabels to a string that tells us what the bin edges were\n",
    "# # ax.set_xticklabels(['{} - {}'.format(bins[i],bins[i+1]) for i,j in enumerate(hist)])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#predict = model.predict(X_test)\n",
    "\n",
    "#plt.hist(subs, bins =bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard residual plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.pyplot import subplots_adjust\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "subplots_adjust(right = 2, wspace=0.2)\n",
    "predict = model.predict(X_test)\n",
    "predict = np.array([p[0] for p in predict])\n",
    "axs[0] = sns.residplot(y_test[:,0], predict, lowess=True, color=\"g\", ax=axs[0])\n",
    "axs[0].set_ylim([-0.4,0.4])\n",
    "axs[0].set_title(\"Residual for Valance\") \n",
    "\n",
    "\n",
    "predict1 = model1.predict(X_test)\n",
    "predict1 = np.array([p[0] for p in predict1])\n",
    "axs[1] = sns.residplot(y_test[:,1], predict1, lowess=True, color=\"r\", ax=axs[1])\n",
    "axs[1].set_ylim([-0.4,0.4])\n",
    "axs[1].set_title(\"Residual for Activation\")\n",
    "\n",
    "predict2 = model2.predict(X_test)\n",
    "predict2 = np.array([p[0] for p in predict2])\n",
    "axs[2] = sns.residplot(y_test[:,2], predict2, lowess=True, color=\"b\", ax=axs[2])\n",
    "axs[2].set_ylim([-0.5,0.5])\n",
    "axs[2].set_title(\"Residual for Dominance\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual plot show that the error followed random pattern. If the residual plot is not enough random, then the model would be likely to lose something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing around with prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict = model.predict(X_test)\n",
    "#print(predict)\n",
    "\n",
    "subs = np.abs(predict - y_test[:,0])\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "subplots_adjust(right = 2, wspace=0.2)\n",
    "axs[0].hist(y_train[:,0])\n",
    "axs[0].set_title(\"True value in train data\\n\" + \n",
    "                 \"avg:\" + str(np.average(y_train[:,0])) + \n",
    "                 \"\\nstd:\" + str(np.std(y_train[:,0])))\n",
    "axs[1].hist(y_test[:,0])\n",
    "axs[1].set_title(\"True value in test data\\n\" + \n",
    "                 \"avg:\" + str(np.average(y_test[:,0])) + \n",
    "                 \"\\nstd:\" + str(np.std(y_test[:,0])))\n",
    "axs[1].set_ylim([0,400])\n",
    "                        \n",
    "axs[2].hist(predict) \n",
    "axs[2].set_title(\"Prediction\\n\" +\n",
    "                \"avg:\" + str(np.average(predict)) + \n",
    "                 \"\\nstd:\" + str(np.std(predict)))\n",
    "                \n",
    "# axs[2].hist(subs, bins='auto')\n",
    "# axs[2].set_title(\"error on predictions\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.99):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "predict = model1.predict(X_test)\n",
    "bins = np.linspace(0, 1, 6)\n",
    "bin_ranges = [bins[i-1: i+1] for i in range (1,len(bins))]\n",
    "bin_ranges =  np.array(bin_ranges)\n",
    "\n",
    "# y = y_test[:,1]\n",
    "# subs = np.abs(y-predict)\n",
    "# print(\"ave, std, per90:\", np.average(subs), \", \", np.std(subs), \",\", np.percentile(subs, 60))\n",
    "    \n",
    "for r in bin_ranges:\n",
    "    # Get valance\n",
    "    y = y_test[:,1]\n",
    "    condition1 = [r[0] <= e <= r[1]for e in y]\n",
    "    \n",
    "    predict = predict.flatten()\n",
    "    condition2 = [r[0] <= e <= r[1]for e in predict]\n",
    "    \n",
    "    y = y[condition1 or condition2]\n",
    "    pre = predict[condition1 or condition2]\n",
    "    #pre= pre.flatten()\n",
    "    #print(pre)\n",
    "    subs = np.abs(y-pre)\n",
    "   # print(subs)\n",
    "    try:\n",
    "        #print(subs)\n",
    "        print(\"\\nIn range \" , r, \n",
    "              \"\\nNum samples: \", len(subs),\n",
    "              \"\\nAverage err: \", np.average(subs), \n",
    "              \"\\nStd err: \", np.std(subs),\n",
    "              \"\\nMedian err: \", np.median(subs), \n",
    "             \"\\nPercentile: \", np.percentile(subs, 90),\n",
    "              \"\\nConfident Interval: \", mean_confidence_interval(subs)\n",
    "             )\n",
    "    except:\n",
    "        print(\"null array\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.99):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "predict = clf2.predict(X_test)\n",
    "bins = np.linspace(0, 6, 7)\n",
    "bin_ranges = [bins[i-1: i+1] for i in range (1,len(bins))]\n",
    "bin_ranges =  np.array(bin_ranges)\n",
    "\n",
    "\n",
    "for r in bin_ranges:\n",
    "    # Get valance\n",
    "    y = y_test[:,1]\n",
    "    condition1 = [r[0] <= e <= r[1]for e in y]\n",
    "    \n",
    "    predict = predict.flatten()\n",
    "    condition2 = [r[0] <= e <= r[1]for e in predict]\n",
    "    \n",
    "    y = y[condition1 or condition2]\n",
    "    pre = predict[condition1 or condition2]\n",
    "    #pre= pre.flatten()\n",
    "    print(pre)\n",
    "    subs = np.abs(y-pre)\n",
    "   # print(subs)\n",
    "    try:\n",
    "        print(subs)\n",
    "        print(\"\\nIn range \" , r, \n",
    "              \"\\nNum samples: \", len(subs),\n",
    "              \"\\nAverage err: \", np.average(subs), \n",
    "              \"\\nStd err: \", np.std(subs),\n",
    "              \"\\nMedian err: \", np.median(subs), \n",
    "             \"\\nPercentile: \", np.percentile(subs, 95),\n",
    "              \"\\nConfident Interval: \", mean_confidence_interval(subs)\n",
    "             )\n",
    "    except:\n",
    "        print(\"null array\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "predict = model.predict(X_test)\n",
    "predict = np.array([p[0] for p in predict])\n",
    "\n",
    "predict1 = model1.predict(X_test)\n",
    "predict1 = np.array([p[0] for p in predict1])\n",
    "\n",
    "predict2 = model2.predict(X_test)\n",
    "predict2 = np.array([p[0] for p in predict2])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test[:,1], predict1, edgecolors=(0, 0, 0))\n",
    "#ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "\n",
    "# ax[1].scatter(y_test[:,1], predict, edgecolors=(0, 0, 0))\n",
    "# ax[1].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "# ax[1].set_xlabel('Actual')\n",
    "# ax[1].set_ylabel('Predicted')\n",
    "\n",
    "# ax[2].scatter(y_test[:,2], predict, edgecolors=(0, 0, 0))\n",
    "# ax[2].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "# ax[2].set_xlabel('Actual')\n",
    "# ax[2].set_ylabel('Predicted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing around **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "predict = model2.predict(X_test)\n",
    "\n",
    "def reducer(acc, val):\n",
    "    if (val > 0.5):\n",
    "        acc = acc + 1\n",
    "    return acc\n",
    "\n",
    "num = reduce(lambda acc,val: acc+1 if val >1 else acc, predict.flatten(), 0) \n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence in interval for Arouse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf2.predict(X_test)\n",
    "print(predict)\n",
    "#predict = clf2.predict(X_test)\n",
    "#print(predict)\n",
    "#predict = np.array([p[0] for p in predict])\n",
    "y = y_test[:, 1]\n",
    "y1=y\n",
    "#y1 = np.argmax(y,1)\n",
    "cm = confusion_matrix(y1, predict)\n",
    "print(\"Confusion matrix: \\n\", cm)\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 5, 6)\n",
    "bin_ranges = [bins[i-1: i+1] for i in range (1,len(bins))]\n",
    "bin_ranges =  np.array(bin_ranges)\n",
    "\n",
    "for r in bin_ranges:\n",
    "     condition = [r[0] <= e <= r[1] for e in y]\n",
    "     y1 = y[condition]\n",
    "     pre = predict.flatten()\n",
    "     pre = pre[condition]\n",
    "     num_true = 0\n",
    "     for i in range (0, len(y1)):\n",
    "        if (np.abs(y1[i] - pre[i]) ==0 ):\n",
    "            num_true += 1\n",
    "     print(\"Num samples:\", len(pre))\n",
    "     print(\"Accurcy in range: \", r, \": \", num_true / (len(pre) +1 ) )\n",
    "            \n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "y_int = y.astype(int)\n",
    "\n",
    "ax[0].hist(y)\n",
    "ax[0].set_ylim([0,650])\n",
    "ax[1].hist(y_int)\n",
    "ax[1].set_ylim([0,650])\n",
    "ax[2].hist(predict)\n",
    "ax[2].set_ylim([0,650])\n",
    "        \n",
    "print(set(predict))\n",
    "print(len(predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
