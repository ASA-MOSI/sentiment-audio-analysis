{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This file will analyze the emotion analysis on IEMOCAP database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\THEDE\\Miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots_adjust\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "EMOTION = {'ang': 0, 'hap' : 1, 'sad' : 2, 'neu' : 3, 'fru' : 4, 'exc': 5,\n",
    "           'fea' : 6,'sur' : 7,'dis' : 8, 'oth' : 9, 'xxx':10}\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def parse_metric(confusion_matrix):\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    return {'FP': FP, 'FN': FN, 'TP':TP, 'TN':TN, 'TPR': TPR, 'TNR': TNR, 'PPV' : PPV, 'NPV' :NPV, 'FPR': FPR, 'FNR': FNR,\n",
    "           'FDR': FDR, 'ACC':ACC}\n",
    "\n",
    "class Input:\n",
    "    \n",
    "    def __init__(self, code=None, spectrogram=None, acoustic_features=None, transcript=None):\n",
    "        self.__code = code\n",
    "        self.__spectrogram = spectrogram\n",
    "        self.__acoustic_features = acoustic_features\n",
    "        self.__transcript = transcript\n",
    "    \n",
    "    def get_code(self):\n",
    "        return self.__code\n",
    "            \n",
    "    def get_spectrogram(self):\n",
    "        return self.__spectrogram\n",
    "    \n",
    "    def get_acoustic_features(self):\n",
    "        return self.__acoustic_features\n",
    "    \n",
    "    def get_transcript(self):\n",
    "        return self.__transcript\n",
    "    \n",
    "    def set_transcript(self, transcript):\n",
    "        self.__transcript = transcript\n",
    "    \n",
    "    \n",
    "class Output:\n",
    "    def __init__(self, code, duration, annotated_categories, chosen_category, attributes):\n",
    "        self.__code = code # file name\n",
    "        self.__duration = duration #  duration [t1-t2]\n",
    "        self.__annotated_categories= annotated_categories # array of different category evaluation of annotator\n",
    "        self.__chosen_category = chosen_category  # final category which most of annotators choose\n",
    "        self.__attributes = attributes # valance, arouse, dominance of emotion\n",
    "        \n",
    "    def get_code(self):\n",
    "        return self.__code\n",
    "\n",
    "    def get_duration(self):\n",
    "        return self.__duration\n",
    "    \n",
    "    def get_annotated_categories(self):\n",
    "        return self.__annotated_categories\n",
    "    \n",
    "    def get_chosen_category(self):\n",
    "        return self.__chosen_category\n",
    "    \n",
    "    def get_attributes(self):\n",
    "        return self.__attributes\n",
    "                \n",
    "    def get_consitency(self):\n",
    "        \"\"\"Get consitency of evaluation.\n",
    "        \"\"\"\n",
    "        counts = np.unique(self.get_annotated_categories(), return_counts=True)[1]\n",
    "        consitent_degree = np.max(counts)/np.sum(counts)\n",
    "        return consitent_degree\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (IEMOCAP-DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handler_input = open('processed-data/input.obj', 'rb')\n",
    "file_handler_output = open('processed-data/output.obj', 'rb')\n",
    "input = np.array(pickle.load(file_handler_input))\n",
    "output = np.array(pickle.load(file_handler_output))\n",
    "\n",
    "print(\"Size input, output: {}, {}\".format(len(input), len(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An input is an object including attributes such as **spectrogram, acoustic features, transcript**. Only **acoustic features** used in this  task.\n",
    "- An output is an object including emotion labels(happy, neutral, excited...) and VAD (valance, arouse, dominance), number ranging from [1,5]. For example, VAD can be [4.5, 4, 3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyze relationship between Valance, Arouse and Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Valance and Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "category_valance = [(e.get_chosen_category(), e.get_attributes()[0]) for e in output if e.get_consitency() > 0.5]\n",
    "LABELS = ['exc', 'hap', 'neu', 'ang', 'sad', 'fru']\n",
    "\n",
    "groups = {}\n",
    "for label in LABELS:\n",
    "    groups[label] = [el[1] for el in category_valance if el[0] == label]\n",
    "\n",
    "num_col = 3\n",
    "fig, axs = plt.subplots(int(len(LABELS) / num_col), num_col,  figsize=(5,8))\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "\n",
    "for i in range (0, len(LABELS)):\n",
    "    row = int(i / num_col)\n",
    "    col = i % num_col\n",
    "    n, bins, patches = axs[row][col].hist(groups[LABELS[i]], density=1)\n",
    "    axs[row][col].set_xlim([1,5])\n",
    "    axs[row][col].set_title(\"Valance Distribution of {}\".format(LABELS[i]))\n",
    "    \n",
    "    # add a 'best fit' line\n",
    "    mu = np.mean(groups[LABELS[i]])\n",
    "    sigma = np.std(groups[LABELS[i]])\n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "    np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "    axs[row][col].plot(bins, y, '--')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The value of valance can distinguish the pleasure of the emotion. Above figures illustrate that the (excited, hapiness, neu) can be in one group; (ang, sad and fru) can be in other group "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Arouse (Activation) and  emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "category_arouse = [(e.get_chosen_category(), e.get_attributes()[1]) for e in output if e.get_consitency()>0.7]\n",
    "LABELS = ['exc', 'ang', 'fru', 'hap', 'neu', 'sad']\n",
    "\n",
    "groups = {}\n",
    "for label in LABELS:\n",
    "    groups[label] = [el[1] for el in category_arouse if el[0] == label]\n",
    "\n",
    "num_col = 3\n",
    "fig, axs = plt.subplots(int(len(LABELS) / num_col), num_col,  figsize=(5,8))\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "\n",
    "for i in range (0, len(LABELS)):\n",
    "    row = int(i / num_col)\n",
    "    col = i % num_col\n",
    "    n, bins, patches = axs[row][col].hist(groups[LABELS[i]], density = 1)\n",
    "    axs[row][col].set_xlim([1,5])\n",
    "    axs[row][col].set_title(\"Arouse Distribution of {}\".format(LABELS[i]))\n",
    "    \n",
    "      # add a 'best fit' line\n",
    "    mu = np.mean(groups[LABELS[i]])\n",
    "    sigma = np.std(groups[LABELS[i]])\n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "    np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "    axs[row][col].plot(bins, y, '--')\n",
    "    \n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The arouse can distinguish quite well the activation of emotion. Above figures indicate that the group of (excited, angry, frustrated, happy) have higher arouse value than the group of (neutral and sadness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training model and evaluating model in IEMOCAP-DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will choose the emotions that are **popular** in the interview situation **(excited, happy, sad, neutral)**.  The frustrated and anger seems do not exist in interview environment, so I remove them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [ 'exc' ,'hap', 'sad', 'neu']\n",
    "in_out = list(filter(lambda x: ~np.isnan(np.array(x[0].get_acoustic_features(),  dtype=np.float64)).any() and x[1].get_chosen_category() in LABELS , zip(input, output)))\n",
    "input, output = zip(*in_out)\n",
    "\n",
    "print(\"Size input, output after filtering: {}, {}\".format(len(input), len(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get acoustic features and normalize it.\n",
    "X = np.array([i.get_acoustic_features() for i in input])\n",
    "X_norm = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "\n",
    "\n",
    "#Save max and min values of X for Cross-database testting\n",
    "max_acoustic_features = np.max(X, axis=0)\n",
    "min_acoustic_features = np.min(X, axis=0)\n",
    "\n",
    "# Get valance and arouse\n",
    "y_valance = np.array([i.get_attributes()[0] for i in output]) \n",
    "y_arouse =  np.array([i.get_attributes()[1] for i in output]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Scatter plot arouse, valance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['exc', 'hap', 'neu', 'sad']\n",
    "COLORS = {'exc': 'r', 'hap': 'b', 'neu' : 'y', 'sad': 'black'}\n",
    "valance_groups = {}\n",
    "arouse_groups = {}\n",
    "scatter_groups = {}\n",
    "plt.figure(figsize=(7,7))\n",
    "for l in LABELS:\n",
    "    valance_groups[l] = np.array([i.get_attributes()[0] for i in output if i.get_chosen_category() == l])\n",
    "    arouse_groups[l] = np.array([i.get_attributes()[1] for i in output if i.get_chosen_category() == l])\n",
    "    scatter_groups[l] = plt.scatter(valance_groups[l], arouse_groups[l], color= COLORS[l])\n",
    "\n",
    "plt.xlabel(\"Valance\")\n",
    "plt.ylabel(\"Arouse\")\n",
    "plt.legend(tuple(scatter_groups.values()),\n",
    "           tuple(scatter_groups.keys()),\n",
    "           scatterpoints=1,\n",
    "           loc='lower right',\n",
    "           ncol=3,\n",
    "           fontsize=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spit data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, output, test_size=0.2, random_state=300)\n",
    "print(\"Size training, testing set: \", len(X_train), \", \", len(X_test))\n",
    "\n",
    "y_train_label = np.array([i.get_chosen_category() for i in y_train])\n",
    "y_test_label = np.array([i.get_chosen_category() for i in y_test])\n",
    "\n",
    "y_train_valance = np.array([i.get_attributes()[0] for i in y_train]) \n",
    "y_test_valance =  np.array([i.get_attributes()[0] for i in y_test]) \n",
    "y_train_arouse = np.array([i.get_attributes()[1] for i in y_train]) \n",
    "y_test_arouse =  np.array([i.get_attributes()[1] for i in y_test]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Evaluate Valance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will  use the SVM regression model for training.\n",
    "- Then I will  evaluate the learned regression models for a two-class classification task.\n",
    "Based on the idea on this paper https://arxiv.org/pdf/1504.03425.pdf (Section 5.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Training using SVM regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, predicts\n",
    "clf = LogisticRegression()\n",
    "clf = svm.SVR(kernel='linear')\n",
    "clf.fit(X_train, y_train_valance)\n",
    "\n",
    "# Save model into file\n",
    "file_handler = open(\"model/model_regression_valance.sav\", 'wb')\n",
    "pickle.dump(clf, file_handler)\n",
    "\n",
    "\n",
    "predicts_valance = clf.predict(X_test)\n",
    "\n",
    "# Caculates Coefficient and error metrics\n",
    "print(\"Correlation Coefficient:\", np.sqrt(r2_score(y_test_valance, predicts_valance)))\n",
    "print(\"MSE: {}, MAE: {}, MAD: {}, MAPE: {} %\".format(mean_squared_error(y_test_valance, predicts_valance),\n",
    "                                                   mean_absolute_error(y_test_valance, predicts_valance),\n",
    "                                                   median_absolute_error(y_test_valance, predicts_valance),\n",
    "                                                   mean_absolute_percentage_error(y_test_valance, predicts_valance)\n",
    "                                                  ))\n",
    "\n",
    "# Draw ditribution of absolute error\n",
    "plt.hist(np.abs(predicts_valance - y_test_valance), density = 1)\n",
    "plt.title(\"Distribution of Absolute error\")\n",
    "plt.xlabel(\"Absolute error\")\n",
    "plt.ylabel(\"Density\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** With the range of value from [1-5], I think the MSE < 0.6 could be acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['exc', 'hap', 'sad', 'neu']\n",
    "groups = {}\n",
    "for l in LABELS:\n",
    "    groups[l] =  np.array([predicts_valance[i] for i in range (0, len(predicts_valance)) if y_test_label[i] == l])\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "for i in range(0,4):\n",
    "    row = int(i/2)\n",
    "    col = int(i%2)\n",
    "    axs[row][col].hist(np.array(groups[LABELS[i]]), density=1)\n",
    "    axs[row][col].set_xlim([1,5])\n",
    "    axs[row][col].set_title(LABELS[i])\n",
    "    mu = np.mean(groups[LABELS[i]])\n",
    "    sigma = np.std(groups[LABELS[i]])\n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "    axs[row][col].plot(bins, y, '--')\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Evaluation the learned regression model for two-class classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the median value as a split point.\n",
    "split_point = np.median(y_train_valance)\n",
    "\n",
    "# Categorize the values. All the values smaller than \"split_point\" is consider as low-valance and vice-versa\n",
    "y_test_binary = np.array([int(i >= split_point) for i in y_test_valance])\n",
    "\n",
    "# According to the predicting values,\n",
    "# Using thresholds ranging from [1-5] to indicate the True Posivite Rate and False Positive Rate\n",
    "# Then draw the ROC-curve\n",
    "thresholds = np.linspace(0, 5, 50)\n",
    "tpr_arr = []\n",
    "fpr_arr = []\n",
    "thresh_arr = [] \n",
    "for t in thresholds:\n",
    "    predict_binary = np.array([int(i >= t) for i in predicts_valance])\n",
    "    cfm = confusion_matrix(y_test_binary, predict_binary)\n",
    "    parse_values = parse_metric(cfm)\n",
    "    tpr_arr.append(parse_values['TPR'])\n",
    "    fpr_arr.append(parse_values['FPR'])\n",
    "    thresh_arr.append(t)\n",
    "\n",
    "fpr_arr = np.asarray(fpr_arr)\n",
    "tpr_arr = np.asarray(tpr_arr)\n",
    "roc_auc = auc(fpr_arr[:,1], tpr_arr[:,1])\n",
    "plt.plot(fpr_arr[:,1], tpr_arr[:,1], label='ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:  I think that the ROC value (>0.7) is acceptable with psychology-related task.\n",
    "According to this discussion https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Evaluate Arouse(Activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 Training using SVM regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, predicts\n",
    "clf = LogisticRegression()\n",
    "clf = svm.SVR(kernel='linear')\n",
    "clf.fit(X_train, y_train_arouse)\n",
    "predicts_arouse = clf.predict(X_test)\n",
    "\n",
    "# Save model into file\n",
    "file_handler = open(\"model/model_regression_arouse.sav\", 'wb')\n",
    "pickle.dump(clf, file_handler)\n",
    "\n",
    "# Caculates Coefficient and error metrics\n",
    "print(\"Correlation Coefficient:\", np.sqrt(r2_score(y_test_arouse, predicts_arouse)))\n",
    "print(\"MSE: {}, MAE: {}, MAD: {}, MAPE: {} %\".format(mean_squared_error(y_test_arouse, predicts_arouse),\n",
    "                                                   mean_absolute_error(y_test_arouse, predicts_arouse),\n",
    "                                                   median_absolute_error(y_test_arouse, predicts_arouse),\n",
    "                                                   mean_absolute_percentage_error(y_test_arouse, predicts_arouse)\n",
    "                                                  ))\n",
    "\n",
    "# Draw ditribution of absolute error\n",
    "plt.hist(np.abs(predicts_arouse - y_test_arouse), density = 1.0)\n",
    "plt.title(\"Distribution of Absolute error\")\n",
    "plt.xlabel(\"Absolute error\")\n",
    "plt.ylabel(\"Density\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**  \n",
    "- With the range of value from [1-5], I think the MSE < **0.5** could be very good.  \n",
    "- The model for arouse is better than that of valance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of arouse prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['exc', 'hap', 'sad', 'neu']\n",
    "groups = {}\n",
    "for l in LABELS:\n",
    "    groups[l] =  np.array([predicts_arouse[i] for i in range (0, len(predicts_arouse)) if y_test_label[i] == l])\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "subplots_adjust(right = 2, wspace=0.2, hspace=0.5, bottom=0)\n",
    "for i in range(0,4):\n",
    "    row = int(i/2)\n",
    "    col = int(i%2)\n",
    "    axs[row][col].hist(np.array(groups[LABELS[i]]), density=1)\n",
    "    axs[row][col].set_xlim([1,5])\n",
    "    axs[row][col].set_title(LABELS[i])\n",
    "    mu = np.mean(groups[LABELS[i]])\n",
    "    sigma = np.std(groups[LABELS[i]])\n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "    axs[row][col].plot(bins, y, '--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Evaluation the learned regression model for two-class classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the median value as a split point.\n",
    "split_point = np.median(y_train_arouse)\n",
    "\n",
    "# Categorize the values. All the values smaller than \"split_point\" is consider as low-arouse and vice-versa\n",
    "y_test_binary = np.array([int(i >= split_point) for i in y_test_arouse])\n",
    "\n",
    "# According to the predicting values,\n",
    "# Using thresholds ranging from [1-5] to indicate the True Posivite Rate and False Positive Rate\n",
    "# Then draw the ROC-curve\n",
    "thresholds = np.linspace(0, 5, 50)\n",
    "tpr_arr = []\n",
    "fpr_arr = []\n",
    "thresh_arr = [] \n",
    "for t in thresholds:\n",
    "    predict_binary = np.array([int(i >= t) for i in predicts_arouse])\n",
    "    cfm = confusion_matrix(y_test_binary, predict_binary)\n",
    "    parse_values = parse_metric(cfm)\n",
    "    tpr_arr.append(parse_values['TPR'])\n",
    "    fpr_arr.append(parse_values['FPR'])\n",
    "    thresh_arr.append(t)\n",
    "\n",
    "fpr_arr = np.asarray(fpr_arr)\n",
    "tpr_arr = np.asarray(tpr_arr)\n",
    "roc_auc = auc(fpr_arr[:,1], tpr_arr[:,1])\n",
    "plt.plot(fpr_arr[:,1], tpr_arr[:,1], label='ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: The ROC value of model predicting arouse is better than that of valance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Scatter plot of valance, arouse prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['exc', 'hap', 'sad']\n",
    "COLORS = {'exc': 'b', 'hap': 'b', 'sad': 'black'}\n",
    "valance_groups = {}\n",
    "arouse_groups = {}\n",
    "scatter_groups = {}\n",
    "plt.figure(figsize=(10,10))\n",
    "for l in LABELS:\n",
    "    valance_groups[l] = np.array([predicts_valance[i] for i in range (0, len(y_test)) if y_test[i].get_chosen_category() == l])\n",
    "    arouse_groups[l] = np.array([predicts_arouse[i] for i in range(0, len(y_test)) if y_test[i].get_chosen_category() == l])\n",
    "    scatter_groups[l] = plt.scatter(valance_groups[l], arouse_groups[l], color= COLORS[l])\n",
    "\n",
    "plt.xlabel(\"Valance\")\n",
    "plt.ylabel(\"Arouse\")\n",
    "plt.legend(tuple(scatter_groups.values()),\n",
    "           tuple(scatter_groups.keys()),\n",
    "           scatterpoints=1,\n",
    "           loc='lower right',\n",
    "           ncol=3,\n",
    "           fontsize=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use classfication method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handler_input = open('processed-data/input.obj', 'rb')\n",
    "file_handler_output = open('processed-data/output.obj', 'rb')\n",
    "input = np.array(pickle.load(file_handler_input))\n",
    "output = np.array(pickle.load(file_handler_output))\n",
    "\n",
    "\n",
    "in_out = list(filter(lambda x: ~np.isnan(np.array(x[0].get_acoustic_features(),  dtype=np.float64)).any()  , zip(input, output)))\n",
    "input, output = zip(*in_out)\n",
    "X = np.array([i.get_acoustic_features() for i in input])\n",
    "max_acoustic_features = np.max(X, axis=0)\n",
    "min_acoustic_features = np.min(X,  axis=0)\n",
    "mean_acoustic_features = np.mean(X, axis = 0)\n",
    "std_acoustic_features = np.std(X, axis = 0)\n",
    "\n",
    "##Dump max, min, mean, std of acoustic features into files\n",
    "pickle.dump(max_acoustic_features, open('processed-data/max_acoustic_features.obj', 'wb'))\n",
    "pickle.dump(min_acoustic_features, open('processed-data/min_acoustic_features.obj', 'wb'))\n",
    "pickle.dump(mean_acoustic_features, open('processed-data/mean_acoustic_features.obj', 'wb'))\n",
    "pickle.dump(std_acoustic_features, open('processed-data/std_acoustic_features.obj', 'wb'))\n",
    "\n",
    "\n",
    "print(\"Size input, output: {}, {}\".format(len(input), len(output)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [ 'exc' ,'hap', 'sad']\n",
    "in_out = list(filter(lambda x: ~np.isnan(np.array(x[0].get_acoustic_features(),  dtype=np.float64)).any() and x[1].get_chosen_category() in LABELS , zip(input, output)))\n",
    "input, output = zip(*in_out)\n",
    "\n",
    "print(\"Size input, output after filtering: {}, {}\".format(len(input), len(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = np.array([e.get_duration()  for e in output])\n",
    "dentas = durations[:,1]- durations[:,0]\n",
    "print(\"min: {}, max:  {}, mean: {}, median: {}, std: {}\".format(np.min(dentas),np.max(dentas), np.mean(dentas)\n",
    "                                                               ,np.median(dentas), np.std(dentas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get acoustic features and normalize it.\n",
    "X = np.array([i.get_acoustic_features() for i in input])\n",
    "X_norm = (X - min_acoustic_features)/ (max_acoustic_features - min_acoustic_features)\n",
    "\n",
    "#Categorize ouput\n",
    "LABELS_INT = {'exc':1, 'hap' :1, 'sad': 0}\n",
    "output_labels = np.array([int(LABELS_INT[i.get_chosen_category()]) for i in output ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spit data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, output_labels, test_size=0.2, random_state=300)\n",
    "print(\"Size training, testing set: \", len(X_train), \", \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Training and testing on IEMOCAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Training, predicts\n",
    "#clf = LogisticRegression()\n",
    "#clf = svm.SVR(kernel='linear')\n",
    "#clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "#clf = LogisticRegression()\n",
    "#clf =  RandomForestClassifier(max_depth=3, n_estimators=100, max_features=2)\n",
    "clf = MLPClassifier(hidden_layer_sizes= (200,), learning_rate_init= 0.001)\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation\n",
    "# from keras import backend as K\n",
    "# import tensorflow as tf\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# y_train_one_hot = np.array([to_categorical(y,2)  for y in y_train])\n",
    "\n",
    "# y_test_one_hot = np.array([to_categorical(y,2)  for y in y_test])\n",
    "# clf = Sequential([\n",
    "#         Dense(64, input_shape=(15,), kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(32, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(16,kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(8, kernel_initializer='normal'),\n",
    "#         Activation('relu'),\n",
    "#         Dense(2),\n",
    "#         Activation('sigmoid'),\n",
    "#     ])\n",
    "# clf.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# clf.fit(X_train, y_train_one_hot, validation_data=(X_test, y_test_one_hot), epochs = 10)\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "# Save model into file\n",
    "file_handler = open(\"model/model_classification_excited.sav\", 'wb')\n",
    "pickle.dump(clf, file_handler)\n",
    "\n",
    "\n",
    "predicts = clf.predict(X_test)\n",
    "pre_pro = clf.predict_proba(X_test)\n",
    "print(\"confusion matrix: \\n\", confusion_matrix(y_test, predicts))\n",
    "parse_matrix = parse_metric(confusion_matrix(y_test, predicts))\n",
    "print(\"Presison: {}\\nRecall: {}\\nAcc: {}\".format(parse_matrix['PPV'], parse_matrix['TPR'], parse_matrix['ACC'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds  = roc_curve(y_test, pre_pro[:, 1], pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "          lw=0.9,label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
